# YOLOv3 ğŸš€ by Ultralytics, GPL-3.0 license
"""
General utils
"""

import contextlib
import glob
import logging
import math
import os
import platform
import random
import re
import shutil
import signal
import time
import urllib
from itertools import repeat
from multiprocessing.pool import ThreadPool
from pathlib import Path
from subprocess import check_output
from zipfile import ZipFile

import cv2
import numpy as np
import pandas as pd
import pkg_resources as pkg
import torch
import torchvision
import yaml

from utils.downloads import gsutil_getsize
from utils.metrics import box_iou, fitness

# Settings

# è®¾ç½® PyTorch çš„æ‰“å°é€‰é¡¹ï¼šè¡Œå®½ä¸º 320ï¼Œç²¾åº¦ä¸ºå°æ•°ç‚¹å 5 ä½ï¼Œå¹¶ä½¿ç”¨ 'long' æ ¼å¼æ–‡ä»¶æ‰“å°
torch.set_printoptions(linewidth=320, precision=5, profile='long')
# è®¾ç½® NumPy çš„æ‰“å°é€‰é¡¹ï¼šè¡Œå®½ä¸º 320ï¼Œå°æ•°ç‚¹æ ¼å¼ä¸º '%11.5g'ï¼ˆå³å°æ•°ç‚¹å 5 ä½ï¼Œæœ‰æ•ˆæ•°å­—ä¸º 11 ä½ï¼‰
np.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5
# è®¾ç½® pandas æ˜¾ç¤ºé€‰é¡¹ï¼šæœ€å¤šæ˜¾ç¤º 10 åˆ—
pd.options.display.max_columns = 10
# è®¾ç½® OpenCV ä½¿ç”¨çš„çº¿ç¨‹æ•°ä¸º 0ï¼Œé˜²æ­¢å…¶ä¸ PyTorch DataLoader çš„å¤šçº¿ç¨‹ä¸å…¼å®¹é—®é¢˜
cv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)
# è®¾ç½®ç¯å¢ƒå˜é‡ 'NUMEXPR_MAX_THREADS' ä¸ºå½“å‰ CPU æ ¸å¿ƒæ•°ä¸ 8 ä¹‹é—´çš„æœ€å°å€¼ï¼Œæœ€å¤§çº¿ç¨‹æ•°ä¸è¶…è¿‡ 8
os.environ['NUMEXPR_MAX_THREADS'] = str(min(os.cpu_count(), 8))  # NumExpr max threads

FILE = Path(__file__).resolve()
ROOT = FILE.parents[1]  # root directory


def set_logging(name=None, verbose=True):
    # è®¾ç½®æ—¥å¿—è®°å½•çš„çº§åˆ«å¹¶è¿”å›æ—¥å¿—è®°å½•å™¨
    rank = int(os.getenv('RANK', -1))  # è·å–ç¯å¢ƒå˜é‡ 'RANK' çš„å€¼ï¼Œå¦‚æœæœªè®¾ç½®åˆ™é»˜è®¤ä¸º -1ï¼Œç”¨äºå¤š GPU è®­ç»ƒä¸­çš„æ’å
    # é…ç½®æ—¥å¿—çš„åŸºæœ¬è®¾ç½®ï¼šæ¶ˆæ¯æ ¼å¼ä¸ºç®€å•çš„ "%(message)s"
    # å¦‚æœ verbose ä¸º True å¹¶ä¸” rank ä¸º -1 æˆ– 0ï¼ˆå³å• GPU æˆ–ä¸»è¿›ç¨‹ï¼‰ï¼Œåˆ™æ—¥å¿—çº§åˆ«ä¸º INFOï¼Œå¦åˆ™ä¸º WARNING
    logging.basicConfig(format="%(message)s", level=logging.INFO if (verbose and rank in (-1, 0)) else logging.WARNING)
    return logging.getLogger(name)  # è¿”å›é…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨
# # å®šä¹‰å…¨å±€æ—¥å¿—è®°å½•å™¨ï¼ˆåœ¨ train.pyã€val.pyã€detect.py ç­‰æ¨¡å—ä¸­ä½¿ç”¨ï¼‰
LOGGER = set_logging(__name__)


class Profile(contextlib.ContextDecorator):
    # ç”¨æ³•ï¼šå¯ä»¥ç”¨ä½œ @Profile() è£…é¥°å™¨æˆ– 'with Profile():' ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    def __enter__(self):
        # è¿›å…¥ä¸Šä¸‹æ–‡æ—¶è®°å½•å¼€å§‹æ—¶é—´
        self.start = time.time()

    def __exit__(self, type, value, traceback):
        # é€€å‡ºä¸Šä¸‹æ–‡æ—¶è®¡ç®—å¹¶æ‰“å°æ‰€è€—æ—¶é—´
        print(f'Profile results: {time.time() - self.start:.5f}s')



class Timeout(contextlib.ContextDecorator):
    # ç”¨æ³•ï¼šå¯ä»¥ç”¨ä½œ @Timeout(seconds) è£…é¥°å™¨æˆ– 'with Timeout(seconds):' ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    def __init__(self, seconds, *, timeout_msg='', suppress_timeout_errors=True):
        self.seconds = int(seconds)  # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.timeout_message = timeout_msg  # è®¾ç½®è¶…æ—¶ä¿¡æ¯
        self.suppress = bool(suppress_timeout_errors)  # æ˜¯å¦æŠ‘åˆ¶è¶…æ—¶é”™è¯¯

    def _timeout_handler(self, signum, frame):
        # è¶…æ—¶å¤„ç†ç¨‹åºï¼ŒæŠ›å‡º TimeoutError å¹¶æ˜¾ç¤ºè¶…æ—¶ä¿¡æ¯
        raise TimeoutError(self.timeout_message)

    def __enter__(self):
        # è¿›å…¥ä¸Šä¸‹æ–‡æ—¶è®¾ç½®ä¿¡å·å¤„ç†å™¨ä¸º _timeout_handlerï¼Œå¼€å§‹å€’è®¡æ—¶
        signal.signal(signal.SIGALRM, self._timeout_handler)  # è®¾ç½® SIGALRM çš„å¤„ç†ç¨‹åº
        signal.alarm(self.seconds)  # å¼€å§‹å€’è®¡æ—¶ï¼Œè§¦å‘ SIGALRM

    def __exit__(self, exc_type, exc_val, exc_tb):
        # é€€å‡ºä¸Šä¸‹æ–‡æ—¶å–æ¶ˆä»»ä½•å·²è°ƒåº¦çš„ SIGALRM
        signal.alarm(0)  # å–æ¶ˆè°ƒåº¦çš„ SIGALRM
        if self.suppress and exc_type is TimeoutError:  # å¦‚æœæŠ‘åˆ¶è¶…æ—¶é”™è¯¯å¹¶ä¸”å‘ç”Ÿäº† TimeoutError
            return True  # æŠ‘åˆ¶ TimeoutError

class WorkingDirectory(contextlib.ContextDecorator):
    # ç”¨æ³•ï¼šå¯ä»¥ç”¨ä½œ @WorkingDirectory(dir) è£…é¥°å™¨æˆ– 'with WorkingDirectory(dir):' ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    def __init__(self, new_dir):
        self.dir = new_dir  # ç›®æ ‡ç›®å½•
        self.cwd = Path.cwd().resolve()  # å½“å‰å·¥ä½œç›®å½•

    def __enter__(self):
        # è¿›å…¥ä¸Šä¸‹æ–‡æ—¶åˆ‡æ¢åˆ°ç›®æ ‡ç›®å½•
        os.chdir(self.dir)

    def __exit__(self, exc_type, exc_val, exc_tb):
        # é€€å‡ºä¸Šä¸‹æ–‡æ—¶åˆ‡æ¢å›åŸå·¥ä½œç›®å½•
        os.chdir(self.cwd)

def try_except(func):
    # try-except å‡½æ•°ã€‚ç”¨æ³•ï¼š@try_except è£…é¥°å™¨
    def handler(*args, **kwargs):
        try:
            # å°è¯•æ‰§è¡Œè¢«è£…é¥°çš„å‡½æ•°
            func(*args, **kwargs)
        except Exception as e:
            # æ•è·ä»»ä½•å¼‚å¸¸å¹¶æ‰“å°å¼‚å¸¸ä¿¡æ¯
            print(e)
    return handler


def methods(instance):
    # è·å–ç±»æˆ–å®ä¾‹çš„æ–¹æ³•
    return [f for f in dir(instance) if callable(getattr(instance, f)) and not f.startswith("__")]

# æ‰“å°å‚æ•°çš„åŠŸèƒ½
def print_args(name, opt):
    # æ‰“å°å‘½ä»¤è¡Œè§£æå™¨çš„å‚æ•°
    # LOGGER.info(colorstr(f'{name}: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))
    print(colorstr(f'{name}: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))

def init_seeds(seed=0):
    # åˆå§‹åŒ–éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆRNGï¼‰çš„ç§å­ https://pytorch.org/docs/stable/notes/randomness.html
    # cudnn seed 0 è®¾ç½®æ›´æ…¢ä½†æ›´å¯é‡å¤ï¼Œå¦åˆ™æ›´å¿«ä½†è¾ƒä¸å¯é‡å¤
    import torch.backends.cudnn as cudnn
    random.seed(seed)  # è®¾ç½® Python éšæœºç§å­
    np.random.seed(seed)  # è®¾ç½® NumPy éšæœºç§å­
    torch.manual_seed(seed)  # è®¾ç½® PyTorch éšæœºç§å­
    # æ ¹æ®ç§å­è®¾ç½® cudnn çš„ benchmark å’Œ deterministic å±æ€§
    cudnn.benchmark, cudnn.deterministic = (False, True) if seed == 0 else (True, False)

def intersect_dicts(da, db, exclude=()):
    # è¿”å›ä¸¤ä¸ªå­—å…¸ä¸­é”®å’Œå€¼åŒ¹é…çš„äº¤é›†å­—å…¸ï¼Œæ’é™¤ 'exclude' ä¸­çš„é”®ï¼Œå¹¶ä½¿ç”¨ da çš„å€¼
    return {k: v for k, v in da.items() if k in db and not any(x in k for x in exclude) and v.shape == db[k].shape}

def get_latest_run(search_dir='.'):
    # è¿”å› /runs ç›®å½•ä¸­æœ€è¿‘çš„ 'last.pt' æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
    last_list = glob.glob(f'{search_dir}/**/last*.pt', recursive=True)  # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„ 'last*.pt' æ–‡ä»¶
    return max(last_list, key=os.path.getctime) if last_list else ''  # è¿”å›æœ€æ–°çš„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²

def user_config_dir(dir='Ultralytics', env_var='YOLOV3_CONFIG_DIR'):
    # è¿”å›ç”¨æˆ·é…ç½®ç›®å½•çš„è·¯å¾„ã€‚å¦‚æœç¯å¢ƒå˜é‡å­˜åœ¨ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡çš„å€¼ã€‚å¦‚æœéœ€è¦åˆ™åˆ›å»ºè¯¥ç›®å½•ã€‚
    env = os.getenv(env_var)  # è·å–ç¯å¢ƒå˜é‡å€¼
    if env:
        path = Path(env)  # ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„è·¯å¾„
    else:
        # ä¸åŒæ“ä½œç³»ç»Ÿçš„é»˜è®¤é…ç½®ç›®å½•
        cfg = {'Windows': 'AppData/Roaming', 'Linux': '.config', 'Darwin': 'Library/Application Support'}
        path = Path.home() / cfg.get(platform.system(), '')  # è·å–æ“ä½œç³»ç»Ÿç‰¹å®šçš„é…ç½®ç›®å½•
        # å¦‚æœç›®å½•ä¸å¯å†™ï¼Œåˆ™ä½¿ç”¨ /tmp ç›®å½•
        path = (path if is_writeable(path) else Path('/tmp')) / dir  # GCP å’Œ AWS lambda ä¿®å¤ï¼Œåªæœ‰ /tmp å¯å†™
    path.mkdir(exist_ok=True)  # å¦‚æœç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º
    return path  # è¿”å›é…ç½®ç›®å½•è·¯å¾„


def is_writeable(dir, test=False):
    # å¦‚æœç›®å½•å…·æœ‰å†™æƒé™åˆ™è¿”å› Trueï¼Œå¦‚æœ test=True åˆ™æµ‹è¯•æ‰“å¼€ä¸€ä¸ªå…·æœ‰å†™æƒé™çš„æ–‡ä»¶
    if test:  # æ–¹æ³• 1
        file = Path(dir) / 'tmp.txt'
        try:
            with open(file, 'w'):  # ä»¥å†™æƒé™æ‰“å¼€æ–‡ä»¶
                pass
            file.unlink()  # åˆ é™¤æ–‡ä»¶
            return True
        except OSError:
            return False
    else:  # æ–¹æ³• 2
        return os.access(dir, os.R_OK)  # åœ¨ Windows ä¸Šå¯èƒ½å­˜åœ¨é—®é¢˜

def is_docker():
    # åˆ¤æ–­å½“å‰ç¯å¢ƒæ˜¯å¦ä¸º Docker å®¹å™¨
    return Path('/workspace').exists()  # æˆ–è€… Path('/.dockerenv').exists()

def is_colab():
    # åˆ¤æ–­å½“å‰ç¯å¢ƒæ˜¯å¦ä¸º Google Colab å®ä¾‹
    try:
        import google.colab
        return True
    except ImportError:
        return False

def is_pip():
    # åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åœ¨ pip åŒ…ä¸­
    return 'site-packages' in Path(__file__).resolve().parts

def is_ascii(s=''):
    # åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦ç”±æ‰€æœ‰ ASCIIï¼ˆæ—  UTFï¼‰å­—ç¬¦ç»„æˆ
    # æ³¨æ„ï¼šstr().isascii() åœ¨ Python 3.7 ä¸­å¼•å…¥
    s = str(s)  # å°†åˆ—è¡¨ã€å…ƒç»„ã€None ç­‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    return len(s.encode().decode('ascii', 'ignore')) == len(s)

def is_chinese(s='äººå·¥æ™ºèƒ½'):
    # åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦åŒ…å«ä»»ä½•ä¸­æ–‡å­—ç¬¦
    return re.search('[\u4e00-\u9fff]', s)

def emojis(str=''):
    # è¿”å›å¹³å°ç›¸å…³çš„è¡¨æƒ…ç¬¦å·å®‰å…¨ç‰ˆæœ¬çš„å­—ç¬¦ä¸²
    return str.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else str

def file_size(path):
    # è¿”å›æ–‡ä»¶/ç›®å½•çš„å¤§å°ï¼ˆä»¥ MB ä¸ºå•ä½ï¼‰
    path = Path(path)
    if path.is_file():
        # å¦‚æœè·¯å¾„æ˜¯æ–‡ä»¶ï¼Œåˆ™è¿”å›æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰
        return path.stat().st_size / 1E6
    elif path.is_dir():
        # å¦‚æœè·¯å¾„æ˜¯ç›®å½•ï¼Œåˆ™è¿”å›ç›®å½•å†…æ‰€æœ‰æ–‡ä»¶çš„æ€»å¤§å°ï¼ˆMBï¼‰
        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / 1E6
    else:
        # å¦‚æœè·¯å¾„æ—¢ä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯ç›®å½•ï¼Œåˆ™è¿”å› 0.0
        return 0.0

def check_online():
    """
        æ£€æŸ¥äº’è”ç½‘è¿æ¥ã€‚

        å°è¯•åˆ›å»ºåˆ°å·²çŸ¥å¯é æœåŠ¡å™¨çš„å¥—æ¥å­—è¿æ¥ã€‚

        å‚æ•°:
        timeout (int): è¿æ¥å°è¯•çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚

        è¿”å›:
        bool: åœ¨çº¿æ—¶è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
        """
    import socket
    try:
        # å°è¯•åˆ›å»ºåˆ°å¯é æœåŠ¡å™¨çš„å¥—æ¥å­—è¿æ¥
        socket.create_connection(("1.1.1.1", 443), 5)  # check host accessibility
        return True
    except OSError:
        return False


@try_except
@WorkingDirectory(ROOT)
def check_git_status():
    # å¦‚æœä»£ç è¿‡æœŸï¼Œæ¨èä½¿ç”¨ 'git pull'
    msg = ', for updates see https://github.com/ultralytics/yolov3'
    print(colorstr('github: '), end='')
    assert Path('.git').exists(), 'skipping check (not a git repository)' + msg  # ç¡®ä¿å½“å‰ç›®å½•æ˜¯ä¸€ä¸ª git ä»“åº“
    assert not is_docker(), 'skipping check (Docker image)' + msg # ç¡®ä¿ä»£ç ä¸æ˜¯è¿è¡Œåœ¨ Docker å®¹å™¨ä¸­
    assert check_online(), 'skipping check (offline)' + msg # ç¡®ä¿ç³»ç»Ÿåœ¨çº¿
    # è·å–æœ€æ–°æ›´æ”¹å¹¶è·å–è¿œç¨‹ä»“åº“çš„ URL
    cmd = 'git fetch && git config --get remote.origin.url'
    url = check_output(cmd, shell=True, timeout=5).decode().strip().rstrip('.git')  # git fetch
    branch = check_output('git rev-parse --abbrev-ref HEAD', shell=True).decode().strip()  # è·å–å½“å‰åˆ†æ”¯åç§°
    n = int(check_output(f'git rev-list {branch}..origin/master --count', shell=True))  # è®¡ç®—æœ¬åœ°åˆ†æ”¯è½åäºè¿œç¨‹ä¸»åˆ†æ”¯çš„æäº¤æ•°é‡
    if n > 0:
        s = f"âš ï¸ YOLOv3 is out of date by {n} commit{'s' * (n > 1)}. Use `git pull` or `git clone {url}` to update."  # å¦‚æœæœ‰æ›´æ–°ï¼Œå»ºè®®æ‹‰å–æœ€æ–°æ›´æ”¹
    else:
        s = f'up to date with {url} âœ…'  # å¦‚æœå·²ç»æ˜¯æœ€æ–°ï¼Œæ˜¾ç¤ºåŒæ­¥ä¿¡æ¯
    print(emojis(s))  # emoji-safe   # æ‰“å°å¸¦æœ‰è¡¨æƒ…ç¬¦å·çš„çŠ¶æ€ä¿¡æ¯


def check_python(minimum='3.6.2'):
    """
            check_pythonæ˜¯æ£€æŸ¥å½“å‰çš„ç‰ˆæœ¬å·æ˜¯å¦æ»¡è¶³æœ€å°ç‰ˆæœ¬å·minimum
            è¢«è°ƒç”¨ï¼šå‡½æ•°check_requirementsä¸­
        """
    # å¯¹æ¯”å½“å‰ç‰ˆæœ¬å·å’Œè¾“å‡ºçš„è‡³å°‘çš„ç‰ˆæœ¬å·(pythonç‰ˆæœ¬ä¸€èˆ¬æ˜¯å‘ä¸‹å…¼å®¹çš„)
    # å¦‚æœæ»¡è¶³è¿”å›result=True åæ­£è¿”å›result=False
    # pkg.parse_version(ç‰ˆæœ¬å·)ç”¨äºå¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬å·çš„å¤§å°
    check_version(platform.python_version(), minimum, name='Python ', hard=True)


def check_version(current='0.0.0', minimum='0.0.0', name='version ', pinned=False, hard=False):
    # Check version vs. required version
    current, minimum = (pkg.parse_version(x) for x in (current, minimum))
    result = (current == minimum) if pinned else (current >= minimum)  # bool
    if hard:  # assert min requirements met
        assert result, f'{name}{minimum} required by YOLOv3, but {name}{current} is currently installed'
    else:
        return result


@try_except
def check_requirements(requirements=ROOT / 'requirements.txt', exclude=(), install=True):
    """
        æ£€æŸ¥å·²å®‰è£…çš„ä¾èµ–é¡¹æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œå¹¶å¯é€‰åœ°å®‰è£…ç¼ºå¤±çš„ä¾èµ–é¡¹ã€‚

        Args:
            requirements (str or Path, optional): requirements.txt æ–‡ä»¶çš„è·¯å¾„æˆ–åŒ…åç§°çš„åˆ—è¡¨/å…ƒç»„ã€‚é»˜è®¤ä¸º ROOT / 'requirements.txt'ã€‚
            exclude (tuple, optional): è¦æ’é™¤æ£€æŸ¥æˆ–å®‰è£…çš„ç‰¹å®šä¾èµ–é¡¹ã€‚
            install (bool, optional): æ˜¯å¦å°è¯•è‡ªåŠ¨å®‰è£…ç¼ºå¤±çš„ä¾èµ–é¡¹ã€‚

        Returns:
            None

        Raises:
            AssertionError: å¦‚æœæŒ‡å®šçš„ requirements æ–‡ä»¶ä¸å­˜åœ¨ã€‚

        """
    # è®¾ç½®å¸¦é¢œè‰²çš„æ—¥å¿—å‰ç¼€
    prefix = colorstr('red', 'bold', 'requirements:')
    check_python()  # æ£€æŸ¥ Python ç‰ˆæœ¬

    # è§£ærequirements.txtä¸­çš„æ‰€æœ‰åŒ… è§£ææˆlist é‡Œé¢å­˜æ”¾ç€ä¸€ä¸ªä¸ªçš„pkg_resources.Requirementç±»
    # å¦‚: ['matplotlib>=3.2.2', 'numpy>=1.18.5', â€¦â€¦]
    if isinstance(requirements, (str, Path)):  # requirements.txt file
        file = Path(requirements)  # å°†strå­—ç¬¦ä¸²requirementsè½¬æ¢æˆè·¯å¾„requirements
        assert file.exists(), f"{prefix} {file.resolve()} not found, check failed."
        with file.open() as f:
            # pkg_resources.parse_requirements:å¯ä»¥è§£æfileä¸­çš„æ¯ä¸€æ¡è¦æ±‚
            # æ¯ä¸€è¡Œè½¬æ¢ä¸ºpkg_resources.Requirementç±»å¹¶è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†
            # å¤„ç†å½¢å¼ä¸ºè°ƒç”¨æ¯ä¸€è¡Œå¯¹åº”çš„nameå’Œspecifierå±æ€§ã€‚å‰è€…ä»£è¡¨éœ€è¦åŒ…çš„åç§°ï¼Œåè€…ä»£è¡¨ç‰ˆæœ¬
            # è¿”å›list æ¯ä¸ªå…ƒç´ æ˜¯requirements.txtçš„ä¸€è¡Œ å¦‚: ['matplotlib>=3.2.2', 'numpy>=1.18.5', â€¦â€¦]
            requirements = [f'{x.name}{x.specifier}' for x in pkg.parse_requirements(f) if x.name not in exclude]
    else:  # list or tuple of packages
        requirements = [x for x in requirements if x not in exclude]

    n = 0  # ç»Ÿè®¡ä¸‹é¢ç¨‹åºæ›´æ–°åŒ…çš„ä¸ªæ•° number of packages updates
    for r in requirements:  # ä¾æ¬¡æ£€æŸ¥ç¯å¢ƒä¸­å®‰è£…çš„åŒ…(åŠæ¯ä¸ªåŒ…å¯¹åº”çš„ä¾èµ–åŒ…)æ˜¯å¦æ»¡è¶³requirementsä¸­çš„æ¯ä¸€ä¸ªæœ€ä½è¦æ±‚å®‰è£…åŒ…
        try:
            pkg.require(r)  # pkg_resources.require(file) è¿”å›å¯¹åº”åŒ…æ‰€éœ€çš„æ‰€æœ‰ä¾èµ–åŒ… å½“è¿™äº›åŒ…æœ‰å“ªä¸ªæœªå®‰è£…æˆ–è€…ç‰ˆæœ¬ä¸å¯¹çš„æ—¶å€™å°±ä¼šæŠ¥é”™
        except Exception as e:
            s = f"{prefix} {r} not found and is required by YOLOv3"
            if install:
                print(f"{s}, attempting auto-update...")
                try:
                    assert check_online(), f"'pip install {r}' skipped (offline)"
                    print(check_output(f"pip install '{r}'", shell=True).decode())
                    n += 1
                except Exception as e:
                    print(f'{prefix} {e}')
            else:
                print(f'{s}. Please install and rerun your command.')

    if n:  # if packages updated
        # if packages updated æ‰“å°ä¸€äº›æ›´æ–°ä¿¡æ¯
        source = file.resolve() if 'file' in locals() else requirements
        s = f"{prefix} {n} package{'s' * (n > 1)} updated per {source}\n" \
            f"{prefix} âš ï¸ {colorstr('bold', 'Restart runtime or rerun command for updates to take effect')}\n"
        print(emojis(s))


def check_img_size(imgsz, s=32, floor=0):
    # éªŒè¯å›¾åƒå°ºå¯¸åœ¨æ¯ä¸ªç»´åº¦ä¸Šæ˜¯å¦ä¸ºæ­¥å¹… s çš„å€æ•°
    # imgsz å¯ä»¥æ˜¯æ•´æ•°ï¼ˆä¾‹å¦‚ img_size=640ï¼‰æˆ–åˆ—è¡¨ï¼ˆä¾‹å¦‚ img_size=[640, 480]ï¼‰
    if isinstance(imgsz, int):  # å¦‚æœ imgsz æ˜¯æ•´æ•°
        new_size = max(make_divisible(imgsz, int(s)), floor)  # å°†å°ºå¯¸è°ƒæ•´ä¸ºæ­¥å¹… s çš„å€æ•°ï¼Œå¹¶ä¸ä½äº floor
    else:  # å¦‚æœ imgsz æ˜¯åˆ—è¡¨
        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]  # å¯¹åˆ—è¡¨ä¸­çš„æ¯ä¸ªå°ºå¯¸è¿›è¡Œç›¸åŒçš„è°ƒæ•´
    # å¦‚æœè°ƒæ•´åçš„å°ºå¯¸ä¸åŸå§‹å°ºå¯¸ä¸åŒï¼Œæ‰“å°è­¦å‘Šä¿¡æ¯
    if new_size != imgsz:
        print(f'WARNING: --img-size {imgsz} must be multiple of max stride {s}, updating to {new_size}')
    return new_size  # è¿”å›è°ƒæ•´åçš„å°ºå¯¸


def check_imshow():
    # æ£€æŸ¥ç¯å¢ƒæ˜¯å¦æ”¯æŒå›¾åƒæ˜¾ç¤º
    try:
        # ç¡®ä¿ä¸åœ¨ Docker ç¯å¢ƒä¸­
        assert not is_docker(), 'cv2.imshow() åœ¨ Docker ç¯å¢ƒä¸­è¢«ç¦ç”¨'
        # ç¡®ä¿ä¸åœ¨ Google Colab ç¯å¢ƒä¸­
        assert not is_colab(), 'cv2.imshow() åœ¨ Google Colab ç¯å¢ƒä¸­è¢«ç¦ç”¨'
        # å°è¯•ä½¿ç”¨ OpenCV æ˜¾ç¤ºä¸€ä¸ªæµ‹è¯•å›¾åƒ
        cv2.imshow('test', np.zeros((1, 1, 3)))  # æ˜¾ç¤ºä¸€ä¸ª 1x1 åƒç´ çš„é»‘è‰²å›¾åƒ
        cv2.waitKey(1)  # ç­‰å¾… 1 æ¯«ç§’ä»¥å¤„ç†æ˜¾ç¤º
        cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰ OpenCV çª—å£
        cv2.waitKey(1)  # ç­‰å¾… 1 æ¯«ç§’ä»¥ç¡®ä¿çª—å£å…³é—­
        return True  # å¦‚æœæ²¡æœ‰å¼‚å¸¸ï¼Œåˆ™ç¯å¢ƒæ”¯æŒå›¾åƒæ˜¾ç¤º
    except Exception as e:
        # æ•è·ä»»ä½•å¼‚å¸¸å¹¶æ‰“å°è­¦å‘Šä¿¡æ¯
        print(f'WARNING: ç¯å¢ƒä¸æ”¯æŒ cv2.imshow() æˆ– PIL Image.show() å›¾åƒæ˜¾ç¤º\n{e}')
        return False  # ç¯å¢ƒä¸æ”¯æŒå›¾åƒæ˜¾ç¤º

# æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªå‡½æ•°ç¡®ä¿äº†æ–‡ä»¶çš„åç¼€ç¬¦åˆæŒ‡å®šçš„æ ¼å¼è¦æ±‚ï¼Œå¹¶åœ¨æ ¼å¼ä¸ç¬¦æ—¶æä¾›æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯ã€‚
def check_suffix(file='yolov3.pt', suffix=('.pt',), msg=''):
    # æ£€æŸ¥æ–‡ä»¶çš„åç¼€æ˜¯å¦å¯æ¥å—
    if file and suffix:
        if isinstance(suffix, str):
            suffix = [suffix]
        for f in file if isinstance(file, (list, tuple)) else [file]:
            s = Path(f).suffix.lower()  # ä½¿ç”¨ Path å¯¹è±¡è·å–æ–‡ä»¶çš„åç¼€ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå°å†™ã€‚
            if len(s):
                assert s in suffix, f"{msg}{f} acceptable suffix is {suffix}"  # æ£€æŸ¥åç¼€æ˜¯å¦åœ¨å…è®¸çš„åç¼€åˆ—è¡¨ä¸­ï¼Œå¦‚æœä¸åœ¨ï¼Œåˆ™æŠ›å‡ºå¸¦æœ‰è‡ªå®šä¹‰æ¶ˆæ¯çš„æ–­è¨€é”™è¯¯ã€‚

def check_yaml(file, suffix=('.yaml', '.yml')):
    #  æœç´¢/ä¸‹è½½YAMLæ–‡ä»¶ï¼ˆå¦‚æœæœ‰å¿…è¦ï¼‰å¹¶è¿”å›è·¯å¾„ï¼Œæ£€æŸ¥åç¼€
    return check_file(file, suffix)

# é¦–å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™ç›´æ¥è¿”å›æ–‡ä»¶è·¯å¾„ã€‚
# å¦‚æœ file ä¸æ˜¯ URLï¼Œåˆ™åœ¨é¢„å®šä¹‰çš„ç›®å½•ä¸­æœç´¢æ–‡ä»¶ï¼Œå¹¶è¿”å›æ‰¾åˆ°çš„å”¯ä¸€æ–‡ä»¶è·¯å¾„ã€‚
def check_file(file, suffix=''):
    check_suffix(file, suffix)  # è°ƒç”¨check_suffix å‡½æ•°ï¼Œç”¨äºæ£€æŸ¥æ–‡ä»¶åç¼€æ˜¯å¦ç¬¦åˆè¦æ±‚
    file = str(file)  # å°†fileè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ã€‚
    if Path(file).is_file() or file == '':  # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨æˆ–è€…fileæ˜¯ç©ºå­—ç¬¦
        return file  #  å¦‚æœæ–‡ä»¶å·²ç»å­˜åœ¨ï¼Œç›´æ¥è¿”å› fileã€‚

    # å¦‚æœ file æ˜¯ä»¥ http:/ æˆ– https:/ å¼€å¤´çš„å­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºéœ€è¦ä¸‹è½½æ–‡ä»¶ã€‚
    # æœ¬è´¨ä¸Šæˆ‘ä»¬éƒ½ä¼šæä¾›æ•°æ®ï¼Œå¦å¤–åˆ©ç”¨è¯¥æ®µä»£ç è¿›è¡Œæ•°æ®æˆ–è€…æƒé‡çš„ä¸‹è½½å®¹æ˜“å¯¼è‡´å¤±è´¥ï¼Œå› æ­¤å¯å¿½ç•¥è¯¥æ®µä»£ç 
    elif file.startswith(('http:/', 'https:/')):  # download
        url = str(Path(file)).replace(':/', '://')  # Pathlib turns :// -> :/
        file = Path(urllib.parse.unquote(file).split('?')[0]).name  # '%2F' to '/', split https://url.com/file.txt?auth
        if Path(file).is_file():
            print(f'Found {url} locally at {file}')  # file already exists
        else:
            print(f'Downloading {url} to {file}...')
            torch.hub.download_url_to_file(url, file)
            assert Path(file).exists() and Path(file).stat().st_size > 0, f'File download failed: {url}'  # check
        return file


    else:  # å¦‚æœ fileä¸æ˜¯URLï¼Œé‚£ä¹ˆæœç´¢æœ¬åœ°æ–‡ä»¶ã€‚
        files = []
        for d in 'data', 'models', 'utils':  # åœ¨ data, models, utils ç›®å½•ä¸‹ä½¿ç”¨ glob.glob æœç´¢ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ï¼š
            files.extend(glob.glob(str(ROOT / d / '**' / file), recursive=True))  # find file
        assert len(files), f'File not found: {file}'  # æ–­è¨€æ‰¾åˆ°äº†å”¯ä¸€çš„æ–‡ä»¶
        assert len(files) == 1, f"Multiple files match '{file}', specify exact path: {files}"  # assert unique
        return files[0]  # è¿”å›æ‰¾åˆ°çš„æ–‡ä»¶è·¯å¾„

def check_dataset(data, autodownload=True):
    # å¦‚æœæ•°æ®é›†åœ¨æœ¬åœ°æœªæ‰¾åˆ°ï¼Œåˆ™ä¸‹è½½å’Œ/æˆ–è§£å‹æ•°æ®é›†
    # ç”¨æ³•ç¤ºä¾‹: https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128_with_yaml.zip
    # ä¸‹è½½ï¼ˆå¯é€‰ï¼‰
    extract_dir = ''
    if isinstance(data, (str, Path)) and str(data).endswith('.zip'):  # å¦‚æœæ•°æ®é›†è·¯å¾„ä»¥ .zip ç»“å°¾ï¼ˆä¾‹å¦‚ gs://bucket/dir/coco128.zipï¼‰
        download(data, dir='../datasets', unzip=True, delete=False, curl=False, threads=1)  # ä¸‹è½½å¹¶è§£å‹
        # æŸ¥æ‰¾è§£å‹åçš„ YAML æ–‡ä»¶
        data = next((Path('../datasets') / Path(data).stem).rglob('*.yaml'))
        extract_dir, autodownload = data.parent, False  # è®¾ç½®è§£å‹ç›®å½•å¹¶ç¦ç”¨è‡ªåŠ¨ä¸‹è½½

    # è¯»å– YAML æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    if isinstance(data, (str, Path)):
        with open(data, errors='ignore') as f:
            data = yaml.safe_load(f)  # è¯»å– YAML æ–‡ä»¶å¹¶è§£æä¸ºå­—å…¸

    # è§£æ YAML æ–‡ä»¶ä¸­çš„æ•°æ®
    path = extract_dir or Path(data.get('path') or '')  # è·å–æ•°æ®é›†è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½• '.'
    for k in 'train', 'val', 'test':
        if data.get(k):  # å¦‚æœ 'train'ã€'val' æˆ– 'test' é”®å­˜åœ¨
            # å°†è·¯å¾„å‰ç¼€æ·»åŠ åˆ°æ•°æ®è·¯å¾„ä¸­
            data[k] = str(path / data[k]) if isinstance(data[k], str) else [str(path / x) for x in data[k]]

    # ç¡®ä¿æ•°æ®é›†ä¸­åŒ…å« 'nc' é”®
    assert 'nc' in data, "Dataset 'nc' key missing."
    # å¦‚æœæ•°æ®é›†ä¸­ç¼ºå°‘ 'names' é”®ï¼Œåˆ™ä¸ºæ¯ä¸ªç±»ç”Ÿæˆé»˜è®¤åç§°
    if 'names' not in data:
        data['names'] = [f'class{i}' for i in range(data['nc'])]

    # è·å–è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•è·¯å¾„å’Œä¸‹è½½ä¿¡æ¯
    train, val, test, s = (data.get(x) for x in ('train', 'val', 'test', 'download'))
    # éªŒè¯ 'val' è·¯å¾„æ˜¯å¦å­˜åœ¨
    if val:
        val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # è§£æ 'val' è·¯å¾„
        if not all(x.exists() for x in val):  # å¦‚æœ 'val' è·¯å¾„ä¸­çš„ä»»ä½•è·¯å¾„ä¸å­˜åœ¨
            print('\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()])
            if s and autodownload:  # å¦‚æœéœ€è¦ä¸‹è½½è„šæœ¬å¹¶ä¸”å¯ç”¨äº†è‡ªåŠ¨ä¸‹è½½
                root = path.parent if 'path' in data else '..'  # è®¾ç½®è§£å‹ç›®å½•
                if s.startswith('http') and s.endswith('.zip'):  # å¦‚æœä¸‹è½½åœ°å€æ˜¯ URL å¹¶ä»¥ .zip ç»“å°¾
                    f = Path(s).name  # è·å–æ–‡ä»¶å
                    print(f'Downloading {s} to {f}...')
                    torch.hub.download_url_to_file(s, f)  # ä¸‹è½½æ–‡ä»¶
                    Path(root).mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•
                    ZipFile(f).extractall(path=root)  # è§£å‹æ–‡ä»¶
                    Path(f).unlink()  # åˆ é™¤ ZIP æ–‡ä»¶
                    r = None  # ä¸‹è½½æˆåŠŸ
                elif s.startswith('bash '):  # å¦‚æœä¸‹è½½åœ°å€æ˜¯ bash è„šæœ¬
                    print(f'Running {s} ...')
                    r = os.system(s)  # æ‰§è¡Œ bash è„šæœ¬
                else:  # å¦‚æœä¸‹è½½åœ°å€æ˜¯ python è„šæœ¬
                    r = exec(s, {'yaml': data})  # æ‰§è¡Œ python è„šæœ¬
                print(f"Dataset autodownload {f'success, saved to {root}' if r in (0, None) else 'failure'}\n")
            else:
                raise Exception('Dataset not found.')  # å¦‚æœæœªæ‰¾åˆ°æ•°æ®é›†ä¸”ä¸å¯ç”¨è‡ªåŠ¨ä¸‹è½½ï¼Œåˆ™å¼•å‘å¼‚å¸¸
    return data  # è¿”å›æ•°æ®å­—å…¸



def url2file(url):
    # å°† URL è½¬æ¢ä¸ºæ–‡ä»¶åï¼Œä¾‹å¦‚ https://url.com/file.txt?auth -> file.txt
    url = str(Path(url)).replace(':/', '://')  # Pathlib å¤„ç† URL æ—¶å°† :// è½¬æ¢ä¸º :/
    file = Path(urllib.parse.unquote(url)).name.split('?')[0]  # è§£ç  URLï¼Œå°† '%2F' è½¬æ¢ä¸º '/'ï¼Œç„¶åå»é™¤æŸ¥è¯¢å‚æ•°éƒ¨åˆ†
    return file  # è¿”å›æ–‡ä»¶å

def download(url, dir='.', unzip=True, delete=True, curl=False, threads=1):
    # å¤šçº¿ç¨‹æ–‡ä»¶ä¸‹è½½å’Œè§£å‹å‡½æ•°ï¼Œç”¨äº data.yaml çš„è‡ªåŠ¨ä¸‹è½½
    def download_one(url, dir):
        # ä¸‹è½½å•ä¸ªæ–‡ä»¶
        f = dir / Path(url).name  # ç”Ÿæˆæ–‡ä»¶å
        if Path(url).is_file():  # å¦‚æœæ–‡ä»¶åœ¨å½“å‰è·¯å¾„ä¸­å­˜åœ¨
            Path(url).rename(f)  # ç§»åŠ¨åˆ°ç›®æ ‡ç›®å½•
        elif not f.exists():  # å¦‚æœç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨
            print(f'Downloading {url} to {f}...')
            if curl:
                # ä½¿ç”¨ curl ä¸‹è½½ï¼Œæ”¯æŒé‡è¯•å’Œæ–­ç‚¹ç»­ä¼ 
                os.system(f"curl -L '{url}' -o '{f}' --retry 9 -C -")
            else:
                # ä½¿ç”¨ torch.hub ä¸‹è½½
                torch.hub.download_url_to_file(url, f, progress=True)
        # å¦‚æœéœ€è¦è§£å‹ä¸”æ–‡ä»¶åç¼€ä¸º .zip æˆ– .gz
        if unzip and f.suffix in ('.zip', '.gz'):
            print(f'Unzipping {f}...')
            if f.suffix == '.zip':
                # è§£å‹ .zip æ–‡ä»¶
                ZipFile(f).extractall(path=dir)
            elif f.suffix == '.gz':
                # è§£å‹ .gz æ–‡ä»¶
                os.system(f'tar xfz {f} --directory {f.parent}')
            if delete:
                # åˆ é™¤åŸå§‹å‹ç¼©æ–‡ä»¶
                f.unlink()
    dir = Path(dir)
    dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if threads > 1:
        # ä½¿ç”¨å¤šçº¿ç¨‹ä¸‹è½½
        pool = ThreadPool(threads)
        pool.imap(lambda x: download_one(*x), zip(url, repeat(dir)))  # å¤šçº¿ç¨‹ä¸‹è½½æ–‡ä»¶
        pool.close()
        pool.join()
    else:
        # å•çº¿ç¨‹ä¸‹è½½
        for u in [url] if isinstance(url, (str, Path)) else url:
            download_one(u, dir)

# make_divisible å‡½æ•°çš„ä½œç”¨æ˜¯å°†è¾“å…¥xè°ƒæ•´ä¸ºå¤§äºæˆ–ç­‰äºxä¸”èƒ½è¢«divisoræ•´é™¤çš„æœ€å°æ•´æ•°ã€‚
# ä¾‹å¦‚math.ceil(1.875) ç­‰äº 2
def make_divisible(x, divisor):
    # Returns x evenly divisible by divisor
    return math.ceil(x / divisor) * divisor

def clean_str(s):
    # æ¸…ç†å­—ç¬¦ä¸²ï¼Œé€šè¿‡å°†ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ _
    return re.sub(pattern="[|@#!Â¡Â·$â‚¬%&()=?Â¿^*;:,Â¨Â´><+]", repl="_", string=s)

def one_cycle(y1=0.0, y2=1.0, steps=100):
    # è¿”å›ä¸€ä¸ª lambda å‡½æ•°ï¼Œç”¨äºä» y1 åˆ° y2 çš„æ­£å¼¦æ³¢å½¢ rampï¼ˆè§ https://arxiv.org/pdf/1812.01187.pdfï¼‰
    # è¯¥å‡½æ•°åŸºäºç»™å®šçš„æ­¥éª¤æ•° steps åˆ›å»ºä¸€ä¸ªä» y1 åˆ° y2 çš„å‘¨æœŸæ€§å˜åŒ–
    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1

def colorstr(*input):
    # ä¸ºå­—ç¬¦ä¸²æ·»åŠ é¢œè‰²æ ·å¼ https://en.wikipedia.org/wiki/ANSI_escape_codeï¼Œä¾‹å¦‚ colorstr('blue', 'hello world')
    # *input å…è®¸ä¼ å…¥å¤šä¸ªå‚æ•°ï¼Œç¬¬ä¸€ä¸ªæˆ–å¤šä¸ªæ˜¯é¢œè‰²æ ·å¼ï¼Œæœ€åä¸€ä¸ªæ˜¯å­—ç¬¦ä¸²å†…å®¹
    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # è§£æé¢œè‰²æ ·å¼å’Œå­—ç¬¦ä¸²å†…å®¹
    colors = {
        'black': '\033[30m',  # åŸºæœ¬é¢œè‰²
        'red': '\033[31m',
        'green': '\033[32m',
        'yellow': '\033[33m',
        'blue': '\033[34m',
        'magenta': '\033[35m',
        'cyan': '\033[36m',
        'white': '\033[37m',
        'bright_black': '\033[90m',  # äº®è‰²
        'bright_red': '\033[91m',
        'bright_green': '\033[92m',
        'bright_yellow': '\033[93m',
        'bright_blue': '\033[94m',
        'bright_magenta': '\033[95m',
        'bright_cyan': '\033[96m',
        'bright_white': '\033[97m',
        'end': '\033[0m',  # ç»“æŸé¢œè‰²æ ·å¼
        'bold': '\033[1m',  # ç²—ä½“
        'underline': '\033[4m'  # ä¸‹åˆ’çº¿
    }
    # æ ¹æ®ä¼ å…¥çš„é¢œè‰²æ ·å¼æ„å»º ANSI é¢œè‰²ç å­—ç¬¦ä¸²ï¼Œå¹¶å°†å…¶ä¸å†…å®¹å­—ç¬¦ä¸²è¿æ¥ï¼Œæœ€ååŠ ä¸Šé‡ç½®é¢œè‰²çš„ç 
    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']

def labels_to_class_weights(labels, nc=80):
    # ä»è®­ç»ƒæ ‡ç­¾ä¸­è·å–ç±»åˆ«æƒé‡ï¼ˆåå‘é¢‘ç‡ï¼‰
    if labels[0] is None:  # å¦‚æœæ²¡æœ‰åŠ è½½æ ‡ç­¾
        return torch.Tensor()

    labels = np.concatenate(labels, 0)  # å°†æ‰€æœ‰æ ‡ç­¾åˆå¹¶æˆä¸€ä¸ªæ•°ç»„ï¼Œå½¢çŠ¶ä¸º (866643, 5)ï¼ˆä¾‹å¦‚ COCO æ•°æ®é›†ï¼‰
    classes = labels[:, 0].astype(np.int)  # æå–ç±»åˆ«åˆ—ï¼Œlabels = [ç±»åˆ« xywh]
    weights = np.bincount(classes, minlength=nc)  # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡ºç°æ¬¡æ•°

    # å‰ç½®ç½‘æ ¼ç‚¹è®¡æ•°ï¼ˆç”¨äº uCE è®­ç»ƒï¼‰
    # gpi = ((320 / 32 * np.array([1, 2, 4])) ** 2 * 3).sum()  # æ¯å¼ å›¾åƒçš„ç½‘æ ¼ç‚¹æ•°
    # weights = np.hstack([gpi * len(labels)  - weights.sum() * 9, weights * 9]) ** 0.5  # å°†ç½‘æ ¼ç‚¹æ•°å‰ç½®åˆ°å¼€å§‹ä½ç½®

    weights[weights == 0] = 1  # å°†ç©ºçš„ç±»åˆ«æƒé‡æ›¿æ¢ä¸º 1
    weights = 1 / weights  # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡æ•°é‡çš„å€’æ•°
    weights /= weights.sum()  # å½’ä¸€åŒ–
    return torch.from_numpy(weights)  # è½¬æ¢ä¸º PyTorch å¼ é‡å¹¶è¿”å›


def labels_to_image_weights(labels, nc=80, class_weights=np.ones(80)):
    # æ ¹æ®ç±»åˆ«æƒé‡å’Œå›¾åƒå†…å®¹ç”Ÿæˆå›¾åƒæƒé‡
    class_counts = np.array([np.bincount(x[:, 0].astype(np.int), minlength=nc) for x in labels])
    image_weights = (class_weights.reshape(1, nc) * class_counts).sum(1)
    # index = random.choices(range(n), weights=image_weights, k=1)  # æ ¹æ®å›¾åƒæƒé‡è¿›è¡Œæ ·æœ¬é€‰æ‹©
    return image_weights  # è¿”å›å›¾åƒæƒé‡


def coco80_to_coco91_class():
    # å°† COCO æ•°æ®é›†ä¸­çš„ 80 ç±»ç´¢å¼•ï¼ˆval2014ï¼‰è½¬æ¢ä¸º 91 ç±»ç´¢å¼•ï¼ˆè®ºæ–‡ä¸­ä½¿ç”¨çš„ç´¢å¼•ï¼‰
    # å‚è€ƒèµ„æ–™: https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/
    # ä»¥ä¸‹æ˜¯ä» 80 ç±»æ˜ å°„åˆ° 91 ç±»çš„ç´¢å¼•è½¬æ¢
    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\n')
    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\n')
    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet åˆ° coco çš„æ˜ å°„
    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco åˆ° darknet çš„æ˜ å°„

    # ä» COCO 80 ç±»åˆ° COCO 91 ç±»çš„æ˜ å°„åˆ—è¡¨
    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,
         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,
         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]
    return x  # è¿”å› COCO 80 ç±»åˆ° COCO 91 ç±»çš„æ˜ å°„åˆ—è¡¨


def xyxy2xywh(x):
    # å°†è¾¹ç•Œæ¡†ä» [x1, y1, x2, y2] æ ¼å¼è½¬æ¢ä¸º [x, y, w, h] æ ¼å¼
    # å…¶ä¸­ (x1, y1) æ˜¯å·¦ä¸Šè§’åæ ‡ï¼Œ(x2, y2) æ˜¯å³ä¸‹è§’åæ ‡

    # å…‹éš†è¾“å…¥ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼Œæ”¯æŒ PyTorch å¼ é‡å’Œ NumPy æ•°ç»„
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    # è®¡ç®—ä¸­å¿ƒåæ ‡ (x, y) å’Œå®½åº¦ (w), é«˜åº¦ (h)
    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x ä¸­å¿ƒåæ ‡
    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y ä¸­å¿ƒåæ ‡
    y[:, 2] = x[:, 2] - x[:, 0]  # å®½åº¦
    y[:, 3] = x[:, 3] - x[:, 1]  # é«˜åº¦
    return y  # è¿”å›è½¬æ¢åçš„è¾¹ç•Œæ¡†

def xywh2xyxy(x):
    # å°†è¾¹ç•Œæ¡†ä» [x, y, w, h] æ ¼å¼è½¬æ¢ä¸º [x1, y1, x2, y2] æ ¼å¼
    # å…¶ä¸­ (x, y) æ˜¯ä¸­å¿ƒåæ ‡ï¼Œw æ˜¯å®½åº¦ï¼Œh æ˜¯é«˜åº¦
    # è½¬æ¢åçš„æ ¼å¼ä¸­ (x1, y1) æ˜¯å·¦ä¸Šè§’åæ ‡ï¼Œ(x2, y2) æ˜¯å³ä¸‹è§’åæ ‡
    # å…‹éš†è¾“å…¥ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼Œæ”¯æŒ PyTorch å¼ é‡å’Œ NumPy æ•°ç»„
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    # è®¡ç®—å·¦ä¸Šè§’ (x1, y1) å’Œå³ä¸‹è§’ (x2, y2) çš„åæ ‡
    y[:, 0] = x[:, 0] - x[:, 2] / 2  # å·¦ä¸Šè§’ x åæ ‡
    y[:, 1] = x[:, 1] - x[:, 3] / 2  # å·¦ä¸Šè§’ y åæ ‡
    y[:, 2] = x[:, 0] + x[:, 2] / 2  # å³ä¸‹è§’ x åæ ‡
    y[:, 3] = x[:, 1] + x[:, 3] / 2  # å³ä¸‹è§’ y åæ ‡
    return y  # è¿”å›è½¬æ¢åçš„è¾¹ç•Œæ¡†

def xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):
    # å°†æ ‡å‡†åŒ–è¾¹ç•Œæ¡† [x, y, w, h] è½¬æ¢ä¸º [x1, y1, x2, y2] æ ¼å¼
    # å…¶ä¸­ (x, y) æ˜¯ç›¸å¯¹äºå›¾åƒçš„ä¸­å¿ƒåæ ‡ï¼Œw å’Œ h æ˜¯ç›¸å¯¹äºå›¾åƒçš„å®½åº¦å’Œé«˜åº¦
    # è½¬æ¢åçš„æ ¼å¼ä¸­ (x1, y1) æ˜¯å·¦ä¸Šè§’åæ ‡ï¼Œ(x2, y2) æ˜¯å³ä¸‹è§’åæ ‡
    # w å’Œ h æ˜¯å›¾åƒçš„å®½åº¦å’Œé«˜åº¦ï¼Œpadw å’Œ padh æ˜¯å®½åº¦å’Œé«˜åº¦çš„å¡«å……é‡
    # å…‹éš†è¾“å…¥ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼Œæ”¯æŒ PyTorch å¼ é‡å’Œ NumPy æ•°ç»„
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    # è®¡ç®—å·¦ä¸Šè§’ (x1, y1) å’Œå³ä¸‹è§’ (x2, y2) çš„åæ ‡
    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + padw  # å·¦ä¸Šè§’ x åæ ‡
    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + padh  # å·¦ä¸Šè§’ y åæ ‡
    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + padw  # å³ä¸‹è§’ x åæ ‡
    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + padh  # å³ä¸‹è§’ y åæ ‡
    return y  # è¿”å›è½¬æ¢åçš„è¾¹ç•Œæ¡†


def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):
    # å°†è¾¹ç•Œæ¡†ä» [x1, y1, x2, y2] æ ¼å¼è½¬æ¢ä¸º [x, y, w, h] æ ¼å¼ï¼Œå…¶ä¸­ x å’Œ y æ˜¯ä¸­å¿ƒåæ ‡ï¼Œw å’Œ h æ˜¯å®½åº¦å’Œé«˜åº¦
    # è½¬æ¢åçš„æ ¼å¼æ˜¯æ ‡å‡†åŒ–çš„ï¼Œå³ç›¸å¯¹äºå›¾åƒçš„å®½åº¦å’Œé«˜åº¦
    # clip: æ˜¯å¦å°†åæ ‡å‰ªè£åˆ° [0, 1] èŒƒå›´å†…
    # eps: ä¸€ä¸ªå°çš„å¸¸æ•°ï¼Œç”¨äºé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
    if clip:
        # å¦‚æœ clip ä¸º Trueï¼Œåˆ™å¯¹åæ ‡è¿›è¡Œå‰ªè£ï¼Œä»¥ç¡®ä¿å®ƒä»¬åœ¨ [0, 1] èŒƒå›´å†…
        clip_coords(x, (h - eps, w - eps))  # æ³¨æ„ï¼šæ­¤æ“ä½œä¼šå°±åœ°ä¿®æ”¹ x
    # å…‹éš†è¾“å…¥ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼Œæ”¯æŒ PyTorch å¼ é‡å’Œ NumPy æ•°ç»„
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    # è®¡ç®—ä¸­å¿ƒåæ ‡ (x, y) å’Œå®½åº¦ (w) é«˜åº¦ (h)
    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # ä¸­å¿ƒ x åæ ‡
    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # ä¸­å¿ƒ y åæ ‡
    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # å®½åº¦
    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # é«˜åº¦
    return y  # è¿”å›è½¬æ¢åçš„è¾¹ç•Œæ¡†


def xyn2xy(x, w=640, h=640, padw=0, padh=0):
    # å°†å½’ä¸€åŒ–çš„çº¿æ®µåæ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡ï¼Œè¾“å…¥å½¢çŠ¶ä¸º (n, 2)
    # x: å½’ä¸€åŒ–çš„çº¿æ®µåæ ‡æ•°ç»„æˆ–å¼ é‡ï¼ŒåŒ…å« (x_center, y_center) çš„å½’ä¸€åŒ–å€¼
    # w: å›¾åƒå®½åº¦
    # h: å›¾åƒé«˜åº¦
    # padw: æ°´å¹³åç§»é‡
    # padh: å‚ç›´åç§»é‡

    # å…‹éš†è¾“å…¥ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼Œæ”¯æŒ PyTorch å¼ é‡å’Œ NumPy æ•°ç»„
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    # å°†å½’ä¸€åŒ–çš„åæ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡
    y[:, 0] = w * x[:, 0] + padw  # è®¡ç®—åƒç´ åæ ‡ x
    y[:, 1] = h * x[:, 1] + padh  # è®¡ç®—åƒç´ åæ ‡ y

    return y  # è¿”å›åƒç´ åæ ‡çš„æ•°ç»„æˆ–å¼ é‡


def segment2box(segment, width=640, height=640):
    # å°†ä¸€ä¸ªåˆ†æ®µæ ‡ç­¾è½¬æ¢ä¸ºä¸€ä¸ªæ¡†æ ‡ç­¾ï¼Œå¹¶åº”ç”¨å›¾åƒå†…éƒ¨çº¦æŸï¼Œå³å°† (xy1, xy2, ...) è½¬æ¢ä¸º (xyxy)
    # segment: åŒ…å«åˆ†æ®µçš„ x å’Œ y åæ ‡çš„æ•°ç»„æˆ–å¼ é‡
    # width: å›¾åƒçš„å®½åº¦
    # height: å›¾åƒçš„é«˜åº¦

    # æå– x å’Œ y åæ ‡
    x, y = segment.T  # x å’Œ y åæ ‡çš„è½¬ç½®ï¼Œå‡è®¾ segment çš„å½¢çŠ¶æ˜¯ (n, 2)

    # çº¦æŸåœ¨å›¾åƒå†…éƒ¨çš„ç‚¹
    inside = (x >= 0) & (y >= 0) & (x <= width) & (y <= height)  # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…

    # è¿‡æ»¤åªä¿ç•™åœ¨å›¾åƒå†…éƒ¨çš„ç‚¹
    x, y = x[inside], y[inside]

    # è®¡ç®—æ¡†çš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œå½¢æˆ (x1, y1, x2, y2) æ ¼å¼
    return np.array([x.min(), y.min(), x.max(), y.max()]) if any(x) else np.zeros((1, 4))  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç‚¹ï¼Œè¿”å›å…¨é›¶çš„æ¡†


def segments2boxes(segments):
    # å°†åˆ†æ®µæ ‡ç­¾è½¬æ¢ä¸ºæ¡†æ ‡ç­¾ï¼Œå³ (cls, xy1, xy2, ...) è½¬æ¢ä¸º (cls, xywh)
    boxes = []
    for s in segments:
        x, y = s.T  # segment xy
        # å°†åˆ†æ®µç‚¹çš„ x å’Œ y åæ ‡è½¬æ¢ä¸ºæœ€å°è¾¹ç•Œæ¡†
        boxes.append([x.min(), y.min(), x.max(), y.max()])  # (x1, y1, x2, y2)
    # å°†è¾¹ç•Œæ¡†ä» (x1, y1, x2, y2) è½¬æ¢ä¸º (x, y, w, h)
    return xyxy2xywh(np.array(boxes))  # (cls, xywh)


def resample_segments(segments, n=1000):
    # å¯¹æ¯ä¸ª (n,2) çš„æ®µè¿›è¡Œä¸Šé‡‡æ ·
    for i, s in enumerate(segments):
        x = np.linspace(0, len(s) - 1, n)  # åˆ›å»ºå‡åŒ€åˆ†å¸ƒçš„ x å€¼ç”¨äºæ’å€¼
        xp = np.arange(len(s))  # åŸå§‹ x å€¼
        segments[i] = np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)]) \
            .reshape(2, -1).T  # å°†æ’å€¼ç»“æœé‡å¡‘ä¸º (2, -1) çš„æ•°ç»„ï¼Œå¹¶è½¬ç½®
    return segments


def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):
    # å°†åæ ‡ (xyxy) ä» img1_shape é‡æ–°ç¼©æ”¾åˆ° img0_shape
    if ratio_pad is None:  # å¦‚æœæ²¡æœ‰æä¾› ratio_padï¼Œä» img0_shape è®¡ç®—
        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # è®¡ç®—ç¼©æ”¾å› å­
        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # è®¡ç®—å¡«å……
    else:
        gain = ratio_pad[0][0]
        pad = ratio_pad[1]

    coords[:, [0, 2]] -= pad[0]  # åº”ç”¨ x è½´å¡«å……
    coords[:, [1, 3]] -= pad[1]  # åº”ç”¨ y è½´å¡«å……
    coords[:, :4] /= gain  # åå‘åº”ç”¨ç¼©æ”¾å› å­
    clip_coords(coords, img0_shape)  # é™åˆ¶åæ ‡åœ¨å›¾åƒè¾¹ç•Œå†…
    return coords



def clip_coords(boxes, shape):
    # å°†è¾¹ç•Œæ¡† (xyxy) é™åˆ¶åœ¨å›¾åƒå°ºå¯¸ (height, width) å†…
    if isinstance(boxes, torch.Tensor):  # å¯¹äºå•ç‹¬çš„ Tensorï¼Œæ›´å¿«
        boxes[:, 0].clamp_(0, shape[1])  # é™åˆ¶ x1
        boxes[:, 1].clamp_(0, shape[0])  # é™åˆ¶ y1
        boxes[:, 2].clamp_(0, shape[1])  # é™åˆ¶ x2
        boxes[:, 3].clamp_(0, shape[0])  # é™åˆ¶ y2
    else:  # å¯¹äº np.arrayï¼Œæ›´å¿«
        boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, shape[1])  # é™åˆ¶ x1 å’Œ x2
        boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, shape[0])  # é™åˆ¶ y1 å’Œ y2


def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,
                        labels=(), max_det=300):
    """å¯¹æ¨ç†ç»“æœè¿›è¡Œéæå¤§å€¼æŠ‘åˆ¶ (NMS)

    è¿”å›:
         æ¯å¼ å›¾åƒçš„æ£€æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒæ˜¯ä¸€ä¸ª (n,6) çš„å¼ é‡ [xyxy, conf, cls]
    """

    nc = prediction.shape[2] - 5  # ç±»åˆ«æ•°é‡
    xc = prediction[..., 4] > conf_thres  # ç½®ä¿¡åº¦å€™é€‰

    # æ£€æŸ¥
    assert 0 <= conf_thres <= 1, f'æ— æ•ˆçš„ç½®ä¿¡åº¦é˜ˆå€¼ {conf_thres}ï¼Œæœ‰æ•ˆå€¼èŒƒå›´æ˜¯ 0.0 åˆ° 1.0'
    assert 0 <= iou_thres <= 1, f'æ— æ•ˆçš„ IoU é˜ˆå€¼ {iou_thres}ï¼Œæœ‰æ•ˆå€¼èŒƒå›´æ˜¯ 0.0 åˆ° 1.0'

    # è®¾ç½®
    min_wh, max_wh = 2, 4096  # (åƒç´ ) æœ€å°å’Œæœ€å¤§æ¡†å®½é«˜
    max_nms = 30000  # ä¼ é€’åˆ° torchvision.ops.nms() çš„æœ€å¤§æ¡†æ•°é‡
    time_limit = 10.0  # è¶…è¿‡æ­¤æ—¶é—´åé€€å‡º
    redundant = True  # éœ€è¦å†—ä½™æ£€æµ‹
    multi_label &= nc > 1  # æ¯ä¸ªæ¡†å¤šä¸ªæ ‡ç­¾ (å¢åŠ  0.5ms/å›¾åƒ)
    merge = False  # ä½¿ç”¨åˆå¹¶ NMS

    t = time.time()
    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]
    for xi, x in enumerate(prediction):  # å›¾åƒç´¢å¼•ï¼Œå›¾åƒæ¨ç†
        # åº”ç”¨çº¦æŸ
        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # å®½é«˜
        x = x[xc[xi]]  # ç½®ä¿¡åº¦

        # å¦‚æœæœ‰è‡ªåŠ¨æ ‡è®°çš„æ ‡ç­¾åˆ™æ‹¼æ¥
        if labels and len(labels[xi]):
            l = labels[xi]
            v = torch.zeros((len(l), nc + 5), device=x.device)
            v[:, :4] = l[:, 1:5]  # æ¡†
            v[:, 4] = 1.0  # ç½®ä¿¡åº¦
            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # ç±»åˆ«
            x = torch.cat((x, v), 0)

        # å¦‚æœæ²¡æœ‰å‰©ä½™çš„æ¡†ï¼Œå¤„ç†ä¸‹ä¸€å¼ å›¾åƒ
        if not x.shape[0]:
            continue

        # è®¡ç®—ç½®ä¿¡åº¦
        x[:, 5:] *= x[:, 4:5]  # ç½®ä¿¡åº¦ = å¯¹è±¡ç½®ä¿¡åº¦ * ç±»åˆ«ç½®ä¿¡åº¦

        # æ¡† (ä¸­å¿ƒ x, ä¸­å¿ƒ y, å®½, é«˜) è½¬æ¢ä¸º (x1, y1, x2, y2)
        box = xywh2xyxy(x[:, :4])

        # æ£€æµ‹çŸ©é˜µ nx6 (xyxy, ç½®ä¿¡åº¦, ç±»åˆ«)
        if multi_label:
            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T
            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)
        else:  # ä»…æœ€ä½³ç±»åˆ«
            conf, j = x[:, 5:].max(1, keepdim=True)
            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]

        # æŒ‰ç±»åˆ«ç­›é€‰
        if classes is not None:
            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]

        # åº”ç”¨æœ‰é™çº¦æŸ
        # if not torch.isfinite(x).all():
        #     x = x[torch.isfinite(x).all(1)]

        # æ£€æŸ¥å½¢çŠ¶
        n = x.shape[0]  # æ¡†æ•°é‡
        if not n:  # æ²¡æœ‰æ¡†
            continue
        elif n > max_nms:  # æ¡†è¿‡å¤š
            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # æŒ‰ç½®ä¿¡åº¦æ’åº

        # æ‰¹é‡ NMS
        c = x[:, 5:6] * (0 if agnostic else max_wh)  # ç±»åˆ«
        boxes, scores = x[:, :4] + c, x[:, 4]  # æ¡† (æŒ‰ç±»åˆ«åç§»), ç½®ä¿¡åº¦
        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS
        if i.shape[0] > max_det:  # é™åˆ¶æ£€æµ‹æ•°é‡
            i = i[:max_det]
        if merge and (1 < n < 3E3):  # åˆå¹¶ NMS (æ¡†ä½¿ç”¨åŠ æƒå¹³å‡åˆå¹¶)
            # æ›´æ–°æ¡†ä½œä¸º boxes(i,4) = weights(i,n) * boxes(n,4)
            iou = box_iou(boxes[i], boxes) > iou_thres  # iou çŸ©é˜µ
            weights = iou * scores[None]  # æ¡†æƒé‡
            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # åˆå¹¶çš„æ¡†
            if redundant:
                i = i[iou.sum(1) > 1]  # éœ€è¦å†—ä½™

        output[xi] = x[i]
        if (time.time() - t) > time_limit:
            print(f'è­¦å‘Š: è¶…è¿‡ NMS æ—¶é—´é™åˆ¶ {time_limit}s')
            break  # è¶…è¿‡æ—¶é—´é™åˆ¶
    return output



def strip_optimizer(f='best.pt', s=''):  # from utils.general import *; strip_optimizer()
    # ä» 'f' ä¸­å»é™¤ä¼˜åŒ–å™¨ä¿¡æ¯ï¼Œä»¥å®Œæˆè®­ç»ƒï¼Œç»“æœå¯é€‰æ‹©æ€§ä¿å­˜ä¸º 's'
    x = torch.load(f, map_location=torch.device('cpu'))
    if x.get('ema'):
        x['model'] = x['ema']  # ç”¨ ema æ›¿æ¢æ¨¡å‹
    for k in 'optimizer', 'training_results', 'wandb_id', 'ema', 'updates':  # å»é™¤è¿™äº›é”®
        x[k] = None
    x['epoch'] = -1
    x['model'].half()  # è½¬ä¸º FP16
    for p in x['model'].parameters():
        p.requires_grad = False
    torch.save(x, s or f)
    mb = os.path.getsize(s or f) / 1E6  # æ–‡ä»¶å¤§å°
    print(f"Optimizer stripped from {f},{(' saved as %s,' % s) if s else ''} {mb:.1f}MB")

def print_mutation(results, hyp, save_dir, bucket):
    evolve_csv, results_csv, evolve_yaml = save_dir / 'evolve.csv', save_dir / 'results.csv', save_dir / 'hyp_evolve.yaml'
    keys = ('metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',
            'val/box_loss', 'val/obj_loss', 'val/cls_loss') + tuple(hyp.keys())  # [results + hyps]
    keys = tuple(x.strip() for x in keys)
    vals = results + tuple(hyp.values())
    n = len(keys)

    # ä¸‹è½½ï¼ˆå¯é€‰ï¼‰
    if bucket:
        url = f'gs://{bucket}/evolve.csv'
        if gsutil_getsize(url) > (os.path.getsize(evolve_csv) if os.path.exists(evolve_csv) else 0):
            os.system(f'gsutil cp {url} {save_dir}')  # å¦‚æœ evolve.csv å¤§äºæœ¬åœ°æ–‡ä»¶ï¼Œåˆ™ä¸‹è½½

    # è®°å½•åˆ° evolve.csv
    s = '' if evolve_csv.exists() else (('%20s,' * n % keys).rstrip(',') + '\n')  # æ·»åŠ è¡¨å¤´
    with open(evolve_csv, 'a') as f:
        f.write(s + ('%20.5g,' * n % vals).rstrip(',') + '\n')

    # æ‰“å°åˆ°å±å¹•
    print(colorstr('evolve: ') + ', '.join(f'{x.strip():>20s}' for x in keys))
    print(colorstr('evolve: ') + ', '.join(f'{x:20.5g}' for x in vals), end='\n\n\n')

    # ä¿å­˜ä¸º yaml
    with open(evolve_yaml, 'w') as f:
        data = pd.read_csv(evolve_csv)
        data = data.rename(columns=lambda x: x.strip())  # å»é™¤é”®çš„å¤šä½™ç©ºæ ¼
        i = np.argmax(fitness(data.values[:, :7]))  # è®¡ç®—æœ€ä½³é€‚åº”åº¦
        f.write('# YOLOv3 Hyperparameter Evolution Results\n' +
                f'# Best generation: {i}\n' +
                f'# Last generation: {len(data)}\n' +
                '# ' + ', '.join(f'{x.strip():>20s}' for x in keys[:7]) + '\n' +
                '# ' + ', '.join(f'{x:>20.5g}' for x in data.values[i, :7]) + '\n\n')
        yaml.safe_dump(hyp, f, sort_keys=False)

    if bucket:
        os.system(f'gsutil cp {evolve_csv} {evolve_yaml} gs://{bucket}')  # ä¸Šä¼ åˆ°æŒ‡å®š bucket


def apply_classifier(x, model, img, im0):
    # å¯¹ YOLO è¾“å‡ºåº”ç”¨ç¬¬äºŒé˜¶æ®µåˆ†ç±»å™¨
    # ç¤ºä¾‹æ¨¡å‹ = torchvision.models.__dict__['efficientnet_b0'](pretrained=True).to(device).eval()
    im0 = [im0] if isinstance(im0, np.ndarray) else im0
    for i, d in enumerate(x):  # é’ˆå¯¹æ¯å¼ å›¾ç‰‡
        if d is not None and len(d):
            d = d.clone()

            # é‡æ–°è°ƒæ•´å’Œå¡«å……åˆ‡å‰²åŒºåŸŸ
            b = xyxy2xywh(d[:, :4])  # è½¬æ¢ä¸º [x, y, w, h]
            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # è½¬æ¢ä¸ºæ­£æ–¹å½¢
            b[:, 2:] = b[:, 2:] * 1.3 + 30  # å¡«å……
            d[:, :4] = xywh2xyxy(b).long()  # è½¬æ¢å› [x1, y1, x2, y2]

            # å°†æ¡†çš„åæ ‡ä» img_size è°ƒæ•´åˆ° im0 å¤§å°
            scale_coords(img.shape[2:], d[:, :4], im0[i].shape)

            # ç±»åˆ«é¢„æµ‹
            pred_cls1 = d[:, 5].long()  # åŸå§‹ç±»åˆ«é¢„æµ‹
            ims = []
            for j, a in enumerate(d):  # é’ˆå¯¹æ¯ä¸ªæ£€æµ‹æ¡†
                cutout = im0[i][int(a[1]):int(a[3]), int(a[0]):int(a[2])]  # è£å‰ªå›¾åƒåŒºåŸŸ
                im = cv2.resize(cutout, (224, 224))  # è°ƒæ•´å¤§å°åˆ° 224x224 BGR
                # cv2.imwrite('example%i.jpg' % j, cutout)

                im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR è½¬ RGBï¼Œè°ƒæ•´ä¸º 3x224x224
                im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 è½¬ float32
                im /= 255  # 0 - 255 è½¬ 0.0 - 1.0
                ims.append(im)

            pred_cls2 = model(torch.Tensor(ims).to(d.device)).argmax(1)  # åˆ†ç±»å™¨é¢„æµ‹
            x[i] = x[i][pred_cls1 == pred_cls2]  # ä¿ç•™åŒ¹é…çš„åˆ†ç±»æ£€æµ‹

    return x


def increment_path(path, exist_ok=False, sep='', mkdir=False):
    # å¢åŠ æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„ï¼Œä¾‹å¦‚ runs/exp --> runs/exp{sep}2, runs/exp{sep}3, ... ç­‰ç­‰
    path = Path(path)  # å…¼å®¹æ“ä½œç³»ç»Ÿ
    if path.exists() and not exist_ok:
        path, suffix = (path.with_suffix(''), path.suffix) if path.is_file() else (path, '')
        dirs = glob.glob(f"{path}{sep}*")  # ç±»ä¼¼è·¯å¾„
        matches = [re.search(rf"%s{sep}(\d+)" % path.stem, d) for d in dirs]
        i = [int(m.groups()[0]) for m in matches if m]  # ç´¢å¼•
        n = max(i) + 1 if i else 2  # å¢é‡æ•°
        path = Path(f"{path}{sep}{n}{suffix}")  # å¢åŠ è·¯å¾„
    if mkdir:
        path.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•
    return path

# Variables
NCOLS = 0 if is_docker() else shutil.get_terminal_size().columns  # ç»ˆç«¯çª—å£å¤§å°
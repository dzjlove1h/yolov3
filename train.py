# YOLOv3 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Train a  model on a custom dataset
    è¿™ä¸ªæ–‡ä»¶æ˜¯yolov3çš„è®­ç»ƒè„šæœ¬ã€‚
    æŠ“ä½ æ•°æ® + æ¨¡å‹ + å­¦ä¹ ç‡ + ä¼˜åŒ–å™¨ + è®­ç»ƒè¿™äº”æ­¥å³å¯ã€‚
    Train a YOLOv3 model on a custom dataset.
"""
import argparse
import math
import os
import random
import sys
import time
from copy import deepcopy
from datetime import datetime
from pathlib import Path

import numpy as np
import torch
import torch.distributed as dist
import torch.nn as nn
import yaml
from torch.cuda import amp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import SGD, Adam, lr_scheduler
from tqdm import tqdm

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

import val  # for end-of-epoch mAP
from models.experimental import attempt_load
from models.yolo import Model
from utils.autoanchor import check_anchors
from utils.autobatch import check_train_batch_size
from utils.callbacks import Callbacks
from utils.datasets import create_dataloader
from utils.downloads import attempt_download
from utils.general import (LOGGER, NCOLS, check_dataset, check_file, check_git_status, check_img_size,
                           check_requirements, check_suffix, check_yaml, colorstr, get_latest_run, increment_path,
                           init_seeds, intersect_dicts, labels_to_class_weights, labels_to_image_weights, methods,
                           one_cycle, print_args, print_mutation, strip_optimizer)
from utils.loggers import Loggers
from utils.loggers.wandb.wandb_utils import check_wandb_resume
from utils.loss import ComputeLoss
from utils.metrics import fitness
from utils.plots import plot_evolve, plot_labels
from utils.torch_utils import EarlyStopping, ModelEMA, de_parallel, select_device, torch_distributed_zero_first

LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # è¿™ä¸ª Worker æ˜¯è¿™å°æœºå™¨ä¸Šçš„ç¬¬å‡ ä¸ª Worker
RANK = int(os.getenv('RANK', -1))  # è¿™ä¸ª Worker æ˜¯å…¨å±€ç¬¬å‡ ä¸ª Worker
WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))  # æ€»å…±æœ‰å‡ ä¸ª Worker


def train(hyp,  # path/to/hyp.yaml or hyp dictionary
          opt,
          device,
          callbacks
          ):
    """
        :params hyp: data/hyps/hyp.scratch.yaml   hyp dictionary
        :params opt: mainä¸­optå‚æ•°
        :params device: å½“å‰è®¾å¤‡
    """
    # ----------------------------------------------- åˆå§‹åŒ–å‚æ•°å’Œé…ç½®ä¿¡æ¯ ----------------------------------------------
    # åˆå§‹åŒ–ptå‚æ•° + è·¯å¾„ä¿¡æ¯ + è¶…å‚è®¾ç½®ä¿å­˜ + ä¿å­˜opt + åŠ è½½æ•°æ®é…ç½®ä¿¡æ¯ + æ‰“å°æ—¥å¿—ä¿¡æ¯(logger + wandb) + å…¶ä»–å‚æ•°(plotsã€cudaã€ncã€namesã€is_coco)
    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, = \
        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \
        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze

    # Directories
    w = save_dir / 'weights'  # ä¿å­˜æƒé‡çš„è·¯å¾„ å¦‚runs/train/exp18/weights
    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir
    last, best = w / 'last.pt', w / 'best.pt'

    # Hyperparametersè¶…å‚æ•°
    if isinstance(hyp, str):
        with open(hyp, errors='ignore') as f:
            hyp = yaml.safe_load(f)  # åŠ è½½hypè¶…å‚ä¿¡æ¯
    # æ—¥å¿—è¾“å‡ºè¶…å‚ä¿¡æ¯ hyperparameters: ...
    LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))

    with open(save_dir / 'hyp.yaml', 'w') as f:  # ä¿å­˜hyp
        yaml.safe_dump(hyp, f, sort_keys=False)
    with open(save_dir / 'opt.yaml', 'w') as f:  # ä¿å­˜opt
        yaml.safe_dump(vars(opt), f, sort_keys=False)
    data_dict = None

    # æ—¥å¿—è®°å½•å™¨
    if RANK in [-1, 0]:  # ä»…åœ¨ä¸»è¦è¿›ç¨‹ä¸­åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # åˆ›å»ºæ—¥å¿—è®°å½•å™¨å®ä¾‹
        if loggers.wandb:  # å¦‚æœä½¿ç”¨ wandb è¿›è¡Œæ—¥å¿—è®°å½•
            data_dict = loggers.wandb.data_dict
            if resume:  # å¦‚æœæ˜¯æ¢å¤è®­ç»ƒ
                weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp  # ä» opt ä¸­è·å–æƒé‡ã€epochs å’Œè¶…å‚æ•°

        # æ³¨å†Œå›è°ƒå‡½æ•°
        for k in methods(loggers):  # è·å– loggers çš„æ–¹æ³•
            callbacks.register_action(k, callback=getattr(loggers, k))  # æ³¨å†Œæ¯ä¸ªæ–¹æ³•ä¸ºå›è°ƒå‡½æ•°

    # é…ç½®
    plots = not evolve  # åˆ›å»ºç»˜å›¾
    cuda = device.type != 'cpu'  # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ GPU
    init_seeds(1 + RANK)  # åˆå§‹åŒ–éšæœºç§å­
    with torch_distributed_zero_first(LOCAL_RANK):  # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ç¡®ä¿åªæœ‰ä¸€ä¸ªè¿›ç¨‹æ‰§è¡Œ
        data_dict = data_dict or check_dataset(data)  # æ£€æŸ¥æ•°æ®é›†ï¼Œå¦‚æœä¸º None åˆ™éªŒè¯æ•°æ®é›†
    train_path, val_path = data_dict['train'], data_dict['val']  # è·å–è®­ç»ƒå’ŒéªŒè¯è·¯å¾„
    nc = 1 if single_cls else int(data_dict['nc'])  # ç±»åˆ«æ•°é‡
    names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # ç±»åˆ«åç§°
    # æ£€æŸ¥ç±»åˆ«åç§°ä¸ç±»åˆ«æ•°é‡æ˜¯å¦åŒ¹é…
    assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'
    is_coco = isinstance(val_path, str) and val_path.endswith('coco/val2017.txt')  # æ£€æŸ¥æ˜¯å¦ä¸º COCO æ•°æ®é›†

    # Model
    check_suffix(weights, '.pt')  # æ£€æŸ¥æƒé‡æ–‡ä»¶åç¼€
    pretrained = weights.endswith('.pt')  # åˆ¤æ–­æ˜¯å¦ä¸ºé¢„è®­ç»ƒæ¨¡å‹
    if pretrained:
        with torch_distributed_zero_first(LOCAL_RANK):  # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ç¡®ä¿åªæœ‰ä¸€ä¸ªè¿›ç¨‹æ‰§è¡Œä¸‹è½½
            weights = attempt_download(weights)  # å¦‚æœæœ¬åœ°ä¸å­˜åœ¨ï¼Œåˆ™ä¸‹è½½æƒé‡æ–‡ä»¶
        ckpt = torch.load(weights, map_location=device)  # åŠ è½½æ£€æŸ¥ç‚¹
        # è¿™é‡ŒåŠ è½½æ¨¡å‹æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€ç§æ˜¯é€šè¿‡opt.cfg å¦ä¸€ç§æ˜¯é€šè¿‡ckpt['model'].yaml
        # åŒºåˆ«åœ¨äºæ˜¯å¦ä½¿ç”¨resume å¦‚æœä½¿ç”¨resumeä¼šå°†opt.cfgè®¾ä¸ºç©ºï¼ŒæŒ‰ç…§ckpt['model'].yamlæ¥åˆ›å»ºæ¨¡å‹
        # è¿™ä¹Ÿå½±å“äº†ä¸‹é¢æ˜¯å¦é™¤å»anchorçš„key(ä¹Ÿå°±æ˜¯ä¸åŠ è½½anchor), å¦‚æœresumeåˆ™ä¸åŠ è½½anchor
        # åŸå› : ä¿å­˜çš„æ¨¡å‹ä¼šä¿å­˜anchorsï¼Œæœ‰æ—¶å€™ç”¨æˆ·è‡ªå®šä¹‰äº†anchorä¹‹åï¼Œå†resumeï¼Œåˆ™åŸæ¥åŸºäºcocoæ•°æ®é›†çš„anchorä¼šè‡ªå·±è¦†ç›–è‡ªå·±è®¾å®šçš„anchor
        model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # åˆ›å»ºæ¨¡å‹
        # æ’é™¤çš„é”®
        exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []
        csd = ckpt['model'].float().state_dict()  # å°†æ£€æŸ¥ç‚¹çš„çŠ¶æ€å­—å…¸è½¬æ¢ä¸º FP32
        csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # å–äº¤é›†
        model.load_state_dict(csd, strict=False)  # åŠ è½½çŠ¶æ€å­—å…¸
        LOGGER.info(f'ä» {weights} è½¬ç§»äº† {len(csd)}/{len(model.state_dict())} é¡¹')  # è®°å½•è½¬ç§»çš„é¡¹ç›®æ•°é‡
    else:
        model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # åˆ›å»ºæ¨¡å‹

    # å†»ç»“å±‚
    freeze = [f'model.{x}.' for x in range(freeze)]  # è¦å†»ç»“çš„å±‚
    for k, v in model.named_parameters():
        v.requires_grad = True  # å…è®¸æ‰€æœ‰å±‚è¿›è¡Œè®­ç»ƒ
        if any(x in k for x in freeze):  # å¦‚æœå½“å‰å‚æ•°ååœ¨å†»ç»“åˆ—è¡¨ä¸­
            LOGGER.info(f'å†»ç»“ {k}')  # è®°å½•å†»ç»“çš„å±‚
            v.requires_grad = False  # å–æ¶ˆè¯¥å±‚çš„æ¢¯åº¦è®¡ç®—

    # Image size
    gs = max(int(model.stride.max()), 32)  # ç½‘æ ¼å¤§å°ï¼ˆæœ€å¤§æ­¥å¹…ï¼‰
    imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # éªŒè¯å›¾åƒå¤§å°æ˜¯ gs çš„å€æ•°

    # Batch size
    if RANK == -1 and batch_size == -1:  # ä»…åœ¨å• GPU ä¸‹ï¼Œä¼°è®¡æœ€ä½³æ‰¹é‡å¤§å°
        batch_size = check_train_batch_size(model, imgsz)  # æ£€æŸ¥å¹¶ç¡®å®šæœ€ä½³æ‰¹é‡å¤§å°

    # Optimizer
    nbs = 64  # åä¹‰æ‰¹é‡å¤§å°
    accumulate = max(round(nbs / batch_size), 1)  # åœ¨ä¼˜åŒ–å‰ç´¯ç§¯æŸå¤±
    hyp['weight_decay'] *= batch_size * accumulate / nbs  # æ ¹æ®æ‰¹é‡å¤§å°ç¼©æ”¾æƒé‡è¡°å‡
    LOGGER.info(f"Scaled weight_decay = {hyp['weight_decay']}")

    g0, g1, g2 = [], [], []  # ä¼˜åŒ–å™¨å‚æ•°ç»„
    for v in model.modules():
        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # åç½®
            g2.append(v.bias)
        if isinstance(v, nn.BatchNorm2d):  # æƒé‡ï¼ˆä¸è¡°å‡ï¼‰
            g0.append(v.weight)
        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # æƒé‡ï¼ˆæœ‰è¡°å‡ï¼‰
            g1.append(v.weight)

    if opt.adam:
        optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # è°ƒæ•´ beta1 ä¸ºåŠ¨é‡
    else:
        optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)

    optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # æ·»åŠ  g1ï¼ˆå¸¦è¡°å‡çš„æƒé‡ï¼‰
    optimizer.add_param_group({'params': g2})  # æ·»åŠ  g2ï¼ˆåç½®ï¼‰

    LOGGER.info(f"{colorstr('optimizer:')} {type(optimizer).__name__} with parameter groups "
                f"{len(g0)} weight, {len(g1)} weight (no decay), {len(g2)} bias")
    del g0, g1, g2

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if opt.linear_lr:
        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # çº¿æ€§å­¦ä¹ ç‡è°ƒæ•´
    else:
        lf = one_cycle(1, hyp['lrf'], epochs)  # ä½™å¼¦å­¦ä¹ ç‡è°ƒæ•´ï¼Œä» 1 åˆ° hyp['lrf']
    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    # plot_lr_scheduler(optimizer, scheduler, epochs)  # å¯é€‰ï¼šç»˜åˆ¶å­¦ä¹ ç‡è°ƒåº¦æ›²çº¿

    # å•å¡è®­ç»ƒ: ä½¿ç”¨EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰å¯¹æ¨¡å‹çš„å‚æ•°åšå¹³å‡, ä¸€ç§ç»™äºˆè¿‘æœŸæ•°æ®æ›´é«˜æƒé‡çš„å¹³å‡æ–¹æ³•, ä»¥æ±‚æé«˜æµ‹è¯•æŒ‡æ ‡å¹¶å¢åŠ æ¨¡å‹é²æ£’ã€‚
    ema = ModelEMA(model) if RANK in [-1, 0] else None

    # ä½¿ç”¨é¢„è®­ç»ƒ
    start_epoch, best_fitness = 0, 0.0
    if pretrained:
        # ä¼˜åŒ–å™¨
        if ckpt['optimizer'] is not None:  # å¦‚æœå­˜åœ¨ä¼˜åŒ–å™¨çŠ¶æ€
            optimizer.load_state_dict(ckpt['optimizer'])  # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            best_fitness = ckpt['best_fitness']  # æ›´æ–°æœ€ä½³æ€§èƒ½

        # EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
        if ema and ckpt.get('ema'):  # å¦‚æœå¯ç”¨äº† EMA
            ema.ema.load_state_dict(ckpt['ema'].float().state_dict())  # åŠ è½½ EMA çŠ¶æ€
            ema.updates = ckpt['updates']  # æ›´æ–°æ¬¡æ•°

        # è®­ç»ƒè½®æ•°
        start_epoch = ckpt['epoch'] + 1  # ä»æ£€æŸ¥ç‚¹è·å–èµ·å§‹è½®æ•°
        if resume:
            assert start_epoch > 0, f'{weights} training to {epochs} epochs is finished, nothing to resume.'
        if epochs < start_epoch:  # å‡è®¾ä½ è®¾å®šçš„epochä¸º100æ¬¡ï¼Œä½†æ˜¯æ­¤æ—¶åŠ è½½çš„æ¨¡å‹å·²ç»è®­ç»ƒ150æ¬¡ï¼Œåˆ™æ¥ç€å†è®­ç»ƒ100æ¬¡ã€‚å¦‚æœå°äºåˆ™è®­ç»ƒå‰©ä½™çš„éƒ¨åˆ†ã€‚
            LOGGER.info(f"{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs.")
            epochs += ckpt['epoch']  # finetune additional epochs

        del ckpt, csd

    # æ˜¯å¦ä½¿ç”¨DP mode
    # å¦‚æœrank=-1ä¸”gpuæ•°é‡>1ï¼Œåˆ™ä½¿ç”¨DataParallelå•æœºå¤šå¡æ¨¡å¼ï¼Œæ•ˆæœå¹¶ä¸å¥½ï¼ˆåˆ†å¸ƒä¸å¹³å‡ï¼‰
    if cuda and RANK == -1 and torch.cuda.device_count() > 1:
        LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\n'
                       'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')
        model = torch.nn.DataParallel(model)

    # SyncBatchNorm  æ˜¯å¦ä½¿ç”¨è·¨å¡BN
    if opt.sync_bn and cuda and RANK != -1:
        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)
        LOGGER.info('Using SyncBatchNorm()')

    # Trainloader
    # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨å’Œæ•°æ®é›†
    train_loader, dataset = create_dataloader(
        train_path,  # è®­ç»ƒæ•°æ®è·¯å¾„
        imgsz,  # å›¾åƒå¤§å°
        batch_size // WORLD_SIZE,  # æ¯ä¸ªè¿›ç¨‹çš„æ‰¹å¤§å°
        gs,  # ç½‘æ ¼å¤§å°
        single_cls,  # æ˜¯å¦ä¸ºå•ç±»æ£€æµ‹
        hyp=hyp,  # è¶…å‚æ•°
        augment=True,  # æ˜¯å¦è¿›è¡Œæ•°æ®å¢å¼º
        cache=opt.cache,  # æ˜¯å¦ç¼“å­˜æ•°æ®
        rect=opt.rect,  # æ˜¯å¦ä½¿ç”¨çŸ©å½¢æ‰¹æ¬¡
        rank=LOCAL_RANK,  # å½“å‰è¿›ç¨‹çš„rank
        workers=workers,  # æ•°æ®åŠ è½½çš„å·¥ä½œçº¿ç¨‹æ•°é‡
        image_weights=opt.image_weights,  # æ˜¯å¦ä½¿ç”¨å›¾åƒæƒé‡
        quad=opt.quad,  # æ˜¯å¦ä½¿ç”¨å››å…ƒç»„æ ¼å¼
        prefix=colorstr('train: '),  # å‰ç¼€ï¼Œç”¨äºè¾“å‡ºä¿¡æ¯
        shuffle=True  # æ˜¯å¦æ‰“ä¹±æ•°æ®
    )
    # è·å–æœ€å¤§æ ‡ç­¾ç±»
    mlc = int(np.concatenate(dataset.labels, 0)[:, 0].max())  # max label class
    nb = len(train_loader)  # æ‰¹æ¬¡æ•°é‡
    # æ–­è¨€æ ‡ç­¾ç±»ä¸è¶…è¿‡ç±»åˆ«æ•°
    assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'

    # Process 0
    if RANK in [-1, 0]:
        # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
        val_loader = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls,
                                       hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1,
                                       workers=workers, pad=0.5,
                                       prefix=colorstr('val: '))[0]

        if not resume:
            # åˆå¹¶æ ‡ç­¾
            labels = np.concatenate(dataset.labels, 0)
            # c = torch.tensor(labels[:, 0])  # ç±»åˆ«
            # cf = torch.bincount(c.long(), minlength=nc) + 1.  # é¢‘ç‡
            # model._initialize_biases(cf.to(device))

            if plots:
                # ç»˜åˆ¶æ ‡ç­¾åˆ†å¸ƒå›¾
                plot_labels(labels, names, save_dir)

            # æ£€æŸ¥é”šæ¡†
            if not opt.noautoanchor:
                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)
            model.half().float()  # é¢„å‡é”šæ¡†ç²¾åº¦

        # è¿è¡Œå›è°ƒå‡½æ•°
        callbacks.run('on_pretrain_routine_end')

    # DDPæ¨¡å¼
    if cuda and RANK != -1:
        # å°†æ¨¡å‹åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œæ¨¡å¼
        model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)

    # Model parameters
    nl = de_parallel(model).model[-1].nl  # è·å–æ£€æµ‹å±‚çš„æ•°é‡ (number of detection layers)
    hyp['box'] *= 3 / nl  # å°†æ¡†çš„æŸå¤±ç¼©æ”¾åˆ°æ£€æµ‹å±‚æ•°
    hyp['cls'] *= nc / 80 * 3 / nl  # å°†ç±»åˆ«æŸå¤±ç¼©æ”¾åˆ°ç±»åˆ«æ•°é‡å’Œæ£€æµ‹å±‚æ•°
    hyp['obj'] *= (imgsz / 640) ** 2 * 3 / nl  # å°†ç‰©ä½“æŸå¤±ç¼©æ”¾åˆ°å›¾åƒå¤§å°å’Œæ£€æµ‹å±‚æ•°
    hyp['label_smoothing'] = opt.label_smoothing  # è®¾ç½®æ ‡ç­¾å¹³æ»‘å‚æ•°
    model.nc = nc  # å°†ç±»åˆ«æ•°é‡é™„åŠ åˆ°æ¨¡å‹
    model.hyp = hyp  # å°†è¶…å‚æ•°é™„åŠ åˆ°æ¨¡å‹
    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # è®¡ç®—å¹¶é™„åŠ ç±»åˆ«æƒé‡
    model.names = names  # å°†ç±»åé™„åŠ åˆ°æ¨¡å‹

    # Start training
    t0 = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # è®¡ç®—æš–èº«è¿­ä»£æ¬¡æ•°ï¼Œè‡³å°‘1000æ¬¡
    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # é™åˆ¶æš–èº«è¿­ä»£æ¬¡æ•°ä¸ºè®­ç»ƒçš„ä¸€åŠ
    last_opt_step = -1  # æœ€åä¼˜åŒ–æ­¥æ•°
    maps = np.zeros(nc)  # æ¯ä¸ªç±»åˆ«çš„mAP
    results = (0, 0, 0, 0, 0, 0, 0)  # åˆå§‹ç»“æœï¼ŒåŒ…å«P, R, mAP@.5, mAP@.5-.95, val_loss
    scheduler.last_epoch = start_epoch - 1  # ä¸ç§»åŠ¨è°ƒåº¦å™¨çš„æœ€åä¸€ä¸ªepoch
    scaler = amp.GradScaler(enabled=cuda)  # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒçš„Scaler
    stopper = EarlyStopping(patience=opt.patience)  # åˆå§‹åŒ–æ—©åœç±»
    compute_loss = ComputeLoss(model)  # åˆå§‹åŒ–æŸå¤±è®¡ç®—ç±»
    LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\n'
                f'Using {train_loader.num_workers * WORLD_SIZE} dataloader workers\n'
                f"Logging results to {colorstr('bold', save_dir)}\n"
                f'Starting training for {epochs} epochs...')
    for epoch in range(start_epoch, epochs):  # å¼€å§‹è®­ç»ƒå¾ªç¯ï¼Œéå†æ¯ä¸ªepoch
        model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼

        # å¯é€‰ï¼šæ›´æ–°å›¾åƒæƒé‡ï¼ˆä»…é™å•GPUï¼‰
        if opt.image_weights:
            cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # è®¡ç®—ç±»åˆ«æƒé‡
            iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # è®¡ç®—å›¾åƒæƒé‡
            dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # éšæœºé€‰æ‹©åŠ æƒçš„ç´¢å¼•

        mloss = torch.zeros(3, device=device)  # åˆå§‹åŒ–å¹³å‡æŸå¤±
        if RANK != -1:
            train_loader.sampler.set_epoch(epoch)  # è®¾ç½®DistributedSamplerçš„epoch
        pbar = enumerate(train_loader)  # è·å–è®­ç»ƒæ•°æ®çš„è¿­ä»£å™¨
        print(('\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))
        if RANK in [-1, 0]:
            pbar = tqdm(pbar, total=nb, ncols=NCOLS, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # æ˜¾ç¤ºè¿›åº¦æ¡

        optimizer.zero_grad()  # é‡ç½®æ¢¯åº¦
        for i, (
        imgs, targets, paths, _) in pbar:  # éå†æ¯ä¸ªæ‰¹æ¬¡ -------------------------------------------------------------
            ni = i + nb * epoch  # è®¡ç®—è‡ªè®­ç»ƒå¼€å§‹ä»¥æ¥çš„ç´¯è®¡æ‰¹æ¬¡æ•°
            imgs = imgs.to(device, non_blocking=True).float() / 255  # å°†å›¾åƒè½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶å½’ä¸€åŒ–

            # çƒ­èº«é˜¶æ®µ
            if ni <= nw:
                xi = [0, nw]  # xæ’å€¼
                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())  # è®¡ç®—ç´¯è®¡çš„æ‰¹æ¬¡æ•°
                for j, x in enumerate(optimizer.param_groups):
                    # å¯¹æ¯ä¸ªå‚æ•°ç»„è®¾ç½®å­¦ä¹ ç‡å’ŒåŠ¨é‡
                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])
                    if 'momentum' in x:
                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])

            # å¤šå°ºåº¦è®­ç»ƒ
            if opt.multi_scale:
                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # éšæœºé€‰æ‹©å›¾åƒå¤§å°
                sf = sz / max(imgs.shape[2:])  # è®¡ç®—ç¼©æ”¾å› å­
                if sf != 1:
                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # è®¡ç®—æ–°å½¢çŠ¶
                    imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)  # è°ƒæ•´å›¾åƒå¤§å°

            # å‰å‘ä¼ æ’­
            with amp.autocast(enabled=cuda):
                pred = model(imgs)  # æ¨¡å‹å‰å‘ä¼ æ’­
                loss, loss_items = compute_loss(pred, targets.to(device))  # è®¡ç®—æŸå¤±
                if RANK != -1:
                    loss *= WORLD_SIZE  # åœ¨DDPæ¨¡å¼ä¸‹ç¼©æ”¾æŸå¤±

            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()  # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦

            # ä¼˜åŒ–
            if ni - last_opt_step >= accumulate:
                scaler.step(optimizer)  # æ›´æ–°ä¼˜åŒ–å™¨
                scaler.update()  # æ›´æ–°ç¼©æ”¾å™¨
                optimizer.zero_grad()  # é‡ç½®æ¢¯åº¦
                if ema:
                    ema.update(model)  # æ›´æ–°æŒ‡æ•°ç§»åŠ¨å¹³å‡
                last_opt_step = ni  # æ›´æ–°æœ€åä¼˜åŒ–æ­¥éª¤

            # æ—¥å¿—è®°å½•
            if RANK in [-1, 0]:
                mloss = (mloss * i + loss_items) / (i + 1)  # æ›´æ–°å¹³å‡æŸå¤±
                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
                pbar.set_description(('%10s' * 2 + '%10.4g' * 5) % (
                    f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1]))

        # å­¦ä¹ ç‡è°ƒåº¦
        lr = [x['lr'] for x in optimizer.param_groups]  # è®°å½•å­¦ä¹ ç‡
        scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡

        if RANK in [-1, 0]:  # å¦‚æœæ˜¯ä¸»è¿›ç¨‹ï¼ˆæˆ–å•GPUï¼‰
            # è®¡ç®—mAP
            callbacks.run('on_train_epoch_end', epoch=epoch)  # è°ƒç”¨å›è°ƒå‡½æ•°ï¼Œæ ‡è®°epochç»“æŸ
            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])  # æ›´æ–°EMAæ¨¡å‹çš„å±æ€§
            final_epoch = (epoch + 1 == epochs) or stopper.possible_stop  # åˆ¤æ–­æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªepochæˆ–æ˜¯å¦å¯ä»¥åœæ­¢
            if not noval or final_epoch:  # å¦‚æœéœ€è¦éªŒè¯æˆ–æ˜¯æœ€åä¸€ä¸ªepoch
                results, maps, _ = val.run(data_dict,  # è¿›è¡ŒéªŒè¯
                                           batch_size=batch_size // WORLD_SIZE * 2,  # è®¾ç½®æ‰¹å¤§å°
                                           imgsz=imgsz,  # è¾“å…¥å›¾åƒå¤§å°
                                           model=ema.ema,  # ä½¿ç”¨EMAæ¨¡å‹è¿›è¡ŒéªŒè¯
                                           single_cls=single_cls,  # å•ç±»æ£€æµ‹æ ‡å¿—
                                           dataloader=val_loader,  # éªŒè¯æ•°æ®åŠ è½½å™¨
                                           save_dir=save_dir,  # ä¿å­˜ç›®å½•
                                           plots=False,  # ä¸ç»˜åˆ¶å›¾è¡¨
                                           callbacks=callbacks,  # å›è°ƒå‡½æ•°
                                           compute_loss=compute_loss)  # è®¡ç®—æŸå¤±

            # æ›´æ–°æœ€ä½³mAP
            fi = fitness(np.array(results).reshape(1, -1))  # è®¡ç®—åŠ æƒçš„æŒ‡æ ‡ç»„åˆ [P, R, mAP@.5, mAP@.5-.95]
            if fi > best_fitness:  # å¦‚æœå½“å‰ç»“æœä¼˜äºå†å²æœ€ä½³
                best_fitness = fi  # æ›´æ–°æœ€ä½³é€‚åº”åº¦
            log_vals = list(mloss) + list(results) + lr  # è®°å½•æŸå¤±å’Œç»“æœ
            callbacks.run('on_fit_epoch_end', log_vals, epoch, best_fitness, fi)  # è°ƒç”¨å›è°ƒå‡½æ•°ï¼Œè®°å½•epochç»“æŸ

            # ä¿å­˜æ¨¡å‹
            if (not nosave) or (final_epoch and not evolve):  # å¦‚æœéœ€è¦ä¿å­˜æ¨¡å‹
                ckpt = {'epoch': epoch,  # è®°å½•å½“å‰epoch
                        'best_fitness': best_fitness,  # è®°å½•æœ€ä½³é€‚åº”åº¦
                        'model': deepcopy(de_parallel(model)).half(),  # æ·±æ‹·è´æ¨¡å‹å¹¶è½¬æ¢ä¸ºåŠç²¾åº¦
                        'ema': deepcopy(ema.ema).half(),  # æ·±æ‹·è´EMAæ¨¡å‹å¹¶è½¬æ¢ä¸ºåŠç²¾åº¦
                        'updates': ema.updates,  # EMAæ›´æ–°æ¬¡æ•°
                        'optimizer': optimizer.state_dict(),  # ä¼˜åŒ–å™¨çŠ¶æ€
                        'wandb_id': loggers.wandb.wandb_run.id if loggers.wandb else None,  # å¦‚æœä½¿ç”¨wandbï¼Œè®°å½•ID
                        'date': datetime.now().isoformat()}  # å½“å‰æ—¥æœŸæ—¶é—´

                # ä¿å­˜æœ€æ–°æ¨¡å‹å’Œæœ€ä½³æ¨¡å‹ï¼Œå¹¶åˆ é™¤ä¸éœ€è¦çš„æ£€æŸ¥ç‚¹
                torch.save(ckpt, last)  # ä¿å­˜æœ€æ–°æ¨¡å‹
                if best_fitness == fi:  # å¦‚æœå½“å‰ä¸ºæœ€ä½³é€‚åº”åº¦
                    torch.save(ckpt, best)  # ä¿å­˜æœ€ä½³æ¨¡å‹
                if (epoch > 0) and (opt.save_period > 0) and (epoch % opt.save_period == 0):  # æ ¹æ®ä¿å­˜å‘¨æœŸä¿å­˜æ¨¡å‹
                    torch.save(ckpt, w / f'epoch{epoch}.pt')  # ä¿å­˜å½“å‰epochæ¨¡å‹
                del ckpt  # åˆ é™¤æ£€æŸ¥ç‚¹æ•°æ®ä»¥é‡Šæ”¾å†…å­˜
                callbacks.run('on_model_save', last, epoch, final_epoch, best_fitness, fi)  # è°ƒç”¨æ¨¡å‹ä¿å­˜çš„å›è°ƒå‡½æ•°

            # å•GPUæ¨¡å¼ä¸‹çš„æå‰åœæ­¢
            if RANK == -1 and stopper(epoch=epoch, fitness=fi):  # å¦‚æœæ˜¯å•GPUå¹¶ä¸”è§¦å‘äº†åœæ­¢æ¡ä»¶
                break  # é€€å‡ºè®­ç»ƒ

            # Stop DDP TODO: known issues shttps://github.com/ultralytics/yolov5/pull/4576
            # stop = stopper(epoch=epoch, fitness=fi)
            # if RANK == 0:
            #    dist.broadcast_object_list([stop], 0)  # broadcast 'stop' to all ranks

        # Stop DPP
        # with torch_distributed_zero_first(RANK):
        # if stop:
        #    break  # must break all DDP ranks

        # end epoch ----------------------------------------------------------------------------------------------------
    # end training -----------------------------------------------------------------------------------------------------
    if RANK in [-1, 0]:  # å¦‚æœæ˜¯ä¸»è¿›ç¨‹ï¼ˆæˆ–å•GPUï¼‰
        LOGGER.info(
            f'\n{epoch - start_epoch + 1} ä¸ªepochå·²å®Œæˆï¼Œè€—æ—¶ {(time.time() - t0) / 3600:.3f} å°æ—¶ã€‚')  # è®°å½•å·²å®Œæˆçš„epochå’Œè€—æ—¶
        for f in last, best:  # éå†æœ€æ–°æ¨¡å‹å’Œæœ€ä½³æ¨¡å‹
            if f.exists():  # å¦‚æœæ¨¡å‹æ–‡ä»¶å­˜åœ¨
                strip_optimizer(f)  # å»é™¤ä¼˜åŒ–å™¨ä¿¡æ¯
                if f is best:  # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹
                    LOGGER.info(f'\næ­£åœ¨éªŒè¯ {f}...')  # è®°å½•éªŒè¯ä¿¡æ¯
                    results, _, _ = val.run(data_dict,  # è¿›è¡ŒéªŒè¯
                                            batch_size=batch_size // WORLD_SIZE * 2,  # è®¾ç½®æ‰¹å¤§å°
                                            imgsz=imgsz,  # è¾“å…¥å›¾åƒå¤§å°
                                            model=attempt_load(f, device).half(),  # åŠ è½½æ¨¡å‹å¹¶è½¬æ¢ä¸ºåŠç²¾åº¦
                                            iou_thres=0.65 if is_coco else 0.60,  # COCOæ•°æ®é›†æœ€ä½³IOUé˜ˆå€¼
                                            single_cls=single_cls,  # å•ç±»æ£€æµ‹æ ‡å¿—
                                            dataloader=val_loader,  # éªŒè¯æ•°æ®åŠ è½½å™¨
                                            save_dir=save_dir,  # ä¿å­˜ç›®å½•
                                            save_json=is_coco,  # å¦‚æœæ˜¯COCOï¼Œä¿å­˜JSON
                                            verbose=True,  # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                                            plots=True,  # ç»˜åˆ¶å›¾è¡¨
                                            callbacks=callbacks,  # å›è°ƒå‡½æ•°
                                            compute_loss=compute_loss)  # è®¡ç®—æŸå¤±

                    if is_coco:  # å¦‚æœæ˜¯COCOæ•°æ®é›†
                        callbacks.run('on_fit_epoch_end', list(mloss) + list(results) + lr, epoch, best_fitness,
                                      fi)  # è°ƒç”¨å›è°ƒå‡½æ•°ï¼Œè®°å½•ä¿¡æ¯

        callbacks.run('on_train_end', last, best, plots, epoch, results)  # è®­ç»ƒç»“æŸæ—¶è°ƒç”¨å›è°ƒ
        # LOGGER.info(f"ç»“æœå·²ä¿å­˜åˆ° {colorstr('bold', save_dir)}")  # è®°å½•ç»“æœä¿å­˜è·¯å¾„
        print(f"ç»“æœå·²ä¿å­˜åˆ° {colorstr('bold', save_dir)}")  # è®°å½•ç»“æœä¿å­˜è·¯å¾„

    torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
    return results


def parse_opt(known=False):
    """
        å‡½æ•°åŠŸèƒ½ï¼šè®¾ç½®optå‚æ•°
    """
    parser = argparse.ArgumentParser()
    # --------------------------------------------------- å¸¸ç”¨å‚æ•° ---------------------------------------------
    parser.add_argument('--weights', type=str, default=ROOT / 'weight/yolov3.pt', help='initial weights path') # weights: æƒé‡æ–‡ä»¶
    parser.add_argument('--cfg', type=str, default='models/yolov3.yaml', help='model.yaml path')  # cfg: ç½‘ç»œæ¨¡å‹é…ç½®æ–‡ä»¶ åŒ…æ‹¬ncã€depth_multipleã€width_multipleã€anchorsã€backboneã€headç­‰
    parser.add_argument('--data', type=str, default=ROOT / 'data/you.yaml', help='dataset.yaml path') # data: å®ç°æ•°æ®é›†é…ç½®æ–‡ä»¶ åŒ…æ‹¬pathã€trainã€valã€testã€ncã€namesç­‰
    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch.yaml', help='hyperparameters path') # hyp: è®­ç»ƒæ—¶çš„è¶…å‚æ–‡ä»¶
    parser.add_argument('--epochs', type=int, default=50)  # epochs: è®­ç»ƒè½®æ¬¡
    parser.add_argument('--batch-size', type=int, default=32, help='total batch size for all GPUs, -1 for autobatch') # batch-size: è®­ç»ƒæ‰¹æ¬¡å¤§å°
    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=416, help='train, val image size (pixels)') # imgsz: è¾“å…¥ç½‘ç»œçš„å›¾ç‰‡åˆ†è¾¨ç‡å¤§å°
    parser.add_argument('--rect', action='store_true', default=True, help='rectangular training')  # rect: æ˜¯å¦é‡‡ç”¨Rectangular training/inferenceï¼Œä¸€å¼ å›¾ç‰‡ä¸ºé•¿æ–¹å½¢ï¼Œæˆ‘ä»¬åœ¨å°†å…¶é€å…¥æ¨¡å‹å‰éœ€è¦å°†å…¶resizeåˆ°è¦æ±‚çš„å°ºå¯¸ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦é€šè¿‡è¡¥ç°paddingæ¥å˜ä¸ºæ­£æ–¹å½¢çš„å›¾ã€‚
    parser.add_argument('--resume', nargs='?', const=True, default="", help='resume most recent training') # resume: æ–­ç‚¹ç»­è®­, ä»ä¸Šæ¬¡æ‰“æ–­çš„è®­ç»ƒç»“æœå¤„æ¥ç€è®­ç»ƒ  é»˜è®¤False
    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')  # nosave: ä¸ä¿å­˜æ¨¡å‹  é»˜è®¤ä¿å­˜  store_true: only test final epoch
    parser.add_argument('--noval', action='store_true', help='only validate final epoch')  # noval: åªåœ¨æœ€åä¸€æ¬¡è¿›è¡Œæµ‹è¯•ï¼Œé»˜è®¤False
    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')  # noautoanchor: ä¸è‡ªåŠ¨è°ƒæ•´anchor é»˜è®¤False(è‡ªåŠ¨è°ƒæ•´anchor)
    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations') # evolve: æ˜¯å¦è¿›è¡Œè¶…å‚è¿›åŒ–ï¼Œä½¿å¾—æ•°å€¼æ›´å¥½ é»˜è®¤False
    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')   # bucket: è°·æ­Œäº‘ç›˜bucket ä¸€èˆ¬ç”¨ä¸åˆ°
    parser.add_argument('--cache', type=str, nargs='?', const='ram', default=True, help='--cache images in "ram" (default) or "disk"')  # cache:æ˜¯å¦æå‰ç¼“å­˜å›¾ç‰‡åˆ°å†…å­˜ï¼Œä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦
    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')   #  image-weights: å¯¹äºé‚£äº›è®­ç»ƒä¸å¥½çš„å›¾ç‰‡ï¼Œä¼šåœ¨ä¸‹ä¸€è½®ä¸­å¢åŠ ä¸€äº›æƒé‡
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')  # device: è®­ç»ƒçš„è®¾å¤‡
    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')  # multi-scale: æ˜¯å¦ä½¿ç”¨å¤šå°ºåº¦è®­ç»ƒ é»˜è®¤Falseï¼Œè¦è¢«32æ•´é™¤ã€‚
    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class') # single-cls: æ•°æ®é›†æ˜¯å¦åªæœ‰ä¸€ä¸ªç±»åˆ« é»˜è®¤False
    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer') # adam: æ˜¯å¦ä½¿ç”¨adamä¼˜åŒ–å™¨
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode') # sync-bn: æ˜¯å¦ä½¿ç”¨è·¨å¡åŒæ­¥bnæ“ä½œ,å†DDPä¸­ä½¿ç”¨  é»˜è®¤False
    parser.add_argument('--workers', type=int, default=1, help='max dataloader workers (per RANK in DDP mode)')  # workers: dataloaderä¸­çš„æœ€å¤§workæ•°ï¼ˆçº¿ç¨‹ä¸ªæ•°ï¼‰
    parser.add_argument('--project', default=ROOT / 'runs/train', help='save to project/name') # project: è®­ç»ƒç»“æœä¿å­˜çš„æ ¹ç›®å½• é»˜è®¤æ˜¯runs/train
    parser.add_argument('--name', default='exp', help='save to project/name')  # name: è®­ç»ƒç»“æœä¿å­˜çš„ç›®å½• é»˜è®¤æ˜¯exp
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')  # exist_ok: æ˜¯å¦é‡æ–°åˆ›å»ºæ—¥å¿—æ–‡ä»¶, Falseæ—¶é‡æ–°åˆ›å»ºæ–‡ä»¶(é»˜è®¤æ–‡ä»¶éƒ½æ˜¯ä¸å­˜åœ¨çš„)
    parser.add_argument('--quad', action='store_true', help='quad dataloader')  # quad: dataloaderå–æ•°æ®æ—¶, æ˜¯å¦ä½¿ç”¨collate_fn4ä»£æ›¿collate_fn  é»˜è®¤False
    parser.add_argument('--linear-lr', action='store_true', help='linear LR') # linear-lrï¼šç”¨äºå¯¹å­¦ä¹ é€Ÿç‡è¿›è¡Œè°ƒæ•´ï¼Œé»˜è®¤ä¸º Falseï¼Œï¼ˆé€šè¿‡ä½™å¼¦å‡½æ•°æ¥é™ä½å­¦ä¹ ç‡ï¼‰
    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')  # label-smoothing: æ ‡ç­¾å¹³æ»‘å¢å¼º é»˜è®¤0.0ä¸å¢å¼º  è¦å¢å¼ºä¸€èˆ¬å°±è®¾ä¸º0.1
    parser.add_argument('--patience', type=int, default=1000, help='EarlyStopping patience (epochs without improvement)')  # æ—©åœæœºåˆ¶ï¼Œè®­ç»ƒåˆ°ä¸€å®šçš„epochï¼Œå¦‚æœæ¨¡å‹æ•ˆæœæœªæå‡ï¼Œå°±è®©æ¨¡å‹æå‰åœæ­¢è®­ç»ƒã€‚
    parser.add_argument('--freeze', type=int, default=0, help='Number of layers to freeze. backbone=10, all=24')  # freeze: ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è§„å®šå›ºå®šæƒé‡ä¸è¿›è¡Œè°ƒæ•´  --freeze 10  :æ„æ€ä»ç¬¬0å±‚åˆ°åˆ°ç¬¬10å±‚ä¸è®­ç»ƒ
    parser.add_argument('--save-period', type=int, default=-1, help='Save checkpoint every x epochs (disabled if < 1)') # è®¾ç½®å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify') # local_rank: rankä¸ºè¿›ç¨‹ç¼–å·  -1ä¸”gpu=1æ—¶ä¸è¿›è¡Œåˆ†å¸ƒå¼  -1ä¸”å¤šå—gpuä½¿ç”¨DataParallelæ¨¡å¼

    # --------------------------------------------------- W&B(wandb)å‚æ•° ---------------------------------------------
    parser.add_argument('--entity', default=None, help='W&B: Entity') #wandb entity é»˜è®¤None
    parser.add_argument('--upload_dataset', action='store_true', help='W&B: Upload dataset as artifact table')  # æ˜¯å¦ä¸Šä¼ datasetåˆ°wandb tabel(å°†æ•°æ®é›†ä½œä¸ºäº¤äº’å¼ dsvizè¡¨ åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ã€æŸ¥è¯¢ã€ç­›é€‰å’Œåˆ†ææ•°æ®é›†) é»˜è®¤False
    parser.add_argument('--bbox_interval', type=int, default=-1, help='W&B: Set bounding-box image logging interval') # è®¾ç½®ç•Œæ¡†å›¾åƒè®°å½•é—´éš” Set bounding-box image logging interval for W&B é»˜è®¤-1   opt.epochs // 10
    parser.add_argument('--artifact_alias', type=str, default='latest', help='W&B: Version of dataset artifact to use')

    opt = parser.parse_known_args()[0] if known else parser.parse_args()

    return opt


def main(opt, callbacks=Callbacks()):
    # 1ã€loggingå’Œwandbåˆå§‹åŒ–
    # æ—¥å¿—åˆå§‹åŒ–
    if RANK in [-1, 0]:
        print_args(FILE.stem, opt)
        check_git_status()
        check_requirements(exclude=['thop'])

    # 2ã€ä½¿ç”¨æ–­ç‚¹ç»­è®­ å°±ä»last.ptä¸­è¯»å–ç›¸å…³å‚æ•°ï¼›ä¸ä½¿ç”¨æ–­ç‚¹ç»­è®­ å°±ä»æ–‡ä»¶ä¸­è¯»å–ç›¸å…³å‚æ•°
    if opt.resume and not check_wandb_resume(opt) and not opt.evolve:  # resume an interrupted run
        # ä½¿ç”¨æ–­ç‚¹ç»­è®­ å°±ä»last.ptä¸­è¯»å–ç›¸å…³å‚æ•°
        # å¦‚æœresumeæ˜¯strï¼Œåˆ™è¡¨ç¤ºä¼ å…¥çš„æ˜¯æ¨¡å‹çš„è·¯å¾„åœ°å€
        # å¦‚æœresumeæ˜¯Trueï¼Œåˆ™é€šè¿‡get_lastest_run()å‡½æ•°æ‰¾åˆ°runsä¸ºæ–‡ä»¶å¤¹ä¸­æœ€è¿‘çš„æƒé‡æ–‡ä»¶last.pt
        ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent path
        assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'

        # ç›¸å…³çš„optå‚æ•°ä¹Ÿè¦æ›¿æ¢æˆlast.ptä¸­çš„optå‚æ•°
        with open(Path(ckpt).parent.parent / 'opt.yaml', errors='ignore') as f:
            opt = argparse.Namespace(**yaml.safe_load(f))  # replace
        opt.cfg, opt.weights, opt.resume = '', ckpt, True  # reinstate
        LOGGER.info(f'Resuming training from {ckpt}')
    else:
        # ä¸ä½¿ç”¨æ–­ç‚¹ç»­è®­ å°±ä»æ–‡ä»¶ä¸­è¯»å–ç›¸å…³å‚æ•°
        opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = \
            check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(opt.project)  # checks
        assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'
        if opt.evolve:
            opt.project = str(ROOT / 'runs/evolve')
            opt.exist_ok, opt.resume = opt.resume, False  # pass resume to exist_ok and disable resume
        # æ ¹æ®opt.projectç”Ÿæˆç›®å½•  å¦‚: runs/train/exp18
        opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))

    # 3ã€DDP modeè®¾ç½®
    # é€‰æ‹©è®¾å¤‡  cpu/cuda:0
    device = select_device(opt.device, batch_size=opt.batch_size)
    if LOCAL_RANK != -1:
        # LOCAL_RANK != -1 è¿›è¡Œå¤šGPUè®­ç»ƒ
        assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'
        assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'
        assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'
        assert not opt.evolve, '--evolve argument is not compatible with DDP training'
        torch.cuda.set_device(LOCAL_RANK)
        # æ ¹æ®GPUç¼–å·é€‰æ‹©è®¾å¤‡
        device = torch.device('cuda', LOCAL_RANK)
        # åˆå§‹åŒ–è¿›ç¨‹ç»„  distributed backend
        dist.init_process_group(backend="nccl" if dist.is_nccl_available() else "gloo")

    # 4ã€ä¸ä½¿ç”¨è¿›åŒ–ç®—æ³• æ­£å¸¸Train
    if not opt.evolve:
        # å¦‚æœä¸è¿›è¡Œè¶…å‚è¿›åŒ– é‚£ä¹ˆå°±ç›´æ¥è°ƒç”¨train()å‡½æ•°ï¼Œå¼€å§‹è®­ç»ƒ
        train(opt.hyp, opt, device, callbacks)
        # å¦‚æœæ˜¯ä½¿ç”¨å¤šå¡è®­ç»ƒ, é‚£ä¹ˆé”€æ¯è¿›ç¨‹ç»„
        if WORLD_SIZE > 1 and RANK == 0:
            LOGGER.info('Destroying process group... ')
            dist.destroy_process_group()

    # 5ã€é—ä¼ è¿›åŒ–ç®—æ³•ï¼Œè¾¹è¿›åŒ–è¾¹è®­ç»ƒ
    # å¦åˆ™ä½¿ç”¨è¶…å‚è¿›åŒ–ç®—æ³•(é—ä¼ ç®—æ³•) æ±‚å‡ºæœ€ä½³è¶…å‚ å†è¿›è¡Œè®­ç»ƒ
    else:
        # è¶…å‚è¿›åŒ–åˆ—è¡¨ (çªå˜è§„æ¨¡, æœ€å°å€¼, æœ€å¤§å€¼)
        meta = {
            'lr0': (1, 1e-5, 1e-1),  # åˆå§‹å­¦ä¹ ç‡ (SGD=1E-2, Adam=1E-3)
            'lrf': (1, 0.01, 1.0),  # æœ€ç»ˆ OneCycleLR å­¦ä¹ ç‡ (lr0 * lrf)
            'momentum': (0.3, 0.6, 0.98),  # SGD åŠ¨é‡/Adam beta1
            'weight_decay': (1, 0.0, 0.001),  # ä¼˜åŒ–å™¨æƒé‡è¡°å‡
            'warmup_epochs': (1, 0.0, 5.0),  # é¢„çƒ­è½®æ•° (å¯ä¸ºå°æ•°)
            'warmup_momentum': (1, 0.0, 0.95),  # é¢„çƒ­åˆå§‹åŠ¨é‡
            'warmup_bias_lr': (1, 0.0, 0.2),  # é¢„çƒ­åˆå§‹åç½®å­¦ä¹ ç‡
            'box': (1, 0.02, 0.2),  # æ¡†æŸå¤±å¢ç›Š
            'cls': (1, 0.2, 4.0),  # åˆ†ç±»æŸå¤±å¢ç›Š
            'cls_pw': (1, 0.5, 2.0),  # åˆ†ç±» BCELoss æ­£æƒé‡
            'obj': (1, 0.2, 4.0),  # ç›®æ ‡æŸå¤±å¢ç›Š (æ ¹æ®åƒç´ ç¼©æ”¾)
            'obj_pw': (1, 0.5, 2.0),  # ç›®æ ‡ BCELoss æ­£æƒé‡
            'iou_t': (0, 0.1, 0.7),  # IoU è®­ç»ƒé˜ˆå€¼
            'anchor_t': (1, 2.0, 8.0),  # é”šæ¡†å€æ•°é˜ˆå€¼
            'anchors': (2, 2.0, 10.0),  # æ¯ä¸ªè¾“å‡ºç½‘æ ¼çš„é”šæ¡†æ•°é‡ (0 è¡¨ç¤ºå¿½ç•¥)
            'fl_gamma': (0, 0.0, 2.0),  # ç„¦ç‚¹æŸå¤± gamma (efficientDet é»˜è®¤ gamma=1.5)
            'hsv_h': (1, 0.0, 0.1),  # å›¾åƒ HSV-è‰²è°ƒ å¢å¼º (æ¯”ä¾‹)
            'hsv_s': (1, 0.0, 0.9),  # å›¾åƒ HSV-é¥±å’Œåº¦ å¢å¼º (æ¯”ä¾‹)
            'hsv_v': (1, 0.0, 0.9),  # å›¾åƒ HSV-æ˜åº¦ å¢å¼º (æ¯”ä¾‹)
            'degrees': (1, 0.0, 45.0),  # å›¾åƒæ—‹è½¬ (+/- è§’åº¦)
            'translate': (1, 0.0, 0.9),  # å›¾åƒå¹³ç§» (+/- æ¯”ä¾‹)
            'scale': (1, 0.0, 0.9),  # å›¾åƒç¼©æ”¾ (+/- å¢ç›Š)
            'shear': (1, 0.0, 10.0),  # å›¾åƒå‰ªåˆ‡ (+/- è§’åº¦)
            'perspective': (0, 0.0, 0.001),  # å›¾åƒé€è§† (+/- æ¯”ä¾‹), èŒƒå›´ 0-0.001
            'flipud': (1, 0.0, 1.0),  # å›¾åƒä¸Šä¸‹ç¿»è½¬ (æ¦‚ç‡)
            'fliplr': (0, 0.0, 1.0),  # å›¾åƒå·¦å³ç¿»è½¬ (æ¦‚ç‡)
            'mosaic': (1, 0.0, 1.0),  # å›¾åƒæ··åˆ (æ¦‚ç‡)
            'mixup': (1, 0.0, 1.0),  # å›¾åƒæ··åˆ (æ¦‚ç‡)
            'copy_paste': (1, 0.0, 1.0)  # åˆ†å‰²å¤åˆ¶ç²˜è´´ (æ¦‚ç‡)
        }
        with open(opt.hyp, errors='ignore') as f:
            hyp = yaml.safe_load(f)  # è½½å…¥åˆå§‹è¶…å‚
            if 'anchors' not in hyp:  # å¦‚æœ hyp.yaml ä¸­æ²¡æœ‰å®šä¹‰ anchors
                hyp['anchors'] = 3  # è®¾ç½®é»˜è®¤å€¼ä¸º 3
        # è®¾ç½® opt.noval å’Œ opt.nosave ä¸º Trueï¼Œå¹¶æŒ‡å®šä¿å­˜ç›®å½•
        opt.noval, opt.nosave, save_dir = True, True, Path(opt.save_dir)  # åªåœ¨æœ€ç»ˆ epoch éªŒè¯/ä¿å­˜

        # evolvable indices ä»£ç è¢«æ³¨é‡Šæ‰äº†
        # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # å¯è¿›åŒ–çš„ç´¢å¼•
        # è¶…å‚è¿›åŒ–åæ–‡ä»¶ä¿å­˜åœ°å€
        evolve_yaml = save_dir / 'hyp_evolve.yaml'
        evolve_csv = save_dir / 'evolve.csv'
        # å¦‚æœæŒ‡å®šäº†äº‘å­˜å‚¨æ¡¶ï¼Œä¸‹è½½ evolve.csv æ–‡ä»¶
        if opt.bucket:
            os.system(f'gsutil cp gs://{opt.bucket}/evolve.csv {evolve_csv}')  # å¦‚æœå­˜åœ¨ï¼Œä¸‹è½½ evolve.csv
        """
           ä½¿ç”¨é—ä¼ ç®—æ³•è¿›è¡Œå‚æ•°è¿›åŒ– é»˜è®¤æ˜¯è¿›åŒ–300ä»£
           è¿™é‡Œçš„è¿›åŒ–ç®—æ³•æ˜¯ï¼šæ ¹æ®ä¹‹å‰è®­ç»ƒæ—¶çš„hypæ¥ç¡®å®šä¸€ä¸ªbase hypå†è¿›è¡Œçªå˜ï¼›
           å¦‚ä½•æ ¹æ®ï¼Ÿé€šè¿‡ä¹‹å‰æ¯æ¬¡è¿›åŒ–å¾—åˆ°çš„resultsæ¥ç¡®å®šä¹‹å‰æ¯ä¸ªhypçš„æƒé‡
           æœ‰äº†æ¯ä¸ªhypå’Œæ¯ä¸ªhypçš„æƒé‡ä¹‹åæœ‰ä¸¤ç§è¿›åŒ–æ–¹å¼ï¼›
               1.æ ¹æ®æ¯ä¸ªhypçš„æƒé‡éšæœºé€‰æ‹©ä¸€ä¸ªä¹‹å‰çš„hypä½œä¸ºbase hypï¼Œrandom.choices(range(n), weights=w)
               2.æ ¹æ®æ¯ä¸ªhypçš„æƒé‡å¯¹ä¹‹å‰æ‰€æœ‰çš„hypè¿›è¡Œèåˆè·å¾—ä¸€ä¸ªbase hypï¼Œ(x * w.reshape(n, 1)).sum(0) / w.sum()
           evolve.txtä¼šè®°å½•æ¯æ¬¡è¿›åŒ–ä¹‹åçš„results+hyp
           æ¯æ¬¡è¿›åŒ–æ—¶ï¼Œhypä¼šæ ¹æ®ä¹‹å‰çš„resultsè¿›è¡Œä»å¤§åˆ°å°çš„æ’åºï¼Œå†æ ¹æ®fitnesså‡½æ•°è®¡ç®—ä¹‹å‰æ¯æ¬¡è¿›åŒ–å¾—åˆ°çš„hypçš„æƒé‡ï¼Œå†ç¡®å®šå“ªä¸€ç§è¿›åŒ–æ–¹å¼ï¼Œä»è€Œè¿›è¡Œè¿›åŒ–ã€‚
        """
        for _ in range(opt.evolve):  # è¿­ä»£è¿›è¡Œè¶…å‚æ•°è¿›åŒ–
            if evolve_csv.exists():  # å¦‚æœ evolve.csv å­˜åœ¨ï¼šé€‰æ‹©æœ€ä½³è¶…å‚å¹¶è¿›è¡Œå˜å¼‚
                # é€‰æ‹©çˆ¶ä»£
                parent = 'single'  # çˆ¶ä»£é€‰æ‹©æ–¹æ³•ï¼š'single' æˆ– 'weighted'
                x = np.loadtxt(evolve_csv, ndmin=2, delimiter=',', skiprows=1)
                n = min(5, len(x))  # è€ƒè™‘çš„ä¹‹å‰ç»“æœæ•°é‡
                x = x[np.argsort(-fitness(x))][:n]  # å‰ n ä¸ªå˜å¼‚ç»“æœ
                w = fitness(x) - fitness(x).min() + 1E-6  # æƒé‡ (æ€»å’Œ > 0)
                if parent == 'single' or len(x) == 1:
                    # x = x[random.randint(0, n - 1)]  # éšæœºé€‰æ‹©
                    x = x[random.choices(range(n), weights=w)[0]]  # åŠ æƒé€‰æ‹©
                elif parent == 'weighted':
                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # åŠ æƒç»„åˆ
                # å˜å¼‚
                mp, s = 0.8, 0.2  # å˜å¼‚æ¦‚ç‡, sigma
                npr = np.random
                npr.seed(int(time.time()))
                g = np.array([meta[k][0] for k in hyp.keys()])  # å¢ç›Š 0-1
                ng = len(meta)
                v = np.ones(ng)
                while all(v == 1):  # å˜å¼‚ç›´åˆ°å‘ç”Ÿå˜åŒ– (é˜²æ­¢é‡å¤)
                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)
                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)
                    hyp[k] = float(x[i + 7] * v[i])  # å˜å¼‚
            # é™åˆ¶åœ¨èŒƒå›´å†…
            for k, v in meta.items():
                hyp[k] = max(hyp[k], v[1])  # ä¸‹é™
                hyp[k] = min(hyp[k], v[2])  # ä¸Šé™
                hyp[k] = round(hyp[k], 5)  # æœ‰æ•ˆæ•°å­—
            # è®­ç»ƒå˜å¼‚åçš„è¶…å‚
            results = train(hyp.copy(), opt, device, callbacks)
            # å†™å…¥å˜å¼‚ç»“æœ
            print_mutation(results, hyp.copy(), save_dir, opt.bucket)
        # ç»˜åˆ¶ç»“æœ
        plot_evolve(evolve_csv)
        LOGGER.info(f'Hyperparameter evolution finished\n'
                    f"Results saved to {colorstr('bold', save_dir)}\n"
                    f'Use best hyperparameters example: $ python train.py --hyp {evolve_yaml}')


def run(**kwargs):
    # Usage: import train; train.run(data='coco128.yaml', imgsz=320, weights='yolov3.pt')
    opt = parse_opt(True)
    for k, v in kwargs.items():
        setattr(opt, k, v)
    main(opt)


if __name__ == "__main__":
    opt = parse_opt()
    main(opt)

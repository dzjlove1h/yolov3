# YOLOv3 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Activation functions
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


# SiLU https://arxiv.org/pdf/1606.08415.pdf ----------------------------------------------------------------------------
class SiLU(nn.Module):  # å¯¼å‡ºå‹å¥½çš„ nn.SiLU() ç‰ˆæœ¬
    @staticmethod
    def forward(x):
        # SiLU æ¿€æ´»å‡½æ•°çš„å‰å‘ä¼ æ’­
        return x * torch.sigmoid(x)

class Hardswish(nn.Module):  # å¯¼å‡ºå‹å¥½çš„ nn.Hardswish() ç‰ˆæœ¬
    @staticmethod
    def forward(x):
        # ä½¿ç”¨ Hardtanh è¿‘ä¼¼ Hardswish æ¿€æ´»å‡½æ•°ä»¥å…¼å®¹ TorchScriptã€CoreML å’Œ ONNX
        # return x * F.hardsigmoid(x)  # å¯¹äº TorchScript å’Œ CoreML
        return x * F.hardtanh(x + 3, 0.0, 6.0) / 6.0  # å¯¹äº TorchScriptã€CoreML å’Œ ONNX

# Mish https://github.com/digantamisra98/Mish --------------------------------------------------------------------------
class Mish(nn.Module):
    @staticmethod
    def forward(x):
        # Mish æ¿€æ´»å‡½æ•°: x ä¹˜ä»¥ softplus(x) çš„åŒæ›²æ­£åˆ‡
        return x * F.softplus(x).tanh()

class MemoryEfficientMish(nn.Module):
    class F(torch.autograd.Function):
        @staticmethod
        def forward(ctx, x):
            # ä¿å­˜è¾“å…¥ x ä»¥ä¾›åå‘ä¼ æ’­ä½¿ç”¨
            ctx.save_for_backward(x)
            # è®¡ç®— Mish æ¿€æ´»å‡½æ•°: x * tanh(ln(1 + exp(x)))
            return x.mul(torch.tanh(F.softplus(x)))

        @staticmethod
        def backward(ctx, grad_output):
            # è·å–ä¿å­˜çš„è¾“å…¥ x
            x = ctx.saved_tensors[0]
            # è®¡ç®—ä¸­é—´å˜é‡
            sx = torch.sigmoid(x)
            fx = F.softplus(x).tanh()
            # è®¡ç®—æ¢¯åº¦å¹¶è¿”å›
            return grad_output * (fx + x * sx * (1 - fx * fx))

    def forward(self, x):
        # åº”ç”¨è‡ªå®šä¹‰çš„ Mish æ¿€æ´»å‡½æ•°
        return self.F.apply(x)



# FReLU https://arxiv.org/abs/2007.11824 -------------------------------------------------------------------------------
class FReLU(nn.Module):
    def __init__(self, c1, k=3):  # è¾“å…¥é€šé“æ•°, å·ç§¯æ ¸å¤§å°
        super().__init__()
        # æ·±åº¦å¯åˆ†ç¦»å·ç§¯å±‚: æ¯ä¸ªè¾“å…¥é€šé“æœ‰ä¸€ä¸ªç‹¬ç«‹çš„å·ç§¯æ ¸
        self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1, bias=False)
        # æ‰¹é‡å½’ä¸€åŒ–å±‚
        self.bn = nn.BatchNorm2d(c1)

    def forward(self, x):
        # è®¡ç®—æ·±åº¦å¯åˆ†ç¦»å·ç§¯åçš„ç»“æœï¼Œå¹¶åº”ç”¨æ‰¹é‡å½’ä¸€åŒ–
        conv_output = self.bn(self.conv(x))
        # å–è¾“å…¥å’Œå·ç§¯åçš„è¾“å‡ºçš„æœ€å¤§å€¼
        return torch.max(x, conv_output)

# ACON https://arxiv.org/pdf/2009.04759.pdf ----------------------------------------------------------------------------
class AconC(nn.Module):
    r""" ACON æ¿€æ´»å‡½æ•°ï¼ˆæ¿€æ´»æˆ–ä¸æ¿€æ´»ï¼‰ã€‚
    AconC: (p1*x - p2*x) * sigmoid(beta * (p1*x - p2*x)) + p2*xï¼Œå…¶ä¸­ beta æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚
    å‚è§è®ºæ–‡ "Activate or Not: Learning Customized Activation" <https://arxiv.org/pdf/2009.04759.pdf>ã€‚
    """

    def __init__(self, c1):
        super().__init__()
        # åˆå§‹åŒ– p1 å’Œ p2 ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œå½¢çŠ¶ä¸º (1, c1, 1, 1)
        self.p1 = nn.Parameter(torch.randn(1, c1, 1, 1))
        self.p2 = nn.Parameter(torch.randn(1, c1, 1, 1))
        # åˆå§‹åŒ– beta ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œå½¢çŠ¶ä¸º (1, c1, 1, 1)
        self.beta = nn.Parameter(torch.ones(1, c1, 1, 1))

    def forward(self, x):
        # è®¡ç®— dpx = (p1 - p2) * x
        dpx = (self.p1 - self.p2) * x
        # åº”ç”¨ ACON æ¿€æ´»å‡½æ•°
        return dpx * torch.sigmoid(self.beta * dpx) + self.p2 * x

class MetaAconC(nn.Module):
    r""" ACON æ¿€æ´»å‡½æ•°ï¼ˆæ¿€æ´»æˆ–ä¸æ¿€æ´»ï¼‰ã€‚
    MetaAconC: (p1*x - p2*x) * sigmoid(beta * (p1*x - p2*x)) + p2*xï¼Œå…¶ä¸­ beta ç”±ä¸€ä¸ªå°ç½‘ç»œç”Ÿæˆã€‚
    å‚è§è®ºæ–‡ "Activate or Not: Learning Customized Activation" <https://arxiv.org/pdf/2009.04759.pdf>ã€‚
    """

    def __init__(self, c1, k=1, s=1, r=16):  # è¾“å…¥é€šé“æ•°, å·ç§¯æ ¸å¤§å°, æ­¥å¹…, ç»´åº¦å‹ç¼©æ¯”ä¾‹
        super().__init__()
        c2 = max(r, c1 // r)  # è®¡ç®—ä¸­é—´é€šé“æ•° c2
        # åˆå§‹åŒ– p1 å’Œ p2 ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œå½¢çŠ¶ä¸º (1, c1, 1, 1)
        self.p1 = nn.Parameter(torch.randn(1, c1, 1, 1))
        self.p2 = nn.Parameter(torch.randn(1, c1, 1, 1))
        # å®šä¹‰ä¸¤ä¸ªå·ç§¯å±‚ï¼Œç”¨äºç”Ÿæˆ beta å‚æ•°
        self.fc1 = nn.Conv2d(c1, c2, k, s, bias=True)
        self.fc2 = nn.Conv2d(c2, c1, k, s, bias=True)
        # è‡ªå®šä¹‰çš„ Batch Normalization å±‚å·²è¢«æ³¨é‡Šæ‰
        # self.bn1 = nn.BatchNorm2d(c2)
        # self.bn2 = nn.BatchNorm2d(c1)

    def forward(self, x):
        # è®¡ç®—è¾“å…¥ç‰¹å¾å›¾ x çš„å‡å€¼ï¼Œä½œä¸ºç”Ÿæˆ beta çš„è¾“å…¥
        y = x.mean(dim=2, keepdims=True).mean(dim=3, keepdims=True)
        # è®¡ç®— beta å‚æ•°ï¼Œå»æ‰äº† Batch Normalization å±‚ä»¥ä¿®å¤ç¨³å®šæ€§é—®é¢˜
        beta = torch.sigmoid(self.fc2(self.fc1(y)))  # ä½¿ç”¨å°ç½‘ç»œç”Ÿæˆ beta å‚æ•°
        # è®¡ç®— ACON æ¿€æ´»å‡½æ•°
        dpx = (self.p1 - self.p2) * x
        return dpx * torch.sigmoid(beta * dpx) + self.p2 * x
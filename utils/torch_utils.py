# YOLOv3 ğŸš€ by Ultralytics, GPL-3.0 license
"""
PyTorch utils
"""
import datetime
import math
import os
import platform
import subprocess
import time
from contextlib import contextmanager
from copy import deepcopy
from pathlib import Path

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F

from utils.general import LOGGER

try:
    import thop  # for FLOPs computation
except ImportError:
    thop = None


@contextmanager
def torch_distributed_zero_first(local_rank: int):
    """
    è¿™ä¸ªå‡½æ•° torch_distributed_zero_first çš„ä¸»è¦åŠŸèƒ½æ˜¯ä½œä¸ºä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºåˆ†å¸ƒå¼è®­ç»ƒä¸­çš„è¿›ç¨‹åŒæ­¥ã€‚
    è£…é¥°å™¨ï¼Œä½¿åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æ‰€æœ‰è¿›ç¨‹åœ¨æœ¬åœ°ä¸»è¿›ç¨‹å®ŒæˆæŸé¡¹æ“ä½œä¹‹å‰ç­‰å¾…ã€‚
    å‚æ•°:
    - local_rank (int): å½“å‰è¿›ç¨‹çš„æœ¬åœ°æ’åï¼ˆ0è¡¨ç¤ºä¸»è¿›ç¨‹ï¼Œ-1è¡¨ç¤ºå•è¿›ç¨‹æ¨¡å¼ï¼‰ã€‚
    """
    if local_rank not in [-1, 0]:
        # å¦‚æœå½“å‰è¿›ç¨‹ä¸æ˜¯ä¸»è¿›ç¨‹ï¼Œç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆæ“ä½œ
        dist.barrier(device_ids=[local_rank])
    yield  # æš‚åœæ‰§è¡Œï¼Œå…è®¸ä¸»è¿›ç¨‹å®Œæˆæ“ä½œ
    if local_rank == 0:
        # å¦‚æœå½“å‰è¿›ç¨‹æ˜¯ä¸»è¿›ç¨‹ï¼Œç­‰å¾…æ‰€æœ‰è¿›ç¨‹åŒæ­¥
        dist.barrier(device_ids=[0])

def date_modified(path=__file__):
    """
    è¿™ä¸ªå‡½æ•° date_modified çš„ä¸»è¦åŠŸèƒ½æ˜¯è·å–æŒ‡å®šæ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¥æœŸï¼Œå¹¶ä»¥äººç±»å¯è¯»çš„æ ¼å¼è¿”å›ã€‚
    """
    # è¿”å›å¯è¯»çš„æ–‡ä»¶ä¿®æ”¹æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
    # è·å–æ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´æˆ³
    t = datetime.datetime.fromtimestamp(Path(path).stat().st_mtime)
    # æ ¼å¼åŒ–æ—¥æœŸå¹¶è¿”å›
    return f'{t.year}-{t.month}-{t.day}'


def git_describe(path=Path(__file__).parent):  # path å¿…é¡»æ˜¯ä¸€ä¸ªç›®å½•
    """
    è¿™ä¸ªå‡½æ•° git_describe çš„ä¸»è¦åŠŸèƒ½æ˜¯è·å–æŒ‡å®šç›®å½•ä¸‹ Git ä»“åº“çš„æè¿°ä¿¡æ¯ã€‚
    """
    # è¿”å›å¯è¯»çš„ git æè¿°ä¿¡æ¯ï¼Œä¾‹å¦‚ 'v5.0-5-g3e25f1e'
    # å‚è€ƒæ–‡æ¡£: https://git-scm.com/docs/git-describe
    s = f'git -C {path} describe --tags --long --always'  # æ„å»º git å‘½ä»¤
    try:
        # æ‰§è¡Œå‘½ä»¤å¹¶è·å–è¾“å‡º
        return subprocess.check_output(s, shell=True, stderr=subprocess.STDOUT).decode()[:-1]
    except subprocess.CalledProcessError as e:
        return ''  # å¦‚æœä¸æ˜¯ä¸€ä¸ª git ä»“åº“ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²


# å‡½æ•°æ¥æ”¶ä¸‰ä¸ªå‚æ•°ï¼šdeviceã€batch_sizeå’Œnewlineã€‚
def select_device(device='', batch_size=None, newline=True):
    # device = 'cpu' æˆ– '0' æˆ– '0,1,2,3'
    s = f'YOLOv3 ğŸš€ {git_describe() or date_modified()} torch {torch.__version__} '  # å®ƒåˆå§‹åŒ–ä¸€ä¸ªåŒ…å«ç¯å¢ƒåˆå§‹ä¿¡æ¯çš„å­—ç¬¦ä¸²sï¼ŒåŒ…æ‹¬YOLOv3ç‰ˆæœ¬å’ŒPyTorchç‰ˆæœ¬ã€‚
    device = str(device).strip().lower().replace('cuda:', '')  # å°† device å‚æ•°è½¬æ¢ä¸ºå°å†™å­—ç¬¦ä¸²ï¼Œå¹¶ç§»é™¤ cuda: å‰ç¼€ã€‚

    # åˆ¤æ–­æ˜¯å¦æ˜¯ CPU è®¾å¤‡
    cpu = device == 'cpu'
    if cpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  #å¦‚æœæ˜¯ï¼Œåˆ™è®¾ç½®ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES ä¸º -1ï¼Œå¼ºåˆ¶ç¦ç”¨CUDAã€‚
    elif device:  # å¦‚æœè¯·æ±‚é CPU è®¾å¤‡ï¼Œåˆ™è®¾ç½®ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES ä¸ºæŒ‡å®šè®¾å¤‡ï¼Œå¹¶æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨ã€‚
        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable
        assert torch.cuda.is_available(), f'CUDA unavailable, invalid device {device} requested'  # check availability

    #  å¦‚æœä½¿ç”¨CUDAè®¾å¤‡ï¼Œè·å–è®¾å¤‡åˆ—è¡¨ã€‚
    cuda = not cpu and torch.cuda.is_available()
    if cuda:
        devices = device.split(',') if device else '0'  # èŒƒå›´(torch.cuda.device_count())  # å¦‚ 0,1,6,7
        n = len(devices)  # è®¾å¤‡æ•°é‡
        if n > 1 and batch_size:   # æ£€æŸ¥ batch_size æ˜¯å¦æ˜¯è®¾å¤‡æ•°é‡çš„å€æ•°
            assert batch_size % n == 0, f'batch-size {batch_size} not multiple of GPU count {n}'
        space = ' ' * (len(s) + 1)
        # éå†è®¾å¤‡åˆ—è¡¨ï¼Œè·å–æ¯ä¸ªè®¾å¤‡çš„å±æ€§ï¼Œå¹¶å°†å…¶ä¿¡æ¯æ·»åŠ åˆ°å­—ç¬¦ä¸² s ä¸­ã€‚
        for i, d in enumerate(devices):
            p = torch.cuda.get_device_properties(i)
            s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 2:.0f}MiB)\n"  # bytes to MB
    else:
        s += 'CPU\n'
    # å¦‚æœ newlineä¸ºFalseï¼Œå»æ‰å­—ç¬¦ä¸²sæœ«å°¾çš„æ¢è¡Œç¬¦ã€‚
    if not newline:
        s = s.rstrip()
    LOGGER.info(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emoji-safe
    return torch.device('cuda:0' if cuda else 'cpu')


def time_sync():
    """
    è¿™ä¸ªå‡½æ•° time_sync çš„ä¸»è¦åŠŸèƒ½æ˜¯è·å–ä¸€ä¸ªç²¾ç¡®çš„å½“å‰æ—¶é—´ã€‚
    """
    # è¿”å›ç²¾ç¡®çš„å½“å‰æ—¶é—´ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰
    # å¦‚æœå¯ç”¨ï¼Œé¦–å…ˆåŒæ­¥ CUDA è®¾å¤‡ï¼Œä»¥ç¡®ä¿æ—¶é—´æµ‹é‡çš„å‡†ç¡®æ€§

    if torch.cuda.is_available():
        torch.cuda.synchronize()  # ç­‰å¾…æ‰€æœ‰CUDAæ“ä½œå®Œæˆ
    return time.time()  # è¿”å›å½“å‰æ—¶é—´


def profile(input, ops, n=10, device=None):
    """
    è¯¥å‡½æ•°ç”¨äºåˆ†æç»™å®šè¾“å…¥å’Œæ¨¡å‹æ“ä½œçš„æ€§èƒ½ï¼Œè®°å½•æ¨¡å‹çš„å‚æ•°æ•°é‡ã€GFLOPsï¼ˆæ¯ç§’åäº¿æ¬¡æµ®ç‚¹è¿ç®—ï¼‰ã€GPUå†…å­˜å ç”¨ã€å‰å‘å’Œåå‘ä¼ æ’­çš„å¹³å‡æ—¶é—´ã€‚
    """
    # é€Ÿåº¦/å†…å­˜/FLOPs åˆ†æå™¨
    #
    # ç”¨æ³•ç¤ºä¾‹ï¼š
    #     input = torch.randn(16, 3, 640, 640)  # ç”Ÿæˆéšæœºè¾“å…¥
    #     m1 = lambda x: x * torch.sigmoid(x)  # ç¤ºä¾‹æ“ä½œ1
    #     m2 = nn.SiLU()  # ç¤ºä¾‹æ“ä½œ2
    #     profile(input, [m1, m2], n=100)  # åœ¨100æ¬¡è¿­ä»£ä¸­è¿›è¡Œåˆ†æ

    results = []  # å­˜å‚¨åˆ†æç»“æœ
    device = device or select_device()  # é€‰æ‹©ä½¿ç”¨çš„è®¾å¤‡ï¼ˆGPU/CPUï¼‰

    # æ‰“å°è¡¨å¤´
    print(f"{'Params':>12s}{'GFLOPs':>12s}{'GPU_mem (GB)':>14s}{'forward (ms)':>14s}{'backward (ms)':>14s}"
          f"{'input':>24s}{'output':>24s}")

    # ç¡®ä¿è¾“å…¥ä¸ºåˆ—è¡¨
    for x in input if isinstance(input, list) else [input]:
        x = x.to(device)  # å°†è¾“å…¥ç§»åŠ¨åˆ°è®¾å¤‡
        x.requires_grad = True  # éœ€è¦è®¡ç®—æ¢¯åº¦

        # ç¡®ä¿æ“ä½œä¸ºåˆ—è¡¨
        for m in ops if isinstance(ops, list) else [ops]:
            m = m.to(device) if hasattr(m, 'to') else m  # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
            # å¦‚æœä½¿ç”¨åŠç²¾åº¦ä¸”è¾“å…¥ä¸ºfloat16ï¼Œè½¬æ¢æ¨¡å‹ä¸ºåŠç²¾åº¦
            m = m.half() if hasattr(m, 'half') and isinstance(x, torch.Tensor) and x.dtype is torch.float16 else m

            tf, tb, t = 0, 0, [0, 0, 0]  # åˆå§‹åŒ–å‰å‘å’Œåå‘ä¼ æ’­æ—¶é—´

            try:
                # è®¡ç®—GFLOPs
                flops = thop.profile(m, inputs=(x,), verbose=False)[0] / 1E9 * 2
            except:
                flops = 0  # å¦‚æœå¤±è´¥ï¼ŒGFLOPsè®¾ä¸º0

            try:
                # å¤šæ¬¡è¿è¡Œä»¥è·å–å¹³å‡æ—¶é—´
                for _ in range(n):
                    t[0] = time_sync()  # è®°å½•å‰å‘ä¼ æ’­å¼€å§‹æ—¶é—´
                    y = m(x)  # å‰å‘ä¼ æ’­
                    t[1] = time_sync()  # è®°å½•å‰å‘ä¼ æ’­ç»“æŸæ—¶é—´
                    try:
                        # è®¡ç®—åå‘ä¼ æ’­
                        _ = (sum(yi.sum() for yi in y) if isinstance(y, list) else y).sum().backward()
                        t[2] = time_sync()  # è®°å½•åå‘ä¼ æ’­ç»“æŸæ—¶é—´
                    except Exception:  # å¦‚æœæ²¡æœ‰åå‘ä¼ æ’­æ–¹æ³•
                        t[2] = float('nan')  # è®°å½•ä¸ºNaN

                    # è®¡ç®—æ¯æ¬¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„å¹³å‡æ—¶é—´
                    tf += (t[1] - t[0]) * 1000 / n  # å‰å‘ä¼ æ’­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
                    tb += (t[2] - t[1]) * 1000 / n  # åå‘ä¼ æ’­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

                # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆGBï¼‰
                mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0
                s_in = tuple(x.shape) if isinstance(x, torch.Tensor) else 'list'  # è¾“å…¥å½¢çŠ¶
                s_out = tuple(y.shape) if isinstance(y, torch.Tensor) else 'list'  # è¾“å‡ºå½¢çŠ¶
                # è®¡ç®—æ¨¡å‹å‚æ•°æ€»æ•°
                p = sum(x.numel() for x in m.parameters()) if isinstance(m, nn.Module) else 0

                # æ‰“å°åˆ†æç»“æœ
                print(f'{p:12}{flops:12.4g}{mem:>14.3f}{tf:14.4g}{tb:14.4g}{str(s_in):>24s}{str(s_out):>24s}')
                results.append([p, flops, mem, tf, tb, s_in, s_out])  # ä¿å­˜ç»“æœ
            except Exception as e:
                print(e)  # æ‰“å°é”™è¯¯ä¿¡æ¯
                results.append(None)  # è®°å½•ç»“æœä¸ºNone
            torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜ä»¥é‡Šæ”¾å†…å­˜

    return results  # è¿”å›ç»“æœåˆ—è¡¨


def is_parallel(model):
    # å¦‚æœæ¨¡å‹æ˜¯ DataParallelï¼ˆDPï¼‰æˆ– DistributedDataParallelï¼ˆDDPï¼‰ç±»å‹ï¼Œåˆ™è¿”å› True
    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)

def de_parallel(model):
    # å°†æ¨¡å‹å»å¹¶è¡ŒåŒ–ï¼šå¦‚æœæ¨¡å‹æ˜¯ DataParallelï¼ˆDPï¼‰æˆ– DistributedDataParallelï¼ˆDDPï¼‰ç±»å‹ï¼Œåˆ™è¿”å›å•GPUæ¨¡å‹
    return model.module if is_parallel(model) else model

# ç”¨äºåˆå§‹åŒ–æ¨¡å‹ä¸­çš„æƒé‡å’Œåç½®ã€‚è¯¥å‡½æ•°ä¼šéå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—.
# æ ¹æ®æ¨¡å—çš„ç±»å‹åº”ç”¨ä¸åŒçš„åˆå§‹åŒ–ç­–ç•¥ã€‚
def initialize_weights(model):
    # model.modules()è¿”å›æ¨¡å‹ä¸­æ‰€æœ‰æ¨¡å—çš„è¿­ä»£å™¨ã€‚
    # mä»£è¡¨å½“å‰éå†åˆ°çš„æ¨¡å—ã€‚
    for m in model.modules():
        t = type(m)
        if t is nn.Conv2d:
            # æ³¨é‡Šä¸­å»ºè®®ä½¿ç”¨ Kaimingæ­£æ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡ã€‚
            # è¿™é‡Œå®é™…çš„åˆå§‹åŒ–æ–¹æ³•è¢«æ³¨é‡Šæ‰äº†ï¼Œå¯ä»¥æ ¹æ®éœ€è¦å–æ¶ˆæ³¨é‡Šä»¥åº”ç”¨è¯¥åˆå§‹åŒ–æ–¹æ³•ã€‚
            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif t is nn.BatchNorm2d:
            # å¯¹äº nn.BatchNorm2då±‚ï¼Œè®¾ç½®epså’Œmomentumå‚æ•°ã€‚
            # eps æ˜¯ä¸€ä¸ªå°æ•°å€¼ï¼Œé˜²æ­¢åœ¨è®¡ç®—è¿‡ç¨‹ä¸­å‡ºç°é™¤ä»¥é›¶çš„æƒ…å†µï¼Œé»˜è®¤å€¼é€šå¸¸æ˜¯1e-5ã€‚
            #  momentumæ˜¯ç”¨äºè¿è¡Œæ—¶å‡å€¼å’Œæ–¹å·®è®¡ç®—çš„åŠ¨é‡ï¼Œé»˜è®¤å€¼é€šå¸¸æ˜¯0.1ã€‚
            m.eps = 1e-3
            m.momentum = 0.03
        # å¯¹äºè¿™äº›æ¿€æ´»å‡½æ•°å±‚ï¼Œè®¾ç½®inplace å‚æ•°ä¸ºTrueã€‚
        elif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:
            m.inplace = True

def find_modules(model, mclass=nn.Conv2d):
    # æ‰¾åˆ°ä¸æ¨¡å—ç±» 'mclass' åŒ¹é…çš„å±‚ç´¢å¼•
    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]

def sparsity(model):
    # è¿”å›æ¨¡å‹çš„å…¨å±€ç¨€ç–æ€§
    a, b = 0, 0
    for p in model.parameters():
        a += p.numel()  # ç´¯è®¡å‚æ•°æ€»æ•°
        b += (p == 0).sum()  # ç´¯è®¡ä¸ºé›¶çš„å‚æ•°æ•°é‡
    return b / a  # è¿”å›ç¨€ç–æ€§æ¯”ä¾‹

def prune(model, amount=0.3):
    # å¯¹æ¨¡å‹è¿›è¡Œå‰ªæï¼Œä»¥è¾¾åˆ°è¯·æ±‚çš„å…¨å±€ç¨€ç–æ€§
    import torch.nn.utils.prune as prune
    print('æ­£åœ¨å‰ªææ¨¡å‹... ', end='')
    for name, m in model.named_modules():
        if isinstance(m, nn.Conv2d):  # åªå¯¹å·ç§¯å±‚è¿›è¡Œå‰ªæ
            prune.l1_unstructured(m, name='weight', amount=amount)  # è¿›è¡ŒL1æ— ç»“æ„å‰ªæ
            prune.remove(m, 'weight')  # ä½¿å‰ªæç»“æœæ°¸ä¹…ç”Ÿæ•ˆ
    print(' %.3g å…¨å±€ç¨€ç–æ€§' % sparsity(model))  # æ‰“å°å‰ªæåçš„å…¨å±€ç¨€ç–æ€§


def fuse_conv_and_bn(conv, bn):
    # èåˆå·ç§¯å±‚å’Œæ‰¹å½’ä¸€åŒ–å±‚ https://tehnokv.com/posts/fusing-batchnorm-and-conv/
    fusedconv = nn.Conv2d(conv.in_channels,
                          conv.out_channels,
                          kernel_size=conv.kernel_size,
                          stride=conv.stride,
                          padding=conv.padding,
                          groups=conv.groups,
                          bias=True).requires_grad_(False).to(conv.weight.device)

    # å‡†å¤‡å·ç§¯å±‚æƒé‡
    w_conv = conv.weight.clone().view(conv.out_channels, -1)
    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))
    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))

    # å‡†å¤‡ç©ºé—´åç½®
    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias
    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)

    return fusedconv  # è¿”å›èåˆåçš„å·ç§¯å±‚


def model_info(model, verbose=False, img_size=640):
    # æ¨¡å‹ä¿¡æ¯ã€‚img_size å¯ä»¥æ˜¯æ•´æ•°æˆ–åˆ—è¡¨ï¼Œä¾‹å¦‚ img_size=640 æˆ– img_size=[640, 320]
    n_p = sum(x.numel() for x in model.parameters())  # å‚æ•°æ€»æ•°
    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # æ¢¯åº¦å‚æ•°æ€»æ•°
    if verbose:
        print(f"{'layer':>5} {'name':>40} {'gradient':>9} {'parameters':>12} {'shape':>20} {'mu':>10} {'sigma':>10}")
        for i, (name, p) in enumerate(model.named_parameters()):
            name = name.replace('module_list.', '')
            print('%5g %40s %9s %12g %20s %10.3g %10.3g' %
                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))

    try:  # è®¡ç®— FLOPs
        from thop import profile
        stride = max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32
        img = torch.zeros((1, model.yaml.get('ch', 3), stride, stride), device=next(model.parameters()).device)  # è¾“å…¥å¼ é‡
        flops = profile(deepcopy(model), inputs=(img,), verbose=False)[0] / 1E9 * 2  # è®¡ç®— GFLOPs
        img_size = img_size if isinstance(img_size, list) else [img_size, img_size]  # å¦‚æœæ˜¯æ•´æ•°ï¼Œæ‰©å±•ä¸ºåˆ—è¡¨
        fs = ', %.1f GFLOPs' % (flops * img_size[0] / stride * img_size[1] / stride)  # 640x640 çš„ GFLOPs
    except (ImportError, Exception):
        fs = ''  # å¦‚æœå‡ºç°é”™è¯¯ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²

    LOGGER.info(f"Model Summary: {len(list(model.modules()))} layers, {n_p} parameters, {n_g} gradients{fs}")  # è¾“å‡ºæ¨¡å‹æ‘˜è¦


"""
img: è¾“å…¥çš„å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, channels, height, width)ã€‚
ratio: ç¼©æ”¾æ¯”ä¾‹ï¼Œé»˜è®¤å€¼ä¸º 1.0ï¼Œè¡¨ç¤ºä¸ç¼©æ”¾ã€‚
same_shape: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ä¿æŒè¾“å…¥å›¾åƒçš„å½¢çŠ¶ï¼Œé»˜è®¤ä¸º Falseã€‚
gs: ç½‘æ ¼å¤§å°ï¼Œé»˜è®¤å€¼ä¸º 32ã€‚
"""
def scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)
    # å¦‚æœratioä¸º1.0ï¼Œå³ä¸è¿›è¡Œç¼©æ”¾ï¼Œç›´æ¥è¿”å›åŸå›¾åƒã€‚
    if ratio == 1.0:
        return img
    else:
        h, w = img.shape[2:]  # è·å–è¾“å…¥å›¾åƒçš„é«˜åº¦å’Œå®½åº¦ã€‚
        s = (int(h * ratio), int(w * ratio))  # è®¡ç®—ç¼©æ”¾åçš„æ–°å°ºå¯¸sï¼Œå³æ–°çš„é«˜åº¦å’Œå®½åº¦ã€‚
        img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # ä½¿ç”¨åŒçº¿æ€§æ’å€¼æ³• (bilinear) å¯¹å›¾åƒè¿›è¡Œç¼©æ”¾ï¼Œå¾—åˆ°æ–°çš„å›¾åƒå°ºå¯¸ã€‚
        if not same_shape:  # å¦‚æœ same_shapeä¸º Falseï¼Œåˆ™æ ¹æ®ç½‘æ ¼å¤§å°gsè®¡ç®—æ–°çš„é«˜åº¦å’Œå®½åº¦ï¼Œç¡®ä¿å®ƒä»¬æ˜¯gsçš„å€æ•°ã€‚
            h, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))
        return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # å¯¹å›¾åƒè¿›è¡Œå¡«å……æ“ä½œï¼Œä½¿å…¶å°ºå¯¸ç¬¦åˆæ–°çš„é«˜åº¦å’Œå®½åº¦ã€‚å¡«å……å€¼ä¸º0.447ï¼Œè¿™æ˜¯åœ¨æ•°æ®å¢å¼ºæ—¶å¸¸ç”¨çš„ç°è‰²å¡«å……å€¼ã€‚

def copy_attr(a, b, include=(), exclude=()):
    # ä» b å¤åˆ¶å±æ€§åˆ° aï¼Œå¯ä»¥é€‰æ‹©ä»…åŒ…å« [...] å’Œæ’é™¤ [...]
    for k, v in b.__dict__.items():
        if (len(include) and k not in include) or k.startswith('_') or k in exclude:
            continue  # å¦‚æœä¸åœ¨åŒ…å«åˆ—è¡¨ä¸­ï¼Œæˆ–è€…æ˜¯ç§æœ‰å±æ€§ï¼Œæˆ–è€…åœ¨æ’é™¤åˆ—è¡¨ä¸­ï¼Œåˆ™è·³è¿‡
        else:
            setattr(a, k, v)  # å°†å±æ€§å€¼è®¾ç½®åˆ° a

class EarlyStopping:
    # ç®€å•çš„æå‰åœæ­¢å™¨
    def __init__(self, patience=30):
        self.best_fitness = 0.0  # æœ€ä½³é€‚åº”åº¦ï¼Œä¾‹å¦‚ mAP
        self.best_epoch = 0  # æœ€ä½³è½®æ¬¡
        self.patience = patience or float('inf')  # åœ¨é€‚åº”åº¦åœæ­¢æ”¹å–„åç­‰å¾…çš„è½®æ¬¡
        self.possible_stop = False  # å¯èƒ½åœ¨ä¸‹ä¸€ä¸ªè½®æ¬¡åœæ­¢
    def __call__(self, epoch, fitness):
        # è°ƒç”¨æ—¶æ£€æŸ¥å½“å‰è½®æ¬¡çš„é€‚åº”åº¦
        if fitness >= self.best_fitness:  # å…è®¸é€‚åº”åº¦ä¸ºé›¶ä»¥åº”å¯¹è®­ç»ƒåˆæœŸé˜¶æ®µ
            self.best_epoch = epoch  # æ›´æ–°æœ€ä½³è½®æ¬¡
            self.best_fitness = fitness  # æ›´æ–°æœ€ä½³é€‚åº”åº¦
        delta = epoch - self.best_epoch  # æ— æ”¹è¿›çš„è½®æ¬¡
        self.possible_stop = delta >= (self.patience - 1)  # ä¸‹ä¸€ä¸ªè½®æ¬¡å¯èƒ½ä¼šåœæ­¢
        stop = delta >= self.patience  # å¦‚æœè¶…å‡ºè€å¿ƒå€¼åˆ™åœæ­¢è®­ç»ƒ
        if stop:
            LOGGER.info(f'Stopping training early as no improvement observed in last {self.patience} epochs. '
                        f'Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\n'
                        f'To update EarlyStopping(patience={self.patience}) pass a new patience value, '
                        f'i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.')
        return stop  # è¿”å›æ˜¯å¦åœæ­¢è®­ç»ƒ

class ModelEMA:
    """
    æ¨¡å‹æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œæ¥æºäº https://github.com/rwightman/pytorch-image-models
    ä¿æŒæ¨¡å‹çŠ¶æ€å­—å…¸ï¼ˆå‚æ•°å’Œç¼“å†²åŒºï¼‰ä¸­çš„ä¸€åˆ‡çš„ç§»åŠ¨å¹³å‡ã€‚
    è¿™æ˜¯ä¸ºäº†å®ç°ç±»ä¼¼äº
    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage çš„åŠŸèƒ½ã€‚
    å¹³æ»‘ç‰ˆæœ¬çš„æƒé‡å¯¹äºæŸäº›è®­ç»ƒæ–¹æ¡ˆçš„è‰¯å¥½è¡¨ç°æ˜¯å¿…è¦çš„ã€‚
    è¯¥ç±»åœ¨æ¨¡å‹åˆå§‹åŒ–ã€GPU åˆ†é…å’Œåˆ†å¸ƒå¼è®­ç»ƒåŒ…è£…å™¨çš„é¡ºåºä¸­åˆå§‹åŒ–æ—¶éå¸¸æ•æ„Ÿã€‚
    """
    def __init__(self, model, decay=0.9999, updates=0):
        # åˆ›å»º EMA
        self.ema = deepcopy(model.module if is_parallel(model) else model).eval()  # FP32 EMA
        # if next(model.parameters()).device.type != 'cpu':
        #     self.ema.half()  # FP16 EMA
        self.updates = updates  # EMA æ›´æ–°æ¬¡æ•°
        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))  # æŒ‡æ•°è¡°å‡ï¼ˆå¸®åŠ©æ—©æœŸè½®æ¬¡ï¼‰
        for p in self.ema.parameters():
            p.requires_grad_(False)  # ä¸éœ€è¦æ¢¯åº¦
    def update(self, model):
        # æ›´æ–° EMA å‚æ•°
        with torch.no_grad():
            self.updates += 1
            d = self.decay(self.updates)  # è®¡ç®—è¡°å‡å€¼

            msd = model.module.state_dict() if is_parallel(model) else model.state_dict()  # æ¨¡å‹çŠ¶æ€å­—å…¸
            for k, v in self.ema.state_dict().items():
                if v.dtype.is_floating_point:  # ä»…æ›´æ–°æµ®ç‚¹å‹å‚æ•°
                    v *= d  # æ›´æ–° EMA
                    v += (1 - d) * msd[k].detach()  # èå…¥å½“å‰æ¨¡å‹å‚æ•°
    def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):
        # æ›´æ–° EMA å±æ€§
        copy_attr(self.ema, model, include, exclude)  # ä»æ¨¡å‹å¤åˆ¶å±æ€§åˆ° EMA


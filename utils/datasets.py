# YOLOv3 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Dataloaders and dataset utils
"""

import glob
import hashlib
import json
import os
import random
import shutil
import time
from itertools import repeat
from multiprocessing.pool import Pool, ThreadPool
from pathlib import Path
from threading import Thread
from zipfile import ZipFile

import cv2
import numpy as np
import torch
import torch.nn.functional as F
import yaml
from PIL import ExifTags, Image, ImageOps
from torch.utils.data import DataLoader, Dataset, dataloader, distributed
from tqdm import tqdm

from utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective
from utils.general import (LOGGER, check_dataset, check_requirements, check_yaml, clean_str, segments2boxes, xyn2xy,
                           xywh2xyxy, xywhn2xyxy, xyxy2xywhn)
from utils.torch_utils import torch_distributed_zero_first

# Parameters
HELP_URL = 'https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data'
IMG_FORMATS = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']  # acceptable image suffixes
VID_FORMATS = ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']  # acceptable video suffixes
WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))  # DPP
NUM_THREADS = min(8, os.cpu_count())  # number of multiprocessing threads

# Get orientation exif tag
for orientation in ExifTags.TAGS.keys():
    if ExifTags.TAGS[orientation] == 'Orientation':
        break

def get_hash(paths):
    # Returns a single hash value of a list of paths (files or dirs)
    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # è®¡ç®—å­˜åœ¨è·¯å¾„çš„æ€»å¤§å°
    h = hashlib.md5(str(size).encode())  # åŸºäºå¤§å°åˆ›å»º MD5 å“ˆå¸Œ
    h.update(''.join(paths).encode())  # è¿½åŠ è·¯å¾„å­—ç¬¦ä¸²çš„å“ˆå¸Œ
    return h.hexdigest()  # è¿”å›å“ˆå¸Œå€¼

def exif_size(img):
    # Returns exif-corrected PIL size
    s = img.size  # è·å–å›¾ç‰‡çš„åŸå§‹å¤§å° (å®½åº¦, é«˜åº¦)
    try:
        # ä»å›¾ç‰‡çš„ EXIF æ•°æ®ä¸­è·å–æ–¹å‘ä¿¡æ¯
        rotation = dict(img._getexif().items())[orientation]
        if rotation == 6:  # å¦‚æœæ–¹å‘ä¸º 6ï¼Œè¡¨ç¤ºéœ€è¦æ—‹è½¬ 270 åº¦
            s = (s[1], s[0])  # äº¤æ¢å®½åº¦å’Œé«˜åº¦
        elif rotation == 8:  # å¦‚æœæ–¹å‘ä¸º 8ï¼Œè¡¨ç¤ºéœ€è¦æ—‹è½¬ 90 åº¦
            s = (s[1], s[0])  # äº¤æ¢å®½åº¦å’Œé«˜åº¦
    except:
        pass  # å¦‚æœè·å– EXIF æ•°æ®å¤±è´¥ï¼Œåˆ™ä¿æŒåŸå§‹å¤§å°

    return s  # è¿”å›è°ƒæ•´åçš„å¤§å°



def exif_transpose(image):
    """
    Transpose a PIL image accordingly if it has an EXIF Orientation tag.
    Inplace version of https://github.com/python-pillow/Pillow/blob/master/src/PIL/ImageOps.py exif_transpose()

    :param image: The image to transpose.
    :return: An image.
    """
    exif = image.getexif()  # è·å–å›¾ç‰‡çš„ EXIF æ•°æ®
    orientation = exif.get(0x0112, 1)  # è·å–æ–¹å‘ä¿¡æ¯ï¼Œé»˜è®¤ä¸º 1
    if orientation > 1:  # å¦‚æœæ–¹å‘ä¿¡æ¯å¤§äº 1
        # æ ¹æ®æ–¹å‘ä¿¡æ¯é€‰æ‹©ç›¸åº”çš„å˜æ¢æ–¹æ³•
        method = {2: Image.FLIP_LEFT_RIGHT,    # æ°´å¹³ç¿»è½¬
                  3: Image.ROTATE_180,       # æ—‹è½¬ 180 åº¦
                  4: Image.FLIP_TOP_BOTTOM,   # å‚ç›´ç¿»è½¬
                  5: Image.TRANSPOSE,         # è½¬ç½®
                  6: Image.ROTATE_270,        # æ—‹è½¬ 270 åº¦
                  7: Image.TRANSVERSE,        # åè½¬è½¬ç½®
                  8: Image.ROTATE_90,         # æ—‹è½¬ 90 åº¦
                  }.get(orientation)  # æ ¹æ®æ–¹å‘è·å–ç›¸åº”çš„æ–¹æ³•
        if method is not None:  # å¦‚æœæ‰¾åˆ°äº†å˜æ¢æ–¹æ³•
            image = image.transpose(method)  # å¯¹å›¾ç‰‡è¿›è¡Œå˜æ¢
            del exif[0x0112]  # åˆ é™¤æ–¹å‘ä¿¡æ¯
            image.info["exif"] = exif.tobytes()  # æ›´æ–°å›¾ç‰‡çš„ EXIF ä¿¡æ¯
    return image  # è¿”å›å˜æ¢åçš„å›¾ç‰‡


def create_dataloader(path, imgsz, batch_size, stride, single_cls=False, hyp=None, augment=False, cache=False, pad=0.0,
                      rect=False, rank=-1, workers=8, image_weights=False, quad=False, prefix='', shuffle=False):
    # æ£€æŸ¥çŸ©å½¢æ¨¡å¼ä¸æ‰“ä¹±æ•°æ®çš„å…¼å®¹æ€§
    if rect and shuffle:
        LOGGER.warning('WARNING: --rect is incompatible with DataLoader shuffle, setting shuffle=False')
        shuffle = False

    with torch_distributed_zero_first(rank):  # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œä»…åˆå§‹åŒ–ä¸€æ¬¡æ•°æ®é›† *.cache
        dataset = LoadImagesAndLabels(path, imgsz, batch_size,
                                       augment=augment,  # æ•°æ®å¢å¼º
                                       hyp=hyp,  # è¶…å‚æ•°
                                       rect=rect,  # çŸ©å½¢æ‰¹æ¬¡
                                       cache_images=cache,
                                       single_cls=single_cls,
                                       stride=int(stride),
                                       pad=pad,
                                       image_weights=image_weights,
                                       prefix=prefix)

    batch_size = min(batch_size, len(dataset))  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸è¶…è¿‡æ•°æ®é›†å¤§å°
    nw = min([os.cpu_count() // WORLD_SIZE, batch_size if batch_size > 1 else 0, workers])  # è®¡ç®—å·¥ä½œçº¿ç¨‹æ•°é‡
    sampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)  # åˆ›å»ºé‡‡æ ·å™¨

    # é€‰æ‹©åˆé€‚çš„ DataLoader
    loader = DataLoader if image_weights else InfiniteDataLoader  # ä»… DataLoader æ”¯æŒå±æ€§æ›´æ–°

    return loader(dataset,
                  batch_size=batch_size,
                  shuffle=shuffle and sampler is None,  # åœ¨æ²¡æœ‰é‡‡æ ·å™¨æ—¶æ‰“ä¹±æ•°æ®
                  num_workers=nw,  # å·¥ä½œçº¿ç¨‹æ•°é‡
                  sampler=sampler,  # æ•°æ®é‡‡æ ·å™¨
                  pin_memory=True,  # å›ºå®šå†…å­˜
                  collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn), dataset



class InfiniteDataLoader(dataloader.DataLoader):
    """ Dataloader that reuses workers

    Uses same syntax as vanilla DataLoader
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)  # åˆå§‹åŒ–çˆ¶ç±» DataLoader
        # è®¾ç½®ä¸€ä¸ªé‡å¤é‡‡æ ·å™¨ï¼Œç¡®ä¿å¯ä»¥æ— é™æ¬¡ä½¿ç”¨æ•°æ®
        object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))
        self.iterator = super().__iter__()  # è·å–è¿­ä»£å™¨

    def __len__(self):
        return len(self.batch_sampler.sampler)  # è¿”å›æ ·æœ¬æ€»æ•°

    def __iter__(self):
        # æ— é™è¿­ä»£ï¼Œé‡æ–°ç”Ÿæˆè¿­ä»£å™¨
        for i in range(len(self)):
            yield next(self.iterator)  # è¿”å›ä¸‹ä¸€ä¸ªæ ·æœ¬



class _RepeatSampler:
    """ Sampler that repeats forever

    Args:
        sampler (Sampler): è¦é‡å¤çš„é‡‡æ ·å™¨
    """

    def __init__(self, sampler):
        self.sampler = sampler  # ä¿å­˜ä¼ å…¥çš„é‡‡æ ·å™¨

    def __iter__(self):
        # æ— é™è¿­ä»£ï¼Œé‡å¤è¿”å›é‡‡æ ·å™¨ä¸­çš„å…ƒç´ 
        while True:
            yield from iter(self.sampler)  # é€ä¸ªç”Ÿæˆé‡‡æ ·å™¨ä¸­çš„å…ƒç´ 


class LoadImages:
    # å›¾åƒ/è§†é¢‘æ•°æ®åŠ è½½å™¨ï¼Œä¾‹å¦‚ `python detect.py --source image.jpg/vid.mp4`

    def __init__(self, path, img_size=640, stride=32, auto=True):
        # è§£æå¹¶å‡†å¤‡æ–‡ä»¶è·¯å¾„
        p = str(Path(path).resolve())  # è·å–å¹³å°æ— å…³çš„ç»å¯¹è·¯å¾„
        if '*' in p:
            files = sorted(glob.glob(p, recursive=True))  # ä½¿ç”¨ glob è·å–åŒ¹é…çš„æ–‡ä»¶
        elif os.path.isdir(p):
            files = sorted(glob.glob(os.path.join(p, '*.*')))  # è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
        elif os.path.isfile(p):
            files = [p]  # å•ä¸ªæ–‡ä»¶
        else:
            raise Exception(f'ERROR: {p} does not exist')  # æ–‡ä»¶æˆ–ç›®å½•ä¸å­˜åœ¨

        # æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ†ç±»å›¾åƒå’Œè§†é¢‘
        images = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]
        videos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]
        ni, nv = len(images), len(videos)  # ç»Ÿè®¡å›¾åƒå’Œè§†é¢‘çš„æ•°é‡

        self.img_size = img_size  # è®¾ç½®å›¾åƒå¤§å°
        self.stride = stride  # è®¾ç½®æ­¥é•¿
        self.files = images + videos  # åˆå¹¶å›¾åƒå’Œè§†é¢‘æ–‡ä»¶åˆ—è¡¨
        self.nf = ni + nv  # æ–‡ä»¶æ€»æ•°
        self.video_flag = [False] * ni + [True] * nv  # è§†é¢‘æ ‡å¿—åˆ—è¡¨
        self.mode = 'image'  # å½“å‰æ¨¡å¼ï¼Œé»˜è®¤ä¸ºå›¾åƒ
        self.auto = auto  # æ˜¯å¦è‡ªåŠ¨è°ƒæ•´
        if any(videos):
            self.new_video(videos[0])  # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªè§†é¢‘
        else:
            self.cap = None  # å¦‚æœæ²¡æœ‰è§†é¢‘ï¼Œè®¾ç½®ä¸º None

        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ–‡ä»¶
        assert self.nf > 0, f'No images or videos found in {p}. ' \
                            f'Supported formats are:\nimages: {IMG_FORMATS}\nvideos: {VID_FORMATS}'

    def __iter__(self):
        self.count = 0  # é‡ç½®è®¡æ•°å™¨
        return self

    def __next__(self):
        if self.count == self.nf:
            raise StopIteration  # è¾¾åˆ°æ–‡ä»¶æœ«å°¾ï¼Œåœæ­¢è¿­ä»£
        path = self.files[self.count]  # è·å–å½“å‰æ–‡ä»¶è·¯å¾„

        if self.video_flag[self.count]:
            # è¯»å–è§†é¢‘å¸§
            self.mode = 'video'  # è®¾ç½®æ¨¡å¼ä¸ºè§†é¢‘
            ret_val, img0 = self.cap.read()  # è¯»å–è§†é¢‘å¸§
            if not ret_val:  # å¦‚æœæœªèƒ½è¯»å–å¸§
                self.count += 1  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ–‡ä»¶
                self.cap.release()  # é‡Šæ”¾å½“å‰è§†é¢‘æ•è·å¯¹è±¡
                if self.count == self.nf:  # å¦‚æœæ˜¯æœ€åä¸€ä¸ªè§†é¢‘
                    raise StopIteration
                else:
                    path = self.files[self.count]  # è·å–ä¸‹ä¸€ä¸ªæ–‡ä»¶è·¯å¾„
                    self.new_video(path)  # åˆå§‹åŒ–æ–°è§†é¢‘
                    ret_val, img0 = self.cap.read()  # è¯»å–æ–°è§†é¢‘å¸§

            self.frame += 1  # å¸§è®¡æ•°å¢åŠ 
            s = f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: '

        else:
            # è¯»å–å›¾åƒ
            self.count += 1  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ–‡ä»¶
            img0 = cv2.imread(path)  # ä½¿ç”¨ OpenCV è¯»å–å›¾åƒï¼ˆBGRæ ¼å¼ï¼‰
            assert img0 is not None, f'Image Not Found {path}'  # æ£€æŸ¥å›¾åƒæ˜¯å¦æœ‰æ•ˆ
            s = f'image {self.count}/{self.nf} {path}: '

        # å¡«å……è°ƒæ•´å¤§å°
        img = letterbox(img0, self.img_size, stride=self.stride, auto=self.auto)[0]

        # è½¬æ¢æ ¼å¼
        img = img.transpose((2, 0, 1))[::-1]  # ä» HWC è½¬æ¢ä¸º CHWï¼Œå¹¶ä» BGR è½¬ä¸º RGB
        img = np.ascontiguousarray(img)  # ç¡®ä¿æ•°ç»„æ˜¯è¿ç»­çš„

        return path, img, img0, self.cap, s  # è¿”å›è·¯å¾„ã€å¤„ç†åçš„å›¾åƒã€åŸå§‹å›¾åƒã€è§†é¢‘æ•è·å¯¹è±¡å’ŒçŠ¶æ€ä¿¡æ¯

    def new_video(self, path):
        self.frame = 0  # é‡ç½®å¸§è®¡æ•°
        self.cap = cv2.VideoCapture(path)  # åˆå§‹åŒ–è§†é¢‘æ•è·
        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))  # è·å–è§†é¢‘æ€»å¸§æ•°

    def __len__(self):
        return self.nf  # è¿”å›æ–‡ä»¶æ€»æ•°


class LoadWebcam:  # ç”¨äºæ¨ç†
    # æœ¬åœ°æ‘„åƒå¤´æ•°æ®åŠ è½½å™¨ï¼Œä¾‹å¦‚ `python detect.py --source 0`
    def __init__(self, pipe='0', img_size=640, stride=32):
        self.img_size = img_size  # è®¾ç½®å›¾åƒå¤§å°
        self.stride = stride  # è®¾ç½®æ­¥é•¿
        self.pipe = eval(pipe) if pipe.isnumeric() else pipe  # è§£æç®¡é“å‚æ•°
        self.cap = cv2.VideoCapture(self.pipe)  # åˆ›å»ºè§†é¢‘æ•è·å¯¹è±¡
        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)  # è®¾ç½®ç¼“å†²åŒºå¤§å°

    def __iter__(self):
        self.count = -1  # åˆå§‹åŒ–è®¡æ•°å™¨
        return self

    def __next__(self):
        self.count += 1  # å¢åŠ è®¡æ•°å™¨
        if cv2.waitKey(1) == ord('q'):  # å¦‚æœæŒ‰ä¸‹ 'q' é”®ï¼Œåˆ™é€€å‡º
            self.cap.release()  # é‡Šæ”¾è§†é¢‘æ•è·å¯¹è±¡
            cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰çª—å£
            raise StopIteration  # åœæ­¢è¿­ä»£

        # è¯»å–å¸§
        ret_val, img0 = self.cap.read()  # ä»æ‘„åƒå¤´è¯»å–å›¾åƒ
        img0 = cv2.flip(img0, 1)  # æ°´å¹³ç¿»è½¬å›¾åƒ

        # æ‰“å°çŠ¶æ€
        assert ret_val, f'Camera Error {self.pipe}'  # æ£€æŸ¥æ‘„åƒå¤´æ˜¯å¦æ­£å¸¸
        img_path = 'webcam.jpg'  # è®¾ç½®å›¾åƒè·¯å¾„
        s = f'webcam {self.count}: '  # çŠ¶æ€ä¿¡æ¯

        # å¡«å……è°ƒæ•´å¤§å°
        img = letterbox(img0, self.img_size, stride=self.stride)[0]

        # è½¬æ¢æ ¼å¼
        img = img.transpose((2, 0, 1))[::-1]  # ä» HWC è½¬æ¢ä¸º CHWï¼Œå¹¶ä» BGR è½¬ä¸º RGB
        img = np.ascontiguousarray(img)  # ç¡®ä¿æ•°ç»„æ˜¯è¿ç»­çš„

        return img_path, img, img0, None, s  # è¿”å›å›¾åƒè·¯å¾„ã€å¤„ç†åçš„å›¾åƒã€åŸå§‹å›¾åƒã€None å’ŒçŠ¶æ€ä¿¡æ¯

    def __len__(self):
        return 0  # è¿”å› 0ï¼Œè¡¨ç¤ºæ— é™å¾ªç¯


class LoadStreams:
    # æµåŠ è½½å™¨ï¼Œä¾‹å¦‚ `python detect.py --source 'rtsp://example.com/media.mp4'`  # æ”¯æŒ RTSPã€RTMPã€HTTP æµ

    def __init__(self, sources='streams.txt', img_size=640, stride=32, auto=True):
        self.mode = 'stream'  # è®¾ç½®æ¨¡å¼ä¸ºæµ
        self.img_size = img_size  # è®¾ç½®å›¾åƒå¤§å°
        self.stride = stride  # è®¾ç½®æ­¥é•¿

        # å¤„ç†è¾“å…¥æºï¼Œè¯»å–æ–‡ä»¶æˆ–ç›´æ¥ä½¿ç”¨æºå­—ç¬¦ä¸²
        if os.path.isfile(sources):
            with open(sources) as f:
                sources = [x.strip() for x in f.read().strip().splitlines() if len(x.strip())]  # ä»æ–‡ä»¶è¯»å–æº
        else:
            sources = [sources]  # å°†æºè®¾ç½®ä¸ºå•ä¸ªå­—ç¬¦ä¸²

        n = len(sources)  # æºçš„æ•°é‡
        # åˆå§‹åŒ–å›¾åƒã€FPSã€å¸§æ•°å’Œçº¿ç¨‹çš„åˆ—è¡¨
        self.imgs, self.fps, self.frames, self.threads = [None] * n, [0] * n, [0] * n, [None] * n
        self.sources = [clean_str(x) for x in sources]  # æ¸…ç†æºåç§°ä»¥ä¾¿åç»­ä½¿ç”¨
        self.auto = auto  # è‡ªåŠ¨è°ƒæ•´æ ‡å¿—

        for i, s in enumerate(sources):  # éå†æ¯ä¸ªæº
            # å¯åŠ¨çº¿ç¨‹ä»¥ä»è§†é¢‘æµè¯»å–å¸§
            st = f'{i + 1}/{n}: {s}... '
            if 'youtube.com/' in s or 'youtu.be/' in s:  # å¦‚æœæºæ˜¯ YouTube è§†é¢‘
                check_requirements(('pafy', 'youtube_dl'))  # æ£€æŸ¥æ‰€éœ€åº“
                import pafy
                s = pafy.new(s).getbest(preftype="mp4").url  # è·å–æœ€ä½³ YouTube URL
            s = eval(s) if s.isnumeric() else s  # å¤„ç†æœ¬åœ°æ‘„åƒå¤´æº
            cap = cv2.VideoCapture(s)  # åˆ›å»ºè§†é¢‘æ•è·å¯¹è±¡
            assert cap.isOpened(), f'{st}Failed to open {s}'  # ç¡®ä¿æˆåŠŸæ‰“å¼€æµ
            w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # è·å–æµçš„å®½åº¦
            h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # è·å–æµçš„é«˜åº¦
            self.fps[i] = max(cap.get(cv2.CAP_PROP_FPS) % 100, 0) or 30.0  # è·å– FPSï¼Œè‹¥æ— æ³•è·å–åˆ™é»˜è®¤ä¸º 30 FPS
            self.frames[i] = max(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), 0) or float('inf')  # è·å–å¸§æ•°ï¼Œé»˜è®¤ä¸ºæ— é™æµ

            _, self.imgs[i] = cap.read()  # ç¡®ä¿è¯»å–ç¬¬ä¸€å¸§
            self.threads[i] = Thread(target=self.update, args=([i, cap, s]), daemon=True)  # åˆ›å»ºçº¿ç¨‹ä»¥æ›´æ–°å¸§
            LOGGER.info(f"{st} Success ({self.frames[i]} frames {w}x{h} at {self.fps[i]:.2f} FPS)")  # æ—¥å¿—è¾“å‡º
            self.threads[i].start()  # å¯åŠ¨çº¿ç¨‹
        LOGGER.info('')  # è¾“å‡ºæ¢è¡Œ

        # æ£€æŸ¥å›¾åƒå½¢çŠ¶æ˜¯å¦ä¸€è‡´
        s = np.stack([letterbox(x, self.img_size, stride=self.stride, auto=self.auto)[0].shape for x in self.imgs])
        self.rect = np.unique(s, axis=0).shape[0] == 1  # å¦‚æœæ‰€æœ‰å½¢çŠ¶ç›¸åŒï¼Œåˆ™è¿›è¡ŒçŸ©å½¢æ¨ç†
        if not self.rect:
            LOGGER.warning('WARNING: Stream shapes differ. For optimal performance supply similarly-shaped streams.')

    def update(self, i, cap, stream):
        # åœ¨å®ˆæŠ¤çº¿ç¨‹ä¸­è¯»å–æµ `i` çš„å¸§
        n, f, read = 0, self.frames[i], 1  # å¸§è®¡æ•°ã€å¸§æ•°ç»„ã€æ¯ 'read' å¸§æ¨ç†ä¸€æ¬¡
        while cap.isOpened() and n < f:  # å¾ªç¯ç›´åˆ°æµå…³é—­æˆ–è¯»å–å®Œå¸§
            n += 1
            cap.grab()  # æŠ“å–ä¸‹ä¸€å¸§
            if n % read == 0:  # æ¯ 'read' å¸§è¿›è¡Œä¸€æ¬¡è¯»å–
                success, im = cap.retrieve()  # å°è¯•è·å–å¸§
                if success:
                    self.imgs[i] = im  # æ›´æ–°å›¾åƒ
                else:
                    LOGGER.warning('WARNING: Video stream unresponsive, please check your IP camera connection.')
                    self.imgs[i] *= 0  # å¦‚æœå¤±è´¥ï¼Œå°†å›¾åƒç½®ä¸º 0
                    cap.open(stream)  # é‡æ–°æ‰“å¼€æµ
            time.sleep(1 / self.fps[i])  # æ ¹æ® FPS ç­‰å¾…

    def __iter__(self):
        self.count = -1  # åˆå§‹åŒ–è®¡æ•°å™¨
        return self

    def __next__(self):
        self.count += 1
        if not all(x.is_alive() for x in self.threads) or cv2.waitKey(1) == ord('q'):  # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦å­˜æ´»ï¼Œæˆ–æŒ‰ 'q' é”®é€€å‡º
            cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰çª—å£
            raise StopIteration  # åœæ­¢è¿­ä»£

        # è¿›è¡Œå¡«å……è°ƒæ•´
        img0 = self.imgs.copy()  # å¤åˆ¶å½“å‰å›¾åƒ
        img = [letterbox(x, self.img_size, stride=self.stride, auto=self.rect and self.auto)[0] for x in img0]

        # å †å å›¾åƒ
        img = np.stack(img, 0)

        # è½¬æ¢æ ¼å¼
        img = img[..., ::-1].transpose((0, 3, 1, 2))  # BGR è½¬ä¸º RGBï¼Œä» BHWC è½¬ä¸º BCHW
        img = np.ascontiguousarray(img)  # ç¡®ä¿æ•°ç»„æ˜¯è¿ç»­çš„

        return self.sources, img, img0, None, ''  # è¿”å›æºã€å¤„ç†åçš„å›¾åƒã€åŸå§‹å›¾åƒã€None å’Œç©ºå­—ç¬¦ä¸²

    def __len__(self):
        return len(self.sources)  # è¿”å›æºçš„æ•°é‡



def img2label_paths(img_paths):
    # æ ¹æ®å›¾åƒè·¯å¾„å®šä¹‰æ ‡ç­¾è·¯å¾„
    sa, sb = os.sep + 'images' + os.sep, os.sep + 'labels' + os.sep  # å®šä¹‰ /images/ å’Œ /labels/ çš„å­å­—ç¬¦ä¸²
    return [sb.join(x.rsplit(sa, 1)).rsplit('.', 1)[0] + '.txt' for x in img_paths]  # å°†å›¾åƒè·¯å¾„è½¬æ¢ä¸ºæ ‡ç­¾è·¯å¾„


class LoadImagesAndLabels(Dataset):
    # è®­ç»ƒåŠ è½½å™¨/éªŒè¯åŠ è½½å™¨ï¼Œç”¨äºåŠ è½½å›¾åƒå’Œæ ‡ç­¾
    cache_version = 0.6  # æ•°æ®é›†æ ‡ç­¾ç¼“å­˜ç‰ˆæœ¬

    def __init__(self, path, img_size=640, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,
                 cache_images=False, single_cls=False, stride=32, pad=0.0, prefix=''):
        # åˆå§‹åŒ–å‚æ•°
        self.img_size = img_size  # å›¾åƒå¤§å°
        self.augment = augment  # æ˜¯å¦è¿›è¡Œæ•°æ®å¢å¼º
        self.hyp = hyp  # è¶…å‚æ•°
        self.image_weights = image_weights  # æ˜¯å¦ä½¿ç”¨å›¾åƒæƒé‡
        self.rect = False if image_weights else rect  # æ˜¯å¦è¿›è¡ŒçŸ©å½¢è®­ç»ƒ
        self.mosaic = self.augment and not self.rect  # è®­ç»ƒæ—¶æ˜¯å¦ä½¿ç”¨é©¬èµ›å…‹å¢å¼º
        self.mosaic_border = [-img_size // 2, -img_size // 2]  # é©¬èµ›å…‹è¾¹ç•Œ
        self.stride = stride  # æ­¥å¹…
        self.path = path  # æ•°æ®è·¯å¾„
        self.albumentations = Albumentations() if augment else None  # å¦‚æœå¯ç”¨å¢å¼ºï¼Œåˆ™åˆå§‹åŒ–Albumentations

        try:
            f = []  # å›¾åƒæ–‡ä»¶åˆ—è¡¨
            for p in path if isinstance(path, list) else [path]:
                p = Path(p)  # ä½¿è·¯å¾„å¹³å°æ— å…³
                if p.is_dir():  # å¦‚æœæ˜¯ç›®å½•
                    f += glob.glob(str(p / '**' / '*.*'), recursive=True)  # é€’å½’è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
                elif p.is_file():  # å¦‚æœæ˜¯æ–‡ä»¶
                    with open(p) as t:
                        t = t.read().strip().splitlines()  # è¯»å–æ–‡ä»¶å†…å®¹
                        parent = str(p.parent) + os.sep
                        f += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # æ›¿æ¢è·¯å¾„
                else:
                    raise Exception(f'{prefix}{p} does not exist')  # æŠ›å‡ºå¼‚å¸¸
            self.img_files = sorted(x.replace('/', os.sep) for x in f if x.split('.')[-1].lower() in IMG_FORMATS)
            # ç­›é€‰æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶
            assert self.img_files, f'{prefix}No images found'  # ç¡®ä¿æ‰¾åˆ°å›¾åƒæ–‡ä»¶
        except Exception as e:
            raise Exception(f'{prefix}Error loading data from {path}: {e}\nSee {HELP_URL}')  # é”™è¯¯å¤„ç†

        # æ£€æŸ¥ç¼“å­˜
        self.label_files = img2label_paths(self.img_files)  # è·å–æ ‡ç­¾æ–‡ä»¶è·¯å¾„
        cache_path = (p if p.is_file() else Path(self.label_files[0]).parent).with_suffix('.cache')
        try:
            cache, exists = np.load(cache_path, allow_pickle=True).item(), True  # åŠ è½½ç¼“å­˜
            assert cache['version'] == self.cache_version  # ç¡®ä¿ç‰ˆæœ¬ä¸€è‡´
            assert cache['hash'] == get_hash(self.label_files + self.img_files)  # ç¡®ä¿å“ˆå¸Œä¸€è‡´
        except:
            cache, exists = self.cache_labels(cache_path, prefix), False  # å¦‚æœç¼“å­˜æ— æ•ˆï¼Œåˆ™é‡æ–°ç¼“å­˜

        # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
        nf, nm, ne, nc, n = cache.pop('results')  # è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        if exists:
            d = f"Scanning '{cache_path}' images and labels... {nf} found, {nm} missing, {ne} empty, {nc} corrupted"
            tqdm(None, desc=prefix + d, total=n, initial=n)  # æ˜¾ç¤ºç¼“å­˜ç»“æœ
            if cache['msgs']:
                LOGGER.info('\n'.join(cache['msgs']))  # æ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
        assert nf > 0 or not augment, f'{prefix}No labels in {cache_path}. Can not train without labels. See {HELP_URL}'  # ç¡®ä¿æœ‰æ ‡ç­¾

        # è¯»å–ç¼“å­˜
        [cache.pop(k) for k in ('hash', 'version', 'msgs')]  # ç§»é™¤ä¸å¿…è¦çš„é¡¹
        labels, shapes, self.segments = zip(*cache.values())  # è§£å‹æ ‡ç­¾å’Œå½¢çŠ¶
        self.labels = list(labels)  # æ ‡ç­¾åˆ—è¡¨
        self.shapes = np.array(shapes, dtype=np.float64)  # å½¢çŠ¶æ•°ç»„
        self.img_files = list(cache.keys())  # æ›´æ–°å›¾åƒæ–‡ä»¶
        self.label_files = img2label_paths(cache.keys())  # æ›´æ–°æ ‡ç­¾æ–‡ä»¶
        n = len(shapes)  # å›¾åƒæ•°é‡
        bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # è®¡ç®—æ‰¹æ¬¡ç´¢å¼•
        nb = bi[-1] + 1  # æ‰¹æ¬¡æ•°
        self.batch = bi  # è®°å½•æ‰¹æ¬¡ç´¢å¼•
        self.n = n  # å›¾åƒæ€»æ•°
        self.indices = range(n)  # ç´¢å¼•èŒƒå›´

        # æ›´æ–°æ ‡ç­¾
        include_class = []  # è¿‡æ»¤æ ‡ç­¾ä»¥ä»…åŒ…å«è¿™äº›ç±»ï¼ˆå¯é€‰ï¼‰
        include_class_array = np.array(include_class).reshape(1, -1)  # è½¬æ¢ä¸ºæ•°ç»„
        for i, (label, segment) in enumerate(zip(self.labels, self.segments)):
            if include_class:  # å¦‚æœæŒ‡å®šäº†ç±»
                j = (label[:, 0:1] == include_class_array).any(1)  # è¿‡æ»¤ç±»
                self.labels[i] = label[j]
                if segment:
                    self.segments[i] = segment[j]
            if single_cls:  # å•ç±»è®­ç»ƒï¼Œå°†æ‰€æœ‰ç±»åˆå¹¶ä¸º0
                self.labels[i][:, 0] = 0
                if segment:
                    self.segments[i][:, 0] = 0

        # çŸ©å½¢è®­ç»ƒ
        if self.rect:
            # æŒ‰é•¿å®½æ¯”æ’åº
            s = self.shapes  # å½¢çŠ¶
            ar = s[:, 1] / s[:, 0]  # è®¡ç®—é•¿å®½æ¯”
            irect = ar.argsort()  # æ’åºç´¢å¼•
            self.img_files = [self.img_files[i] for i in irect]  # æ›´æ–°å›¾åƒæ–‡ä»¶
            self.label_files = [self.label_files[i] for i in irect]  # æ›´æ–°æ ‡ç­¾æ–‡ä»¶
            self.labels = [self.labels[i] for i in irect]  # æ›´æ–°æ ‡ç­¾
            self.shapes = s[irect]  # æ›´æ–°å½¢çŠ¶
            ar = ar[irect]  # æ›´æ–°é•¿å®½æ¯”

            # è®¾ç½®è®­ç»ƒå›¾åƒçš„å½¢çŠ¶
            shapes = [[1, 1]] * nb
            for i in range(nb):
                ari = ar[bi == i]  # è·å–å½“å‰æ‰¹æ¬¡çš„é•¿å®½æ¯”
                mini, maxi = ari.min(), ari.max()  # æœ€å°å’Œæœ€å¤§é•¿å®½æ¯”
                if maxi < 1:
                    shapes[i] = [maxi, 1]  # è®¾ç½®å½¢çŠ¶
                elif mini > 1:
                    shapes[i] = [1, 1 / mini]  # è®¾ç½®å½¢çŠ¶

            self.batch_shapes = np.ceil(np.array(shapes) * img_size / stride + pad).astype(np.int) * stride  # è®¡ç®—æ‰¹æ¬¡å½¢çŠ¶

        # å°†å›¾åƒç¼“å­˜åˆ°å†…å­˜ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ˆè­¦å‘Šï¼šå¤§å‹æ•°æ®é›†å¯èƒ½è¶…è¿‡ç³»ç»Ÿå†…å­˜ï¼‰
        self.imgs, self.img_npy = [None] * n, [None] * n
        if cache_images:
            if cache_images == 'disk':  # å¦‚æœç¼“å­˜åˆ°ç£ç›˜
                self.im_cache_dir = Path(Path(self.img_files[0]).parent.as_posix() + '_npy')  # ç¼“å­˜ç›®å½•
                self.img_npy = [self.im_cache_dir / Path(f).with_suffix('.npy').name for f in self.img_files]  # ç¼“å­˜æ–‡ä»¶è·¯å¾„
                self.im_cache_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç¼“å­˜ç›®å½•
            gb = 0  # ç¼“å­˜å›¾åƒçš„å¤§å°ï¼ˆGBï¼‰
            self.img_hw0, self.img_hw = [None] * n, [None] * n  # åŸå§‹å’Œè°ƒæ•´åçš„å›¾åƒå°ºå¯¸
            results = ThreadPool(NUM_THREADS).imap(lambda x: load_image(*x), zip(repeat(self), range(n)))  # å¤šçº¿ç¨‹åŠ è½½å›¾åƒ
            pbar = tqdm(enumerate(results), total=n)  # æ˜¾ç¤ºè¿›åº¦æ¡
            for i, x in pbar:
                if cache_images == 'disk':
                    if not self.img_npy[i].exists():  # å¦‚æœç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨
                        np.save(self.img_npy[i].as_posix(), x[0])  # ä¿å­˜åˆ°ç£ç›˜
                    gb += self.img_npy[i].stat().st_size  # æ›´æ–°ç¼“å­˜å¤§å°
                else:
                    self.imgs[i], self.img_hw0[i], self.img_hw[i] = x  # åŠ è½½å›¾åƒåŠå…¶å°ºå¯¸
                    gb += self.imgs[i].nbytes  # æ›´æ–°ç¼“å­˜å¤§å°
                pbar.desc = f'{prefix}Caching images ({gb / 1E9:.1f}GB {cache_images})'  # æ›´æ–°è¿›åº¦æè¿°
            pbar.close()  # å…³é—­è¿›åº¦æ¡

    def cache_labels(self, path=Path('./labels.cache'), prefix=''):
        # ç¼“å­˜æ•°æ®é›†æ ‡ç­¾ï¼Œæ£€æŸ¥å›¾åƒå¹¶è¯»å–å½¢çŠ¶
        x = {}  # åˆå§‹åŒ–å­—å…¸
        nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # è®¡æ•°ï¼šç¼ºå¤±ã€æ‰¾åˆ°ã€ç©ºã€æŸåçš„æ ‡ç­¾åŠæ¶ˆæ¯
        desc = f"{prefix}Scanning '{path.parent / path.stem}' images and labels..."  # æè¿°ä¿¡æ¯
        with Pool(NUM_THREADS) as pool:  # åˆ›å»ºå¤šçº¿ç¨‹æ± 
            pbar = tqdm(pool.imap(verify_image_label, zip(self.img_files, self.label_files, repeat(prefix))),
                        desc=desc, total=len(self.img_files))  # æ˜¾ç¤ºè¿›åº¦æ¡
            for im_file, l, shape, segments, nm_f, nf_f, ne_f, nc_f, msg in pbar:
                # æ›´æ–°è®¡æ•°
                nm += nm_f
                nf += nf_f
                ne += ne_f
                nc += nc_f
                if im_file:  # å¦‚æœæ‰¾åˆ°å›¾åƒæ–‡ä»¶
                    x[im_file] = [l, shape, segments]  # ä¿å­˜æ–‡ä»¶ä¿¡æ¯
                if msg:  # å¦‚æœæœ‰æ¶ˆæ¯
                    msgs.append(msg)  # è®°å½•æ¶ˆæ¯
                pbar.desc = f"{desc}{nf} found, {nm} missing, {ne} empty, {nc} corrupted"  # æ›´æ–°è¿›åº¦æè¿°

        pbar.close()  # å…³é—­è¿›åº¦æ¡
        if msgs:  # å¦‚æœæœ‰æ¶ˆæ¯
            LOGGER.info('\n'.join(msgs))  # è®°å½•æ¶ˆæ¯
        if nf == 0:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ‡ç­¾
            LOGGER.warning(f'{prefix}WARNING: No labels found in {path}. See {HELP_URL}')  # å‘å‡ºè­¦å‘Š
        x['hash'] = get_hash(self.label_files + self.img_files)  # ç”Ÿæˆå“ˆå¸Œå€¼
        x['results'] = nf, nm, ne, nc, len(self.img_files)  # ç¼“å­˜ç»“æœä¿¡æ¯
        x['msgs'] = msgs  # è®°å½•è­¦å‘Šæ¶ˆæ¯
        x['version'] = self.cache_version  # ç¼“å­˜ç‰ˆæœ¬
        try:
            np.save(path, x)  # ä¿å­˜ç¼“å­˜ä»¥å¤‡ä¸‹æ¬¡ä½¿ç”¨
            path.with_suffix('.cache.npy').rename(path)  # ç§»é™¤ .npy åç¼€
            LOGGER.info(f'{prefix}New cache created: {path}')  # è®°å½•æ–°ç¼“å­˜åˆ›å»ºçš„ä¿¡æ¯
        except Exception as e:
            LOGGER.warning(f'{prefix}WARNING: Cache directory {path.parent} is not writeable: {e}')  # ç¼“å­˜ç›®å½•ä¸å¯å†™
        return x  # è¿”å›ç¼“å­˜å­—å…¸

    def __len__(self):
        return len(self.img_files)  # è¿”å›å›¾åƒæ–‡ä»¶çš„æ•°é‡

    def __getitem__(self, index):
        index = self.indices[index]  # è·å–çº¿æ€§ã€éšæœºæˆ–åŸºäºå›¾åƒæƒé‡çš„ç´¢å¼•

        hyp = self.hyp  # è¶…å‚æ•°
        mosaic = self.mosaic and random.random() < hyp['mosaic']  # æ ¹æ®æ¦‚ç‡å†³å®šæ˜¯å¦ä½¿ç”¨é©¬èµ›å…‹å¢å¼º
        if mosaic:
            # åŠ è½½é©¬èµ›å…‹å›¾åƒ
            img, labels = load_mosaic(self, index)
            shapes = None

            # MixUpå¢å¼º
            if random.random() < hyp['mixup']:
                img, labels = mixup(img, labels, *load_mosaic(self, random.randint(0, self.n - 1)))

        else:
            # åŠ è½½å•å¹…å›¾åƒ
            img, (h0, w0), (h, w) = load_image(self, index)

            # è¿›è¡Œä¿¡ç®±å¡«å……
            shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size  # æœ€ç»ˆå¡«å……åçš„å½¢çŠ¶
            img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)
            shapes = (h0, w0), ((h / h0, w / w0), pad)  # ç”¨äºCOCO mAPçš„é‡æ ‡å®š

            labels = self.labels[index].copy()  # å¤åˆ¶æ ‡ç­¾
            if labels.size:  # å¦‚æœæœ‰æ ‡ç­¾ï¼Œè¿›è¡Œå½’ä¸€åŒ–xywhè½¬ä¸ºåƒç´ xyxyæ ¼å¼
                labels[:, 1:] = xywhn2xyxy(labels[:, 1:], ratio[0] * w, ratio[1] * h, padw=pad[0], padh=pad[1])

            if self.augment:
                # éšæœºé€è§†å˜æ¢å¢å¼º
                img, labels = random_perspective(img, labels,
                                                 degrees=hyp['degrees'],
                                                 translate=hyp['translate'],
                                                 scale=hyp['scale'],
                                                 shear=hyp['shear'],
                                                 perspective=hyp['perspective'])

        nl = len(labels)  # æ ‡ç­¾æ•°é‡
        if nl:
            labels[:, 1:5] = xyxy2xywhn(labels[:, 1:5], w=img.shape[1], h=img.shape[0], clip=True, eps=1E-3)  # è½¬æ¢æ ‡ç­¾æ ¼å¼

        if self.augment:
            # ä½¿ç”¨Albumentationsè¿›è¡Œæ•°æ®å¢å¼º
            img, labels = self.albumentations(img, labels)
            nl = len(labels)  # æ›´æ–°æ ‡ç­¾æ•°é‡

            # HSVè‰²å½©ç©ºé—´å¢å¼º
            augment_hsv(img, hgain=hyp['hsv_h'], sgain=hyp['hsv_s'], vgain=hyp['hsv_v'])

            # å‚ç›´ç¿»è½¬
            if random.random() < hyp['flipud']:
                img = np.flipud(img)
                if nl:
                    labels[:, 2] = 1 - labels[:, 2]  # æ›´æ–°æ ‡ç­¾

            # æ°´å¹³ç¿»è½¬
            if random.random() < hyp['fliplr']:
                img = np.fliplr(img)
                if nl:
                    labels[:, 1] = 1 - labels[:, 1]  # æ›´æ–°æ ‡ç­¾

            # Cutoutsï¼ˆå¯ä»¥é€‰æ‹©æ€§å¼€å¯ï¼‰
            # labels = cutout(img, labels, p=0.5)

        labels_out = torch.zeros((nl, 6))  # åˆå§‹åŒ–è¾“å‡ºæ ‡ç­¾
        if nl:
            labels_out[:, 1:] = torch.from_numpy(labels)  # å°†æ ‡ç­¾è½¬ä¸ºtorchå¼ é‡

        # è½¬æ¢å›¾åƒæ ¼å¼
        img = img.transpose((2, 0, 1))[::-1]  # HWCè½¬CHWï¼ŒBGRè½¬RGB
        img = np.ascontiguousarray(img)  # ç¡®ä¿æ•°ç»„æ˜¯è¿ç»­çš„

        return torch.from_numpy(img), labels_out, self.img_files[index], shapes  # è¿”å›å›¾åƒã€æ ‡ç­¾ã€æ–‡ä»¶åå’Œå½¢çŠ¶

    @staticmethod
    def collate_fn(batch):
        img, label, path, shapes = zip(*batch)  # è§£å‹ç¼©æ‰¹æ¬¡æ•°æ®ï¼Œå¾—åˆ°å›¾åƒã€æ ‡ç­¾ã€è·¯å¾„å’Œå½¢çŠ¶
        for i, l in enumerate(label):
            l[:, 0] = i  # ä¸ºæ¯ä¸ªæ ‡ç­¾æ·»åŠ ç›®æ ‡å›¾åƒç´¢å¼•ï¼Œç”¨äºæ„å»ºç›®æ ‡
        return torch.stack(img, 0), torch.cat(label, 0), path, shapes  # è¿”å›å †å åçš„å›¾åƒã€æ‹¼æ¥åçš„æ ‡ç­¾ã€è·¯å¾„å’Œå½¢çŠ¶

    @staticmethod
    def collate_fn4(batch):
        img, label, path, shapes = zip(*batch)  # è§£å‹ç¼©æ‰¹æ¬¡æ•°æ®ï¼Œå¾—åˆ°å›¾åƒã€æ ‡ç­¾ã€è·¯å¾„å’Œå½¢çŠ¶
        n = len(shapes) // 4  # æ¯ä¸ªç»„åˆçš„å›¾åƒæ•°é‡
        img4, label4, path4, shapes4 = [], [], path[:n], shapes[:n]  # åˆå§‹åŒ–æ–°åˆ—è¡¨

        ho = torch.tensor([[0.0, 0, 0, 1, 0, 0]])  # å‚ç›´ç¿»è½¬åç§»
        wo = torch.tensor([[0.0, 0, 1, 0, 0, 0]])  # æ°´å¹³ç¿»è½¬åç§»
        s = torch.tensor([[1, 1, 0.5, 0.5, 0.5, 0.5]])  # ç¼©æ”¾å› å­

        for i in range(n):  # éå†æ¯ç»„å›¾åƒ
            i *= 4  # æ¯ç»„åŒ…å«4å¼ å›¾åƒ
            if random.random() < 0.5:  # 50%æ¦‚ç‡é€‰æ‹©æ’å€¼
                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2.0, mode='bilinear', align_corners=False)[
                    0].type(img[i].type())  # ä½¿ç”¨åŒçº¿æ€§æ’å€¼æ”¾å¤§å›¾åƒ
                l = label[i]  # å–å¯¹åº”æ ‡ç­¾
            else:
                # å°†å››å¼ å›¾åƒæ‹¼æ¥æˆä¸€å¼ 
                im = torch.cat((torch.cat((img[i], img[i + 1]), 1), torch.cat((img[i + 2], img[i + 3]), 1)), 2)
                # æ‹¼æ¥æ ‡ç­¾å¹¶åº”ç”¨åç§»å’Œç¼©æ”¾
                l = torch.cat((label[i], label[i + 1] + ho, label[i + 2] + wo, label[i + 3] + ho + wo), 0) * s
            img4.append(im)  # æ·»åŠ å¤„ç†åçš„å›¾åƒ
            label4.append(l)  # æ·»åŠ å¤„ç†åçš„æ ‡ç­¾

        for i, l in enumerate(label4):
            l[:, 0] = i  # ä¸ºæ¯ä¸ªæ ‡ç­¾æ·»åŠ ç›®æ ‡å›¾åƒç´¢å¼•ï¼Œç”¨äºæ„å»ºç›®æ ‡

        return torch.stack(img4, 0), torch.cat(label4, 0), path4, shapes4  # è¿”å›å †å åçš„å›¾åƒã€æ‹¼æ¥åçš„æ ‡ç­¾ã€è·¯å¾„å’Œå½¢çŠ¶


# Ancillary functions --------------------------------------------------------------------------------------------------
def load_image(self, i):
    # ä»æ•°æ®é›†ä¸­åŠ è½½ç´¢å¼•ä¸º 'i' çš„ä¸€å¼ å›¾åƒï¼Œè¿”å›å›¾åƒã€åŸå§‹é«˜åº¦å®½åº¦å’Œè°ƒæ•´åé«˜åº¦å®½åº¦
    im = self.imgs[i]  # ä»ç¼“å­˜ä¸­è·å–å›¾åƒ
    if im is None:  # å¦‚æœæœªç¼“å­˜åˆ°å†…å­˜
        npy = self.img_npy[i]  # è·å–å¯¹åº”çš„.npyæ–‡ä»¶è·¯å¾„
        if npy and npy.exists():  # å¦‚æœ.npyæ–‡ä»¶å­˜åœ¨ï¼Œåˆ™åŠ è½½
            im = np.load(npy)
        else:  # å¦åˆ™ï¼Œä»å›¾åƒè·¯å¾„è¯»å–å›¾åƒ
            path = self.img_files[i]  # è·å–å›¾åƒè·¯å¾„
            im = cv2.imread(path)  # è¯»å–å›¾åƒ (BGRæ ¼å¼)
            assert im is not None, f'Image Not Found {path}'  # ç¡®ä¿å›¾åƒæˆåŠŸè¯»å–
        h0, w0 = im.shape[:2]  # è·å–åŸå§‹é«˜åº¦å’Œå®½åº¦
        r = self.img_size / max(h0, w0)  # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        if r != 1:  # å¦‚æœå°ºå¯¸ä¸ç›¸ç­‰
            im = cv2.resize(im, (int(w0 * r), int(h0 * r)),
                            interpolation=cv2.INTER_AREA if r < 1 and not self.augment else cv2.INTER_LINEAR)  # è°ƒæ•´å›¾åƒå¤§å°
        return im, (h0, w0), im.shape[:2]  # è¿”å›å›¾åƒã€åŸå§‹é«˜åº¦å®½åº¦å’Œè°ƒæ•´åé«˜åº¦å®½åº¦
    else:
        return self.imgs[i], self.img_hw0[i], self.img_hw[i]  # å¦‚æœç¼“å­˜ä¸­æœ‰å›¾åƒï¼Œç›´æ¥è¿”å›ç¼“å­˜çš„å›¾åƒåŠå…¶å°ºå¯¸



def load_mosaic(self, index):
    # 4å›¾åƒæ‹¼æ¥åŠ è½½å™¨ã€‚åŠ è½½1å¼ å›¾åƒå’Œ3å¼ éšæœºå›¾åƒåˆ°ä¸€ä¸ª4å›¾åƒçš„æ‹¼æ¥ä¸­
    labels4, segments4 = [], []  # åˆå§‹åŒ–æ ‡ç­¾å’Œåˆ†æ®µåˆ—è¡¨
    s = self.img_size  # å›¾åƒå°ºå¯¸
    # éšæœºç¡®å®šæ‹¼æ¥ä¸­å¿ƒçš„ x å’Œ y åæ ‡
    yc, xc = (int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border)
    indices = [index] + random.choices(self.indices, k=3)  # éšæœºé€‰æ‹©3ä¸ªé¢å¤–çš„å›¾åƒç´¢å¼•
    random.shuffle(indices)  # æ‰“ä¹±ç´¢å¼•é¡ºåº

    for i, index in enumerate(indices):
        # åŠ è½½å›¾åƒ
        img, _, (h, w) = load_image(self, index)

        # å°†å›¾åƒæ”¾ç½®åœ¨ img4 ä¸­
        if i == 0:  # å·¦ä¸Šè§’
            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # åˆ›å»ºä¸€ä¸ªåŸºäº4ä¸ªæ‹¼æ¥å›¾åƒçš„ç©ºç™½å›¾åƒ
            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # å¤§å›¾åƒçš„è¾¹ç•Œ
            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # å°å›¾åƒçš„è¾¹ç•Œ
        elif i == 1:  # å³ä¸Šè§’
            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc
            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h
        elif i == 2:  # å·¦ä¸‹è§’
            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)
            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)
        elif i == 3:  # å³ä¸‹è§’
            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)
            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)

        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # å°†å›¾åƒæ”¾ç½®åˆ°æ‹¼æ¥å›¾åƒçš„å¯¹åº”ä½ç½®
        padw = x1a - x1b  # è®¡ç®—æ°´å¹³å¡«å……
        padh = y1a - y1b  # è®¡ç®—å‚ç›´å¡«å……

        # å¤„ç†æ ‡ç­¾
        labels, segments = self.labels[index].copy(), self.segments[index].copy()  # å¤åˆ¶æ ‡ç­¾å’Œåˆ†æ®µä¿¡æ¯
        if labels.size:
            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padw, padh)  # å°†å½’ä¸€åŒ–çš„xywhæ ¼å¼è½¬æ¢ä¸ºåƒç´ xyxyæ ¼å¼
            segments = [xyn2xy(x, w, h, padw, padh) for x in segments]  # è½¬æ¢åˆ†æ®µæ ¼å¼
        labels4.append(labels)  # æ·»åŠ æ ‡ç­¾
        segments4.extend(segments)  # æ·»åŠ åˆ†æ®µ

    # è¿æ¥/è£å‰ªæ ‡ç­¾
    labels4 = np.concatenate(labels4, 0)  # è¿æ¥æ‰€æœ‰æ ‡ç­¾
    for x in (labels4[:, 1:], *segments4):
        np.clip(x, 0, 2 * s, out=x)  # åœ¨ä½¿ç”¨ random_perspective() æ—¶è£å‰ª

    # æ•°æ®å¢å¼º
    img4, labels4, segments4 = copy_paste(img4, labels4, segments4, p=self.hyp['copy_paste'])  # æ‹·è´ç²˜è´´å¢å¼º
    img4, labels4 = random_perspective(img4, labels4, segments4,
                                       degrees=self.hyp['degrees'],
                                       translate=self.hyp['translate'],
                                       scale=self.hyp['scale'],
                                       shear=self.hyp['shear'],
                                       perspective=self.hyp['perspective'],
                                       border=self.mosaic_border)  # è¿›è¡Œéšæœºé€è§†å˜æ¢

    return img4, labels4  # è¿”å›æ‹¼æ¥å›¾åƒå’Œæ ‡ç­¾


def load_mosaic9(self, index):
    # 9å›¾åƒæ‹¼æ¥åŠ è½½å™¨ã€‚åŠ è½½1å¼ å›¾åƒå’Œ8å¼ éšæœºå›¾åƒåˆ°ä¸€ä¸ª9å›¾åƒçš„æ‹¼æ¥ä¸­
    labels9, segments9 = [], []  # åˆå§‹åŒ–æ ‡ç­¾å’Œåˆ†æ®µåˆ—è¡¨
    s = self.img_size  # å›¾åƒå°ºå¯¸
    indices = [index] + random.choices(self.indices, k=8)  # éšæœºé€‰æ‹©8ä¸ªé¢å¤–çš„å›¾åƒç´¢å¼•
    random.shuffle(indices)  # æ‰“ä¹±ç´¢å¼•é¡ºåº

    for i, index in enumerate(indices):
        # åŠ è½½å›¾åƒ
        img, _, (h, w) = load_image(self, index)

        # å°†å›¾åƒæ”¾ç½®åœ¨ img9 ä¸­
        if i == 0:  # ä¸­å¿ƒä½ç½®
            img9 = np.full((s * 3, s * 3, img.shape[2]), 114, dtype=np.uint8)  # åˆ›å»ºä¸€ä¸ªåŸºäº9ä¸ªæ‹¼æ¥å›¾åƒçš„ç©ºç™½å›¾åƒ
            h0, w0 = h, w  # ä¿å­˜åŸå§‹é«˜åº¦å’Œå®½åº¦
            c = s, s, s + w, s + h  # åŸºç¡€åæ ‡ (xmin, ymin, xmax, ymax)
        elif i == 1:  # é¡¶éƒ¨
            c = s, s - h, s + w, s
        elif i == 2:  # å³ä¸Šè§’
            c = s + wp, s - h, s + wp + w, s
        elif i == 3:  # å³ä¾§
            c = s + w0, s, s + w0 + w, s + h
        elif i == 4:  # å³ä¸‹è§’
            c = s + w0, s + hp, s + w0 + w, s + hp + h
        elif i == 5:  # åº•éƒ¨
            c = s + w0 - w, s + h0, s + w0, s + h0 + h
        elif i == 6:  # å·¦ä¸‹è§’
            c = s + w0 - wp - w, s + h0, s + w0 - wp, s + h0 + h
        elif i == 7:  # å·¦ä¾§
            c = s - w, s + h0 - h, s, s + h0
        elif i == 8:  # å·¦ä¸Šè§’
            c = s - w, s + h0 - hp - h, s, s + h0 - hp

        padx, pady = c[:2]  # è·å–åç§»é‡
        x1, y1, x2, y2 = (max(x, 0) for x in c)  # åˆ†é…åæ ‡

        # å¤„ç†æ ‡ç­¾
        labels, segments = self.labels[index].copy(), self.segments[index].copy()  # å¤åˆ¶æ ‡ç­¾å’Œåˆ†æ®µä¿¡æ¯
        if labels.size:
            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padx, pady)  # å°†å½’ä¸€åŒ–çš„xywhæ ¼å¼è½¬æ¢ä¸ºåƒç´ xyxyæ ¼å¼
            segments = [xyn2xy(x, w, h, padx, pady) for x in segments]  # è½¬æ¢åˆ†æ®µæ ¼å¼
        labels9.append(labels)  # æ·»åŠ æ ‡ç­¾
        segments9.extend(segments)  # æ·»åŠ åˆ†æ®µ

        # å°†å›¾åƒæ”¾å…¥æ‹¼æ¥å›¾åƒä¸­
        img9[y1:y2, x1:x2] = img[y1 - pady:, x1 - padx:]  # img9[ymin:ymax, xmin:xmax]
        hp, wp = h, w  # ä¿å­˜ä¸Šä¸€ä¸ªå›¾åƒçš„é«˜åº¦å’Œå®½åº¦

    # éšæœºåç§»
    yc, xc = (int(random.uniform(0, s)) for _ in self.mosaic_border)  # æ‹¼æ¥ä¸­å¿ƒçš„ x å’Œ y åæ ‡
    img9 = img9[yc:yc + 2 * s, xc:xc + 2 * s]  # ä»æ‹¼æ¥å›¾åƒä¸­è£å‰ª

    # è¿æ¥/è£å‰ªæ ‡ç­¾
    labels9 = np.concatenate(labels9, 0)  # è¿æ¥æ‰€æœ‰æ ‡ç­¾
    labels9[:, [1, 3]] -= xc  # è°ƒæ•´ x åæ ‡
    labels9[:, [2, 4]] -= yc  # è°ƒæ•´ y åæ ‡
    c = np.array([xc, yc])  # ä¸­å¿ƒåæ ‡
    segments9 = [x - c for x in segments9]  # è°ƒæ•´åˆ†æ®µåæ ‡

    for x in (labels9[:, 1:], *segments9):
        np.clip(x, 0, 2 * s, out=x)  # åœ¨ä½¿ç”¨ random_perspective() æ—¶è£å‰ª

    # æ•°æ®å¢å¼º
    img9, labels9 = random_perspective(img9, labels9, segments9,
                                       degrees=self.hyp['degrees'],
                                       translate=self.hyp['translate'],
                                       scale=self.hyp['scale'],
                                       shear=self.hyp['shear'],
                                       perspective=self.hyp['perspective'],
                                       border=self.mosaic_border)  # è¿›è¡Œéšæœºé€è§†å˜æ¢

    return img9, labels9  # è¿”å›æ‹¼æ¥å›¾åƒå’Œæ ‡ç­¾


def create_folder(path='./new'):
    # åˆ›å»ºæ–‡ä»¶å¤¹
    if os.path.exists(path):
        shutil.rmtree(path)  # åˆ é™¤å·²æœ‰çš„è¾“å‡ºæ–‡ä»¶å¤¹
    os.makedirs(path)  # åˆ›å»ºæ–°çš„è¾“å‡ºæ–‡ä»¶å¤¹

def flatten_recursive(path='../datasets/coco128'):
    # æ‰å¹³åŒ–é€’å½’ç›®å½•ï¼Œå°†æ‰€æœ‰æ–‡ä»¶ç§»åŠ¨åˆ°é¡¶å±‚
    new_path = Path(path + '_flat')  # åˆ›å»ºæ–°è·¯å¾„
    create_folder(new_path)  # åˆ›å»ºæ–°æ–‡ä»¶å¤¹
    for file in tqdm(glob.glob(str(Path(path)) + '/**/*.*', recursive=True)):
        # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        shutil.copyfile(file, new_path / Path(file).name)  # å°†æ–‡ä»¶å¤åˆ¶åˆ°æ–°æ–‡ä»¶å¤¹



def extract_boxes(path='../datasets/coco128'):  # ä» utils.datasets å¯¼å…¥ *; æå–æ¡†
    # å°†æ£€æµ‹æ•°æ®é›†è½¬æ¢ä¸ºåˆ†ç±»æ•°æ®é›†ï¼Œæ¯ä¸ªç±»åˆ«ä¸€ä¸ªç›®å½•
    path = Path(path)  # å›¾åƒç›®å½•
    shutil.rmtree(path / 'classifier') if (path / 'classifier').is_dir() else None  # åˆ é™¤å·²å­˜åœ¨çš„ç›®å½•
    files = list(path.rglob('*.*'))  # è·å–æ‰€æœ‰æ–‡ä»¶
    n = len(files)  # æ–‡ä»¶æ€»æ•°
    for im_file in tqdm(files, total=n):
        if im_file.suffix[1:] in IMG_FORMATS:
            # å¤„ç†å›¾åƒ
            im = cv2.imread(str(im_file))[..., ::-1]  # BGR è½¬ RGB
            h, w = im.shape[:2]  # è·å–å›¾åƒçš„é«˜å’Œå®½

            # åŠ è½½æ ‡ç­¾
            lb_file = Path(img2label_paths([str(im_file)])[0])  # è·å–å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶
            if Path(lb_file).exists():
                with open(lb_file) as f:
                    lb = np.array([x.split() for x in f.read().strip().splitlines()], dtype=np.float32)  # è¯»å–æ ‡ç­¾

                for j, x in enumerate(lb):
                    c = int(x[0])  # ç±»åˆ«
                    f = (path / 'classifier') / f'{c}' / f'{path.stem}_{im_file.stem}_{j}.jpg'  # æ–°æ–‡ä»¶å
                    if not f.parent.is_dir():
                        f.parent.mkdir(parents=True)  # åˆ›å»ºç±»åˆ«ç›®å½•

                    b = x[1:] * [w, h, w, h]  # è¾¹ç•Œæ¡†
                    # b[2:] = b[2:].max()  # çŸ©å½¢è½¬æ¢ä¸ºæ­£æ–¹å½¢
                    b[2:] = b[2:] * 1.2 + 3  # æ‰©å±•è¾¹ç•Œæ¡†
                    b = xywh2xyxy(b.reshape(-1, 4)).ravel().astype(np.int)  # è½¬æ¢ä¸º (x1, y1, x2, y2) æ ¼å¼

                    b[[0, 2]] = np.clip(b[[0, 2]], 0, w)  # é™åˆ¶è¾¹ç•Œæ¡†åœ¨å›¾åƒå†…
                    b[[1, 3]] = np.clip(b[[1, 3]], 0, h)
                    assert cv2.imwrite(str(f), im[b[1]:b[3], b[0]:b[2]]), f'box failure in {f}'  # ä¿å­˜è£å‰ªçš„å›¾åƒ

def autosplit(path='../datasets/coco128/images', weights=(0.9, 0.1, 0.0), annotated_only=False):
    """ è‡ªåŠ¨å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼Œå¹¶ä¿å­˜ path/autosplit_*.txt æ–‡ä»¶
    ä½¿ç”¨æ–¹æ³•: from utils.datasets import *; autosplit()
    å‚æ•°
        path:            å›¾åƒç›®å½•çš„è·¯å¾„
        weights:         è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•çš„æƒé‡ (åˆ—è¡¨æˆ–å…ƒç»„)
        annotated_only:  ä»…ä½¿ç”¨æœ‰æ ‡æ³¨çš„å›¾åƒ
    """
    path = Path(path)  # å›¾åƒç›®å½•
    files = sorted(x for x in path.rglob('*.*') if x.suffix[1:].lower() in IMG_FORMATS)  # ä»…è·å–å›¾åƒæ–‡ä»¶
    n = len(files)  # æ–‡ä»¶æ€»æ•°
    random.seed(0)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§
    indices = random.choices([0, 1, 2], weights=weights, k=n)  # æ ¹æ®æƒé‡åˆ†é…æ¯ä¸ªå›¾åƒåˆ°ä¸åŒçš„åˆ†ç»„

    txt = ['autosplit_train.txt', 'autosplit_val.txt', 'autosplit_test.txt']  # 3 ä¸ª txt æ–‡ä»¶
    [(path.parent / x).unlink(missing_ok=True) for x in txt]  # åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶

    print(f'ä» {path} è‡ªåŠ¨æ‹†åˆ†å›¾åƒ' + ', ä»…ä½¿ç”¨æ ‡æ³¨çš„ *.txt å›¾åƒ' * annotated_only)
    for i, img in tqdm(zip(indices, files), total=n):
        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾
            with open(path.parent / txt[i], 'a') as f:
                f.write('./' + img.relative_to(path.parent).as_posix() + '\n')  # å°†å›¾åƒè·¯å¾„å†™å…¥ç›¸åº”çš„ txt æ–‡ä»¶


def verify_image_label(args):
    # éªŒè¯å•ä¸ªå›¾åƒ-æ ‡ç­¾å¯¹
    im_file, lb_file, prefix = args
    nm, nf, ne, nc, msg, segments = 0, 0, 0, 0, '', []  # ç»Ÿè®¡ï¼ˆç¼ºå¤±ã€æ‰¾åˆ°ã€ç©ºã€æŸåï¼‰ï¼Œæ¶ˆæ¯ï¼Œåˆ†æ®µ
    try:
        # éªŒè¯å›¾åƒ
        im = Image.open(im_file)
        im.verify()  # ä½¿ç”¨ PIL éªŒè¯å›¾åƒ
        shape = exif_size(im)  # è·å–å›¾åƒå°ºå¯¸
        assert (shape[0] > 9) & (shape[1] > 9), f'å›¾åƒå°ºå¯¸ {shape} å°äº 10 åƒç´ '
        assert im.format.lower() in IMG_FORMATS, f'æ— æ•ˆçš„å›¾åƒæ ¼å¼ {im.format}'

        if im.format.lower() in ('jpg', 'jpeg'):
            with open(im_file, 'rb') as f:
                f.seek(-2, 2)
                if f.read() != b'\xff\xd9':  # æ£€æŸ¥ JPEG æ˜¯å¦æŸå
                    ImageOps.exif_transpose(Image.open(im_file)).save(im_file, 'JPEG', subsampling=0, quality=100)
                    msg = f'{prefix}è­¦å‘Š: {im_file}: æŸåçš„ JPEG å·²æ¢å¤å¹¶ä¿å­˜'

        # éªŒè¯æ ‡ç­¾
        if os.path.isfile(lb_file):
            nf = 1  # æ‰¾åˆ°æ ‡ç­¾
            with open(lb_file) as f:
                l = [x.split() for x in f.read().strip().splitlines() if len(x)]
                if any([len(x) > 8 for x in l]):  # åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†æ®µ
                    classes = np.array([x[0] for x in l], dtype=np.float32)
                    segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in l]  # (ç±», xy1...)
                    l = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (ç±», xywh)
                l = np.array(l, dtype=np.float32)
            nl = len(l)
            if nl:
                assert l.shape[1] == 5, f'æ ‡ç­¾éœ€åŒ…å« 5 åˆ—ï¼Œæ£€æµ‹åˆ° {l.shape[1]} åˆ—'
                assert (l >= 0).all(), f'æ ‡ç­¾å€¼ä¸èƒ½ä¸ºè´Ÿæ•° {l[l < 0]}'
                assert (l[:, 1:] <= 1).all(), f'åæ ‡æœªå½’ä¸€åŒ–æˆ–è¶…å‡ºèŒƒå›´ {l[:, 1:][l[:, 1:] > 1]}'
                _, i = np.unique(l, axis=0, return_index=True)
                if len(i) < nl:  # æ£€æŸ¥é‡å¤è¡Œ
                    l = l[i]  # å»é™¤é‡å¤é¡¹
                    if segments:
                        segments = segments[i]
                    msg = f'{prefix}è­¦å‘Š: {im_file}: ç§»é™¤ {nl - len(i)} ä¸ªé‡å¤æ ‡ç­¾'
            else:
                ne = 1  # æ ‡ç­¾ä¸ºç©º
                l = np.zeros((0, 5), dtype=np.float32)
        else:
            nm = 1  # æ ‡ç­¾ç¼ºå¤±
            l = np.zeros((0, 5), dtype=np.float32)
        return im_file, l, shape, segments, nm, nf, ne, nc, msg
    except Exception as e:
        nc = 1
        msg = f'{prefix}è­¦å‘Š: {im_file}: å¿½ç•¥æŸåçš„å›¾åƒ/æ ‡ç­¾: {e}'
        return [None, None, None, None, nm, nf, ne, nc, msg]


def dataset_stats(path='coco128.yaml', autodownload=False, verbose=False, profile=False, hub=False):
    """ è¿”å›æ•°æ®é›†ç»Ÿè®¡å­—å…¸ï¼ŒåŒ…æ‹¬æ¯ä¸ªç±»åˆ«åœ¨æ¯ä¸ªæ‹†åˆ†ä¸­çš„å›¾åƒå’Œå®ä¾‹è®¡æ•°
    è¦åœ¨çˆ¶ç›®å½•ä¸­è¿è¡Œï¼šexport PYTHONPATH="$PWD/yolov3"
    Usage1: from utils.datasets import *; dataset_stats('coco128.yaml', autodownload=True)
    Usage2: from utils.datasets import *; dataset_stats('../datasets/coco128_with_yaml.zip')
    å‚æ•°
        path:           data.yaml æˆ–åŒ…å« data.yaml çš„ data.zip çš„è·¯å¾„
        autodownload:   å¦‚æœæœ¬åœ°ä¸å­˜åœ¨æ•°æ®é›†ï¼Œåˆ™å°è¯•ä¸‹è½½æ•°æ®é›†
        verbose:        æ‰“å°ç»Ÿè®¡å­—å…¸
        profile:        æ‰§è¡Œæ€§èƒ½åˆ†æ
        hub:            æ˜¯å¦è¿›è¡Œ HUB æ“ä½œï¼Œç”¨äºç½‘ç»œ/åº”ç”¨æŸ¥çœ‹
    """

    def round_labels(labels):
        # æ›´æ–°æ ‡ç­¾ä¸ºæ•´æ•°ç±»å’Œ 6 ä½å°æ•°
        return [[int(c), *(round(x, 4) for x in points)] for c, *points in labels]

    def unzip(path):
        # è§£å‹ data.zipï¼Œæ³¨æ„ï¼špath/to/abc.zip å¿…é¡»è§£å‹åˆ° 'path/to/abc/' ä¸­
        if str(path).endswith('.zip'):  # path æ˜¯ data.zip
            assert Path(path).is_file(), f'è§£å‹ {path} æ—¶å‡ºé”™ï¼Œæ–‡ä»¶æœªæ‰¾åˆ°'
            ZipFile(path).extractall(path=path.parent)  # è§£å‹
            dir = path.with_suffix('')  # æ•°æ®é›†ç›®å½• == å‹ç¼©åŒ…åç§°
            return True, str(dir), next(dir.rglob('*.yaml'))  # å·²å‹ç¼©ï¼Œæ•°æ®ç›®å½•ï¼Œyaml è·¯å¾„
        else:  # path æ˜¯ data.yaml
            return False, None, path

    def hub_ops(f, max_dim=1920):
        # HUB æ“ä½œç”¨äºä¸€ä¸ªå›¾åƒ 'f'ï¼šè°ƒæ•´å¤§å°å¹¶ä»¥è¾ƒä½çš„è´¨é‡ä¿å­˜åˆ° /dataset-hub ç”¨äºç½‘ç»œ/åº”ç”¨æŸ¥çœ‹
        f_new = im_dir / Path(f).name  # dataset-hub å›¾åƒæ–‡ä»¶å
        try:  # ä½¿ç”¨ PIL
            im = Image.open(f)
            r = max_dim / max(im.height, im.width)  # æ¯”ä¾‹
            if r < 1.0:  # å›¾åƒå¤ªå¤§
                im = im.resize((int(im.width * r), int(im.height * r)))
            im.save(f_new, 'JPEG', quality=75, optimize=True)  # ä¿å­˜
        except Exception as e:  # ä½¿ç”¨ OpenCV
            print(f'è­¦å‘Š: HUB æ“ä½œ PIL å¤±è´¥ {f}: {e}')
            im = cv2.imread(f)
            im_height, im_width = im.shape[:2]
            r = max_dim / max(im_height, im_width)  # æ¯”ä¾‹
            if r < 1.0:  # å›¾åƒå¤ªå¤§
                im = cv2.resize(im, (int(im_width * r), int(im_height * r)), interpolation=cv2.INTER_LINEAR)
            cv2.imwrite(str(f_new), im)

    zipped, data_dir, yaml_path = unzip(Path(path))
    with open(check_yaml(yaml_path), errors='ignore') as f:
        data = yaml.safe_load(f)  # æ•°æ®å­—å…¸
        if zipped:
            data['path'] = data_dir  # TODO: è¿™åº”è¯¥æ˜¯ dir.resolve() å—ï¼Ÿ
    check_dataset(data, autodownload)  # å¦‚æœç¼ºå¤±ï¼Œä¸‹è½½æ•°æ®é›†
    hub_dir = Path(data['path'] + ('-hub' if hub else ''))
    stats = {'nc': data['nc'], 'names': data['names']}  # ç»Ÿè®¡å­—å…¸
    for split in 'train', 'val', 'test':
        if data.get(split) is None:
            stats[split] = None  # ä¾‹å¦‚æ²¡æœ‰æµ‹è¯•é›†
            continue
        x = []
        dataset = LoadImagesAndLabels(data[split])  # åŠ è½½æ•°æ®é›†
        for label in tqdm(dataset.labels, total=dataset.n, desc='ç»Ÿè®¡ä¿¡æ¯'):
            x.append(np.bincount(label[:, 0].astype(int), minlength=data['nc']))
        x = np.array(x)  # shape(128x80)
        stats[split] = {'instance_stats': {'total': int(x.sum()), 'per_class': x.sum(0).tolist()},
                        'image_stats': {'total': dataset.n, 'unlabelled': int(np.all(x == 0, 1).sum()),
                                        'per_class': (x > 0).sum(0).tolist()},
                        'labels': [{str(Path(k).name): round_labels(v.tolist())} for k, v in
                                   zip(dataset.img_files, dataset.labels)]}

        if hub:
            im_dir = hub_dir / 'images'
            im_dir.mkdir(parents=True, exist_ok=True)
            for _ in tqdm(ThreadPool(NUM_THREADS).imap(hub_ops, dataset.img_files), total=dataset.n, desc='HUB æ“ä½œ'):
                pass

    # æ€§èƒ½åˆ†æ
    stats_path = hub_dir / 'stats.json'
    if profile:
        for _ in range(1):
            file = stats_path.with_suffix('.npy')
            t1 = time.time()
            np.save(file, stats)
            t2 = time.time()
            x = np.load(file, allow_pickle=True)
            print(f'stats.npy times: è¯»å– {time.time() - t2:.3f}s, å†™å…¥ {t2 - t1:.3f}s')

            file = stats_path.with_suffix('.json')
            t1 = time.time()
            with open(file, 'w') as f:
                json.dump(stats, f)  # ä¿å­˜ stats *.json
            t2 = time.time()
            with open(file) as f:
                x = json.load(f)  # åŠ è½½ hyps å­—å…¸
            print(f'stats.json times: è¯»å– {time.time() - t2:.3f}s, å†™å…¥ {t2 - t1:.3f}s')

    # ä¿å­˜ã€æ‰“å°å¹¶è¿”å›
    if hub:
        print(f'ä¿å­˜ {stats_path.resolve()}...')
        with open(stats_path, 'w') as f:
            json.dump(stats, f)  # ä¿å­˜ stats.json
    if verbose:
        print(json.dumps(stats, indent=2, sort_keys=False))
    return stats

